2025-05-24 14:29:24,887 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:29:24,887 - INFO - joeynmt.helpers -                           cfg.name : bpe_2k
2025-05-24 14:29:24,888 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-24 14:29:24,888 - INFO - joeynmt.helpers -                     cfg.data.train : data/train
2025-05-24 14:29:24,888 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2025-05-24 14:29:24,888 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2025-05-24 14:29:24,888 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-24 14:29:24,888 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : bpe/vocab.joint
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 2000
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : bpe/codes.bpe
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : nl
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : bpe/vocab.joint
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : bpe/codes.bpe
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-24 14:29:24,892 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_2k
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-24 14:29:24,893 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-24 14:29:24,894 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-24 14:29:24,895 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-24 14:29:24,895 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-24 14:29:24,895 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-24 14:29:24,908 - INFO - joeynmt.data - Building tokenizer...
2025-05-24 14:29:24,923 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-24 14:29:24,924 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-24 14:29:24,924 - INFO - joeynmt.data - Loading train set...
2025-05-24 14:29:25,107 - INFO - joeynmt.data - Building vocabulary...
2025-05-24 14:29:25,153 - INFO - joeynmt.data - Loading dev set...
2025-05-24 14:29:25,171 - INFO - joeynmt.data - Loading test set...
2025-05-24 14:29:25,181 - INFO - joeynmt.data - Data loaded.
2025-05-24 14:29:25,181 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=nl, has_trg=True, random_subset=-1)
2025-05-24 14:29:25,181 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=1003, src_lang=en, trg_lang=nl, has_trg=True, random_subset=-1)
2025-05-24 14:29:25,181 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1777, src_lang=en, trg_lang=nl, has_trg=True, random_subset=-1)
2025-05-24 14:29:25,181 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ ore : A@@ ver@@ ting the c@@ lim@@ ate c@@ ris@@ is
	[TRG] A@@ l G@@ ore over het af@@ w@@ enden van de k@@ li@@ ma@@ at@@ c@@ ris@@ is
2025-05-24 14:29:25,181 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4)  (5) ! (6) # (7) $ (8) £ (9) %
2025-05-24 14:29:25,181 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4)  (5) ! (6) # (7) $ (8) £ (9) %
2025-05-24 14:29:25,182 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 1997
2025-05-24 14:29:25,182 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 1997
2025-05-24 14:29:25,188 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:29:25,231 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:29:25,235 - INFO - joeynmt.model - Total params: 3410432
2025-05-24 14:29:25,235 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-24 14:29:25,235 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=1997),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=1997),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-24 14:29:25,332 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-24 14:29:25,332 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-24 14:29:25,332 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-24 14:29:25,332 - INFO - joeynmt.training - EPOCH 1
2025-05-24 14:29:31,765 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.928236, Batch Acc: 0.054627, Tokens per Sec:    11335, Lr: 0.000300
2025-05-24 14:29:37,180 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     3.715494, Batch Acc: 0.084765, Tokens per Sec:    13526, Lr: 0.000300
2025-05-24 14:29:42,524 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.617099, Batch Acc: 0.107941, Tokens per Sec:    13810, Lr: 0.000300
2025-05-24 14:29:48,078 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.534421, Batch Acc: 0.121637, Tokens per Sec:    13272, Lr: 0.000300
2025-05-24 14:29:53,608 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.314536, Batch Acc: 0.131716, Tokens per Sec:    13353, Lr: 0.000300
2025-05-24 14:29:53,609 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:29:53,609 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:30:08,969 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.49, ppl:  32.67, acc:   0.12, generation: 15.3379[sec], evaluation: 0.0000[sec]
2025-05-24 14:30:08,969 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:30:09,049 - INFO - joeynmt.training - Example #0
2025-05-24 14:30:09,049 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:30:09,049 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:30:09,049 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'de', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@']
2025-05-24 14:30:09,050 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:30:09,050 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:30:09,050 - INFO - joeynmt.training - 	Hypothesis: Ik ik ik ik ik ik ik ik ik ik ik ik ik ik de kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk
2025-05-24 14:30:09,050 - INFO - joeynmt.training - Example #1
2025-05-24 14:30:09,050 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:30:09,050 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:30:09,050 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'het', 'het', 'het', 'het', 'het', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'z@@', 'den', '.', '</s>']
2025-05-24 14:30:09,050 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:30:09,051 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:30:09,051 - INFO - joeynmt.training - 	Hypothesis: Maar het het het het het gegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegezden .
2025-05-24 14:30:09,051 - INFO - joeynmt.training - Example #2
2025-05-24 14:30:09,051 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:30:09,051 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:30:09,051 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@']
2025-05-24 14:30:09,051 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:30:09,051 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:30:09,051 - INFO - joeynmt.training - 	Hypothesis: De kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk
2025-05-24 14:30:09,051 - INFO - joeynmt.training - Example #3
2025-05-24 14:30:09,052 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:30:09,052 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:30:09,052 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'een', 'een', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@']
2025-05-24 14:30:09,052 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:30:09,053 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:30:09,053 - INFO - joeynmt.training - 	Hypothesis: Het is een een gegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegege
2025-05-24 14:30:09,053 - INFO - joeynmt.training - Example #4
2025-05-24 14:30:09,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:30:09,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:30:09,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@']
2025-05-24 14:30:09,053 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:30:09,053 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:30:09,053 - INFO - joeynmt.training - 	Hypothesis: De kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk
2025-05-24 14:30:14,258 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.367428, Batch Acc: 0.141545, Tokens per Sec:    13873, Lr: 0.000300
2025-05-24 14:30:19,665 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.403190, Batch Acc: 0.152390, Tokens per Sec:    13676, Lr: 0.000300
2025-05-24 14:30:24,907 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.179712, Batch Acc: 0.161738, Tokens per Sec:    13975, Lr: 0.000300
2025-05-24 14:30:30,283 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.192942, Batch Acc: 0.172803, Tokens per Sec:    13757, Lr: 0.000300
2025-05-24 14:30:35,597 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.110472, Batch Acc: 0.188894, Tokens per Sec:    13453, Lr: 0.000300
2025-05-24 14:30:35,597 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:30:35,597 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:30:50,188 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.17, ppl:  23.76, acc:   0.17, generation: 14.5718[sec], evaluation: 0.0000[sec]
2025-05-24 14:30:50,188 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:30:50,270 - INFO - joeynmt.training - Example #0
2025-05-24 14:30:50,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:30:50,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:30:50,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'ben', 'ik', 'ik', 'twee', 'jaar', 'van', 'deze', 'deze', 'deze', 'k@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en@@', 'en', '.', '</s>']
2025-05-24 14:30:50,271 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:30:50,271 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:30:50,271 - INFO - joeynmt.training - 	Hypothesis: Ik ben ik ik twee jaar van deze deze deze kenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenenen .
2025-05-24 14:30:50,271 - INFO - joeynmt.training - Example #1
2025-05-24 14:30:50,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:30:50,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:30:50,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'is', 'de', 'z@@', 'ijk', 'de', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'be@@', 'ste', 'van', 'de', 'de', 'de', 'de', 'de', 'be@@', 'be@@', 'ste', '.', '</s>']
2025-05-24 14:30:50,272 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:30:50,272 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:30:50,272 - INFO - joeynmt.training - 	Hypothesis: Maar dit is de zijk de bebebebebebebebebebebebebeste van de de de de de bebeste .
2025-05-24 14:30:50,272 - INFO - joeynmt.training - Example #2
2025-05-24 14:30:50,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:30:50,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:30:50,272 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'be@@', 'k@@', 'oo@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'd@@', 'ere', 'z@@', 'iek', ',', 'de', 'k@@', 'oo@@', 'd@@', 'ere', 'z@@', 'iek', '.', '</s>']
2025-05-24 14:30:50,272 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:30:50,273 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:30:50,273 - INFO - joeynmt.training - 	Hypothesis: De bekoodddddddddere ziek , de koodere ziek .
2025-05-24 14:30:50,273 - INFO - joeynmt.training - Example #3
2025-05-24 14:30:50,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:30:50,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:30:50,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', '.', '</s>']
2025-05-24 14:30:50,273 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:30:50,273 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:30:50,273 - INFO - joeynmt.training - 	Hypothesis: Het is in in in in in in in in in in in in in in in in in in in in .
2025-05-24 14:30:50,273 - INFO - joeynmt.training - Example #4
2025-05-24 14:30:50,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:30:50,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:30:50,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'z@@', 'a', 'ik', 'je', 'een', 'k@@', 'ijk', 'van', 'een', 'k@@', 'ie@@', 'k@@', 'ie@@', 'den', 'van', 'wat', 'het', 'is', 'wat', 'de', 'wereld', '.', '</s>']
2025-05-24 14:30:50,273 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:30:50,275 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:30:50,275 - INFO - joeynmt.training - 	Hypothesis: De za ik je een kijk van een kiekieden van wat het is wat de wereld .
2025-05-24 14:30:55,332 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.974646, Batch Acc: 0.202170, Tokens per Sec:    14387, Lr: 0.000300
2025-05-24 14:31:00,383 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.919425, Batch Acc: 0.216912, Tokens per Sec:    14817, Lr: 0.000300
2025-05-24 14:31:05,452 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.855251, Batch Acc: 0.232871, Tokens per Sec:    14066, Lr: 0.000300
2025-05-24 14:31:10,558 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.743454, Batch Acc: 0.242571, Tokens per Sec:    14079, Lr: 0.000300
2025-05-24 14:31:15,644 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.588044, Batch Acc: 0.257739, Tokens per Sec:    14515, Lr: 0.000300
2025-05-24 14:31:15,644 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:31:15,644 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:31:29,292 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.86, ppl:  17.44, acc:   0.23, generation: 13.6325[sec], evaluation: 0.0000[sec]
2025-05-24 14:31:29,293 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:31:29,374 - INFO - joeynmt.training - Example #0
2025-05-24 14:31:29,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'ben', 'twee', 'jaar', 'ik', 'deze', 'twee', 'jaar', 'dat', 'twee', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'ïn@@', 'men', '.', '</s>']
2025-05-24 14:31:29,375 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:31:29,375 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:31:29,375 - INFO - joeynmt.training - 	Hypothesis: Ik ben twee jaar ik deze twee jaar dat twee gegegegegegegegegegegegegegegegegegegegegeïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnïnmen .
2025-05-24 14:31:29,375 - INFO - joeynmt.training - Example #1
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'is', 'de', 're@@', 'den@@', 'den@@', 'den@@', 'den@@', 'den@@', 'den@@', 'den@@', 'den@@', 'den@@', 'kt', 'niet', 'niet', 'niet', 'niet', 'niet', 'niet', 'niet', 'niet', '.', '</s>']
2025-05-24 14:31:29,375 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:31:29,375 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:31:29,375 - INFO - joeynmt.training - 	Hypothesis: Maar dit is de redendendendendendendendendenkt niet niet niet niet niet niet niet niet .
2025-05-24 14:31:29,375 - INFO - joeynmt.training - Example #2
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:31:29,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'c@@', 'c@@', 'c@@', 'is', 'een', 'k@@', 'oo@@', 'd@@', 'ere', 'k@@', 'la@@', 's', ',', 'het', 'idee', 'van', 'de', 'mensel@@', 'ijke', 'be@@', 'ste', 'van', 'de', 're@@', 'den@@', 'is', '.', '</s>']
2025-05-24 14:31:29,376 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:31:29,376 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:31:29,376 - INFO - joeynmt.training - 	Hypothesis: De cccis een koodere klas , het idee van de menselijke beste van de redenis .
2025-05-24 14:31:29,376 - INFO - joeynmt.training - Example #3
2025-05-24 14:31:29,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:31:29,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:31:29,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'de', 'tr@@', 'ouw@@', 'd', '.', '</s>']
2025-05-24 14:31:29,376 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:31:29,376 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:31:29,376 - INFO - joeynmt.training - 	Hypothesis: Het is in in in in in in in de trouwd .
2025-05-24 14:31:29,376 - INFO - joeynmt.training - Example #4
2025-05-24 14:31:29,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:31:29,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:31:29,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'eerste', 'eerste', 'van', 'ik', 'je', 'een', 'k@@', 'r@@', 'ond', 'een', 'k@@', 'k@@', 'k@@', 'la@@', '-@@', '-@@', '5', 'jaar', 'van', 'wat', 'de', 'andere', 'jaar', 'jaar', '.', '</s>']
2025-05-24 14:31:29,377 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:31:29,377 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:31:29,377 - INFO - joeynmt.training - 	Hypothesis: De eerste eerste van ik je een krond een kkkla--5 jaar van wat de andere jaar jaar .
2025-05-24 14:31:34,672 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.740826, Batch Acc: 0.264867, Tokens per Sec:    13790, Lr: 0.000300
2025-05-24 14:31:39,733 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.582125, Batch Acc: 0.275415, Tokens per Sec:    14463, Lr: 0.000300
2025-05-24 14:31:44,798 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.627794, Batch Acc: 0.283518, Tokens per Sec:    14668, Lr: 0.000300
2025-05-24 14:31:49,855 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.541751, Batch Acc: 0.298342, Tokens per Sec:    14553, Lr: 0.000300
2025-05-24 14:31:54,922 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.527212, Batch Acc: 0.305099, Tokens per Sec:    13796, Lr: 0.000300
2025-05-24 14:31:54,923 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:31:54,923 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:32:08,716 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.66, ppl:  14.28, acc:   0.27, generation: 13.7802[sec], evaluation: 0.0000[sec]
2025-05-24 14:32:08,716 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:32:08,791 - INFO - joeynmt.training - Example #0
2025-05-24 14:32:08,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:32:08,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:32:08,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'oor', 'ik', 'twee', 'jaar', ',', 'die', 'twee', 'jaar', 'dat', 'dat', 'dat', 'de', 'me@@', 'est@@', 'en', 'die', 'de', 'k@@', 'ar@@', 'c@@', 'ies', 'van', 'de', 'laatste', 'laatste', 'laatste', 'laatste', 'laatste', 'laatste', 'laatste', 'twee', 'miljoen', 'jaar', 'van', 'de', 'laatste', 'laatste', 'laatste', 'miljoen', 'jaar', ',', 'door', 'de', '1@@', '8', 'jaar', '.', '</s>']
2025-05-24 14:32:08,792 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:32:08,792 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:32:08,792 - INFO - joeynmt.training - 	Hypothesis: Voor ik twee jaar , die twee jaar dat dat dat de meesten die de karcies van de laatste laatste laatste laatste laatste laatste laatste twee miljoen jaar van de laatste laatste laatste miljoen jaar , door de 18 jaar .
2025-05-24 14:32:08,792 - INFO - joeynmt.training - Example #1
2025-05-24 14:32:08,792 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:32:08,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:32:08,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'st@@', 'rij@@', 's', 'van', 'deze', 'k@@', 'oo@@', 'd@@', 'd@@', 'wij@@', 's', 'van', 'deze', 'deze', 'deze', 'p@@', 'rij@@', 's', 'omdat', 'het', 'niet', 'de', 'k@@', 'oo@@', 'm@@', 'm@@', 'm@@', 'm@@', 'm@@', 'ati@@', 'st@@', 'st@@', 'st@@', 'eld', '.', '</s>']
2025-05-24 14:32:08,792 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:32:08,792 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:32:08,792 - INFO - joeynmt.training - 	Hypothesis: Maar deze strijs van deze kooddwijs van deze deze deze prijs omdat het niet de koommmmmatistststeld .
2025-05-24 14:32:08,792 - INFO - joeynmt.training - Example #2
2025-05-24 14:32:08,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:32:08,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:32:08,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'c@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'y@@', 'c@@', 'ies', 'van', 'de', 'k@@', 'or@@', 'ale', 'van', 'de', 'k@@', 'or@@', 'p', '.', '</s>']
2025-05-24 14:32:08,793 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:32:08,793 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:32:08,793 - INFO - joeynmt.training - 	Hypothesis: De arccyyyyyyyyycies van de korale van de korp .
2025-05-24 14:32:08,793 - INFO - joeynmt.training - Example #3
2025-05-24 14:32:08,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:32:08,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:32:08,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'in', 'de', 'k@@', 'am@@', 'er', 'in', 'in', 'con@@', 'tr@@', 'ouw@@', 'd', '.', '</s>']
2025-05-24 14:32:08,794 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:32:08,794 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:32:08,794 - INFO - joeynmt.training - 	Hypothesis: Het is in de kamer in in controuwd .
2025-05-24 14:32:08,794 - INFO - joeynmt.training - Example #4
2025-05-24 14:32:08,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:32:08,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:32:08,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'volgende', 'ik', 'je', 'een', 'r@@', 'ond', 'ver@@', 'b@@', 'ouw@@', 'd', 'van', 'wat', 'wat', 'wat', 'wat', 'de', 'laatste', 'laatste', 'laatste', 'laatste', 'laatste', 'laatste', 'jaar', '.', '</s>']
2025-05-24 14:32:08,795 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:32:08,795 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:32:08,795 - INFO - joeynmt.training - 	Hypothesis: De volgende volgende ik je een rond verbouwd van wat wat wat wat de laatste laatste laatste laatste laatste laatste jaar .
2025-05-24 14:32:14,003 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.602013, Batch Acc: 0.309551, Tokens per Sec:    13866, Lr: 0.000300
2025-05-24 14:32:19,465 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.294173, Batch Acc: 0.319237, Tokens per Sec:    13513, Lr: 0.000300
2025-05-24 14:32:24,702 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.276842, Batch Acc: 0.335998, Tokens per Sec:    14194, Lr: 0.000300
2025-05-24 14:32:29,967 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.445557, Batch Acc: 0.345594, Tokens per Sec:    13648, Lr: 0.000300
2025-05-24 14:32:35,179 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.383786, Batch Acc: 0.359401, Tokens per Sec:    14146, Lr: 0.000300
2025-05-24 14:32:35,181 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:32:35,181 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:32:45,551 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.48, ppl:  11.96, acc:   0.32, generation: 10.3553[sec], evaluation: 0.0000[sec]
2025-05-24 14:32:45,551 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:32:45,635 - INFO - joeynmt.training - Example #0
2025-05-24 14:32:45,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:32:45,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:32:45,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'jaar', 'dat', 'deze', 'twee', 'miljoen', 'ik', 'dat', 'de', 'ma@@', 'cht', 'twee', 'miljoen', 'jaar', 'dat', 'de', 'ma@@', 'an@@', 'en', 'dat', 'de', 'k@@', 'oo@@', 'm@@', 'de', 'laatste', 'miljoen', 'jaar', 'van', 'de', 'laatste', 'miljoen', 'jaar', 'heeft', 'heeft', 'de', 'de', 'laatste', 'miljoen', 'jaar', 'van', 'de', 'me@@', 'est@@', 'al', 'van', 'de', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:32:45,635 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:32:45,635 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:32:45,635 - INFO - joeynmt.training - 	Hypothesis: Ik jaar dat deze twee miljoen ik dat de macht twee miljoen jaar dat de maanen dat de koomde laatste miljoen jaar van de laatste miljoen jaar heeft heeft de de laatste miljoen jaar van de meestal van de 48 procent .
2025-05-24 14:32:45,635 - INFO - joeynmt.training - Example #1
2025-05-24 14:32:45,636 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:32:45,636 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:32:45,636 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'ver@@', 'ver@@', 'halen', 'van', 'deze', 'k@@', 'aar@@', 't', ',', 'omdat', 'het', 'niet', 'de', 'gev@@', 'o@@', 'ed', 'omdat', 'het', 'niet', 'de', 'th@@', 'th@@', 'u@@', 'is', '.', '</s>']
2025-05-24 14:32:45,636 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:32:45,636 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:32:45,636 - INFO - joeynmt.training - 	Hypothesis: Maar dit ververhalen van deze kaart , omdat het niet de gevoed omdat het niet de ththuis .
2025-05-24 14:32:45,636 - INFO - joeynmt.training - Example #2
2025-05-24 14:32:45,636 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:32:45,636 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:32:45,636 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'is', 'in', 'een', 'k@@', 'or@@', 'te', ',', 'de', 'k@@', 'aar@@', 't', 'van', 'de', 'de', 'werel@@', 'd@@', 'd@@', 'wij@@', 'de', '.', '</s>']
2025-05-24 14:32:45,636 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:32:45,636 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:32:45,637 - INFO - joeynmt.training - 	Hypothesis: De arctisch is in een korte , de kaart van de de werelddwijde .
2025-05-24 14:32:45,637 - INFO - joeynmt.training - Example #3
2025-05-24 14:32:45,637 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:32:45,637 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:32:45,637 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 's', 'in', 'de', 'vor@@', 'men', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'k@@', 'su@@', 'm', '.', '</s>']
2025-05-24 14:32:45,637 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:32:45,637 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:32:45,637 - INFO - joeynmt.training - 	Hypothesis: Het vers in de vormen en contracten in ksum .
2025-05-24 14:32:45,637 - INFO - joeynmt.training - Example #4
2025-05-24 14:32:45,637 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:32:45,637 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:32:45,637 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'volgende', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'ap@@', 'r@@', 'ap@@', 'ier', ',', 'dan', 'is', 'de', 'laatste', '2@@', '5', 'jaar', 'van', 'wat', 'er', 'gebeur@@', 'de', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:32:45,638 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:32:45,638 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:32:45,638 - INFO - joeynmt.training - 	Hypothesis: De volgende volgende ik jullie een rapaprapier , dan is de laatste 25 jaar van wat er gebeurde 25 jaar .
2025-05-24 14:32:50,873 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.298655, Batch Acc: 0.372929, Tokens per Sec:    13828, Lr: 0.000300
2025-05-24 14:32:56,261 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.170727, Batch Acc: 0.376770, Tokens per Sec:    13670, Lr: 0.000300
2025-05-24 14:33:01,469 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.076726, Batch Acc: 0.380559, Tokens per Sec:    13869, Lr: 0.000300
2025-05-24 14:33:07,042 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.097713, Batch Acc: 0.392725, Tokens per Sec:    13194, Lr: 0.000300
2025-05-24 14:33:12,441 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.225803, Batch Acc: 0.395586, Tokens per Sec:    13915, Lr: 0.000300
2025-05-24 14:33:12,441 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:33:12,442 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:33:22,987 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.35, ppl:  10.48, acc:   0.35, generation: 10.5345[sec], evaluation: 0.0000[sec]
2025-05-24 14:33:22,988 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:33:23,069 - INFO - joeynmt.helpers - delete models/bpe_2k/500.ckpt
2025-05-24 14:33:23,074 - INFO - joeynmt.training - Example #0
2025-05-24 14:33:23,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:33:23,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:33:23,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'v@@', 'ond', 'deze', 'twee', 'jaar', 'dat', 'ik', 'deze', 'twee', 'miljoen', 'die', 'ma@@', 'str@@', 'ie@@', 'ten', 'die', 'de', 'ar@@', 'c@@', 'ij@@', 'f@@', 'te', 'dat', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'de', 'ge@@', 'st@@', 'eld', 'van', 'de', 'groot@@', 'ste', 'van', 'de', 'groot@@', 'ste', 'van', 'de', 'groot@@', 'ste', 'van', 'de', 'groot@@', 'ste', 'van', 'de', 'groot@@', 'ste', 'van', 'de', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:33:23,075 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:33:23,075 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:33:23,075 - INFO - joeynmt.training - 	Hypothesis: Ik vond deze twee jaar dat ik deze twee miljoen die mastrieten die de arcijfte dat de laatste drie miljoen jaar heeft de de gesteld van de grootste van de grootste van de grootste van de grootste van de grootste van de 40 procent .
2025-05-24 14:33:23,075 - INFO - joeynmt.training - Example #1
2025-05-24 14:33:23,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:33:23,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:33:23,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'ver@@', 'ver@@', 'halen', 'van', 'dit', 'be@@', 'w@@', 'u@@', 'st@@', 'ond', ',', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'oo@@', 's', 'niet', 'de', 'z@@', 'u@@', 'u@@', 'id@@', 's@@', 's@@', '-@@', 'ne@@', 'g@@', 'heid', 'van', 'de', 'aar@@', 'de', '.', '</s>']
2025-05-24 14:33:23,076 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:33:23,076 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:33:23,076 - INFO - joeynmt.training - 	Hypothesis: Maar dit ververhalen van dit bewustond , omdat het niet de thickoos niet de zuuidss-negheid van de aarde .
2025-05-24 14:33:23,076 - INFO - joeynmt.training - Example #2
2025-05-24 14:33:23,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:33:23,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:33:23,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'is', ',', 'is', 'in', 'een', 'z@@', 'on', ',', 'de', 'be@@', 'w@@', 'oord', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', '.', '</s>']
2025-05-24 14:33:23,076 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:33:23,076 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:33:23,076 - INFO - joeynmt.training - 	Hypothesis: De arctisch is , is in een zon , de bewoord van de wereldwijde .
2025-05-24 14:33:23,077 - INFO - joeynmt.training - Example #3
2025-05-24 14:33:23,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:33:23,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:33:23,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'chten', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'er', '.', '</s>']
2025-05-24 14:33:23,077 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:33:23,078 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:33:23,078 - INFO - joeynmt.training - 	Hypothesis: Het verwachten in de winter en contracten in zer .
2025-05-24 14:33:23,078 - INFO - joeynmt.training - Example #4
2025-05-24 14:33:23,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:33:23,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:33:23,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'ma@@', 'g', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'ier', 'zijn', ',', 'zal', 'je', 'een', 'r@@', 'ap@@', 'te', 'ver@@', 'der', 'van', 'wat', 'er', 'gebeur@@', 'de', '.', '</s>']
2025-05-24 14:33:23,078 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:33:23,078 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:33:23,078 - INFO - joeynmt.training - 	Hypothesis: De volgende mag ik jullie een rapier zijn , zal je een rapte verder van wat er gebeurde .
2025-05-24 14:33:28,453 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.097539, Batch Acc: 0.407713, Tokens per Sec:    13747, Lr: 0.000300
2025-05-24 14:33:33,703 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.156052, Batch Acc: 0.414900, Tokens per Sec:    14187, Lr: 0.000300
2025-05-24 14:33:38,993 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.137022, Batch Acc: 0.413737, Tokens per Sec:    14274, Lr: 0.000300
2025-05-24 14:33:44,335 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.200484, Batch Acc: 0.420710, Tokens per Sec:    13633, Lr: 0.000300
2025-05-24 14:33:49,440 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.039591, Batch Acc: 0.423358, Tokens per Sec:    14752, Lr: 0.000300
2025-05-24 14:33:49,440 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:33:49,441 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:34:00,289 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.25, ppl:   9.47, acc:   0.38, generation: 10.8347[sec], evaluation: 0.0000[sec]
2025-05-24 14:34:00,289 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:34:00,371 - INFO - joeynmt.helpers - delete models/bpe_2k/1000.ckpt
2025-05-24 14:34:00,376 - INFO - joeynmt.training - Example #0
2025-05-24 14:34:00,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:34:00,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:34:00,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'ik', 'deze', 'twee', 'miljoen', 'ik', 'die', 'twee', 'miljoen', 'jaar', 'zo', 'de@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'en@@', 'kel', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'de', 'groot@@', 'ste', 'van', 'de', 'lo@@', 'g', 'van', 'de', 'lo@@', 'ed', 'van', 'de', 'lo@@', 'g', ',', 'heeft', 'een', 'b@@', 'lo@@', 'k', ',', 'heeft', 'het', 'ver@@', 'der', 'van', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:34:00,377 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:34:00,377 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:34:00,377 - INFO - joeynmt.training - 	Hypothesis: Laat ik deze twee miljoen ik die twee miljoen jaar zo destreren die de arctische enkel , die voor de laatste drie miljoen jaar heeft de de grootste van de log van de loed van de log , heeft een blok , heeft het verder van 40 procent .
2025-05-24 14:34:00,377 - INFO - joeynmt.training - Example #1
2025-05-24 14:34:00,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:34:00,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:34:00,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'int', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'van', 'dit', 'voor@@', 'al', ',', 'omdat', 'het', 'en@@', 'kel', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'e', 'van', 'de', 't@@', 'aal', '.', '</s>']
2025-05-24 14:34:00,377 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:34:00,377 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:34:00,377 - INFO - joeynmt.training - 	Hypothesis: Maar dit begint de serieuze de serieuze van dit vooral , omdat het enkel de thicknee van de taal .
2025-05-24 14:34:00,377 - INFO - joeynmt.training - Example #2
2025-05-24 14:34:00,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:34:00,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:34:00,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'k@@', 'li@@', 'et', 'een', 'gev@@', 'al', 'is', ',', 'in', 'een', 'gev@@', 'oel', ',', 'de', 'be@@', 'pa@@', 'al@@', 'de', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'd', '.', '</s>']
2025-05-24 14:34:00,378 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:34:00,378 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:34:00,378 - INFO - joeynmt.training - 	Hypothesis: De kliet een geval is , in een gevoel , de bepaalde van de wereldwijd .
2025-05-24 14:34:00,378 - INFO - joeynmt.training - Example #3
2025-05-24 14:34:00,378 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:34:00,378 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:34:00,378 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht', 'en', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:34:00,378 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:34:00,378 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:34:00,378 - INFO - joeynmt.training - 	Hypothesis: Het verwacht en en contracten in de summer .
2025-05-24 14:34:00,379 - INFO - joeynmt.training - Example #4
2025-05-24 14:34:00,379 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:34:00,379 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:34:00,379 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'sli@@', 'm', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'i@@', 'me', 'zal', 'een', 'r@@', 'ap@@', 'i@@', 'me', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:34:00,379 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:34:00,379 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:34:00,379 - INFO - joeynmt.training - 	Hypothesis: De volgende slim ik jullie een rapime zal een rapime van wat er gebeurde de 25 jaar .
2025-05-24 14:34:05,459 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.014624, Batch Acc: 0.432782, Tokens per Sec:    14413, Lr: 0.000300
2025-05-24 14:34:10,377 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.018031, Batch Acc: 0.435410, Tokens per Sec:    15169, Lr: 0.000300
2025-05-24 14:34:15,498 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     1.963979, Batch Acc: 0.434359, Tokens per Sec:    14191, Lr: 0.000300
2025-05-24 14:34:20,482 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     1.833946, Batch Acc: 0.442658, Tokens per Sec:    14718, Lr: 0.000300
2025-05-24 14:34:24,898 - INFO - joeynmt.training - Epoch   1: total training loss 10602.77
2025-05-24 14:34:24,898 - INFO - joeynmt.training - EPOCH 2
2025-05-24 14:34:25,648 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     2.036359, Batch Acc: 0.452487, Tokens per Sec:    14720, Lr: 0.000300
2025-05-24 14:34:25,650 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:34:25,650 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:34:36,923 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.20, ppl:   9.01, acc:   0.39, generation: 11.2608[sec], evaluation: 0.0000[sec]
2025-05-24 14:34:36,924 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:34:37,003 - INFO - joeynmt.helpers - delete models/bpe_2k/1500.ckpt
2025-05-24 14:34:37,008 - INFO - joeynmt.training - Example #0
2025-05-24 14:34:37,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:34:37,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:34:37,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'st', 'deze', 'twee', 'twee', 'ma@@', 'at@@', 'ten', 'zo', '&apos;', 'n', 'twee', 'sli@@', 'm', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'ge@@', 'z@@', 'et', ',', 'die', 'voor', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'ed', '4@@', '0', 'jaar', 'ge@@', 'z@@', 'aten', ',', 'die', 'de', 'groot@@', 'te', 'van', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:34:37,010 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:34:37,010 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:34:37,010 - INFO - joeynmt.training - 	Hypothesis: Last deze twee twee maatten zo &apos; n twee slim die de arctische , die de meeste drie miljoen jaar gezet , die voor de grootte van de loed 40 jaar gezaten , die de grootte van 40 procent .
2025-05-24 14:34:37,010 - INFO - joeynmt.training - Example #1
2025-05-24 14:34:37,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:34:37,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:34:37,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'int', 'de', 'ser@@', 'ie@@', 'ie@@', 'u@@', 'ze', 'de', 'be@@', 'pa@@', 'al@@', 'de', 'probleem', 'van', 'deze', 'de@@', 'el@@', 'ijke', 'probleem', ',', 'omdat', 'het', 'niet', 'de', 'z@@', 'en@@', 'ige', '.', '</s>']
2025-05-24 14:34:37,011 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:34:37,011 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:34:37,011 - INFO - joeynmt.training - 	Hypothesis: Maar dit begint de serieieuze de bepaalde probleem van deze deelijke probleem , omdat het niet de zenige .
2025-05-24 14:34:37,011 - INFO - joeynmt.training - Example #2
2025-05-24 14:34:37,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:34:37,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:34:37,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'gev@@', 'al', 'is', ',', 'in', 'een', 'z@@', 'in@@', 'v@@', 'lo@@', 'ed', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'te', 'k@@', 'li@@', 'ma@@', 'at@@', 'te', 'k@@', 'li@@', 'ma@@', 'at@@', 'te', 'k@@', 'li@@', 'ma@@', 'at@@', 'te', 'k@@', 'li@@', 'ma@@', 'at@@', 'te', ',', 'in', 'een', 'z@@', 'in', ',', 'in', 'een', 'z@@', 'in', ',', 'z@@', 'in', 'een', 'z@@', 'in@@', 'k@@', 'ere', 'z@@', 'in', ',', ',', 'is', 'de', 'ge@@', 'z@@', 'in', '.', '</s>']
2025-05-24 14:34:37,011 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:34:37,012 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:34:37,012 - INFO - joeynmt.training - 	Hypothesis: De arctische geval is , in een zinvloed van de wereldwijde van de wereldwijde klimaatte klimaatte klimaatte klimaatte klimaatte , in een zin , in een zin , zin een zinkere zin , , is de gezin .
2025-05-24 14:34:37,012 - INFO - joeynmt.training - Example #3
2025-05-24 14:34:37,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:34:37,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:34:37,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'chten', 'in', 'het', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:34:37,012 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:34:37,012 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:34:37,012 - INFO - joeynmt.training - 	Hypothesis: Het verwachten in het winter en contracten in summer .
2025-05-24 14:34:37,012 - INFO - joeynmt.training - Example #4
2025-05-24 14:34:37,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:34:37,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:34:37,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'sli@@', 'm', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'i@@', 'me', 'voor@@', 't@@', 't@@', 'jes', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:34:37,013 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:34:37,013 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:34:37,013 - INFO - joeynmt.training - 	Hypothesis: De volgende slim ik jullie een rapime voorttjes van wat er gebeurde de de laatste 25 jaar .
2025-05-24 14:34:41,935 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     2.046094, Batch Acc: 0.463538, Tokens per Sec:    14454, Lr: 0.000300
2025-05-24 14:34:46,975 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     1.759510, Batch Acc: 0.457854, Tokens per Sec:    14278, Lr: 0.000300
2025-05-24 14:34:52,213 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     1.683608, Batch Acc: 0.462088, Tokens per Sec:    14299, Lr: 0.000300
2025-05-24 14:34:57,189 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     2.048326, Batch Acc: 0.470274, Tokens per Sec:    14853, Lr: 0.000300
2025-05-24 14:35:02,337 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     1.827579, Batch Acc: 0.472701, Tokens per Sec:    14151, Lr: 0.000300
2025-05-24 14:35:02,337 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:35:02,337 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:35:11,994 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.14, ppl:   8.47, acc:   0.41, generation: 9.6434[sec], evaluation: 0.0000[sec]
2025-05-24 14:35:11,994 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:35:12,076 - INFO - joeynmt.helpers - delete models/bpe_2k/2000.ckpt
2025-05-24 14:35:12,084 - INFO - joeynmt.training - Example #0
2025-05-24 14:35:12,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:35:12,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:35:12,085 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'deze', 'twee', 'jaar', 'lang', 'die', 'ik', 'deze', 'twee', 'sli@@', 'des', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'jaar', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'de', 'ver@@', 'b@@', 'onden', 'van', 'de', 'lo@@', 'ed', '4@@', '0', 'jaar', 'heeft', 'ge@@', 'we@@', 'est', '.', '</s>']
2025-05-24 14:35:12,085 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:35:12,085 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:35:12,085 - INFO - joeynmt.training - 	Hypothesis: Laat deze twee jaar lang die ik deze twee slides zo demonstreren die de arctische jaar die de laatste drie miljoen jaar heeft de de laatste drie miljoen jaar heeft de de verbonden van de loed 40 jaar heeft geweest .
2025-05-24 14:35:12,085 - INFO - joeynmt.training - Example #1
2025-05-24 14:35:12,085 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:35:12,085 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:35:12,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'int', 'de', 'ser@@', 'ie@@', 'us', 'van', 'deze', 'de@@', 'el@@', 'ijke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'n@@', 'oo@@', 'l@@', 's@@', 'ver@@', 's', 'niet', 'de', '.', '</s>']
2025-05-24 14:35:12,086 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:35:12,086 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:35:12,086 - INFO - joeynmt.training - 	Hypothesis: Maar dit begint de serieus van deze deelijke probleem omdat het niet de thicknoolsvers niet de .
2025-05-24 14:35:12,086 - INFO - joeynmt.training - Example #2
2025-05-24 14:35:12,086 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:35:12,086 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:35:12,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'keer', 'is', ',', 'in', 'een', 'z@@', 'at', ',', 'de', 'ver@@', 'tr@@', 'ouw@@', 't', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:35:12,086 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:35:12,086 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:35:12,086 - INFO - joeynmt.training - 	Hypothesis: De arctische keer is , in een zat , de vertrouwt van de wereldklimaatsysteem .
2025-05-24 14:35:12,087 - INFO - joeynmt.training - Example #3
2025-05-24 14:35:12,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:35:12,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:35:12,087 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'chten', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:35:12,087 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:35:12,087 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:35:12,087 - INFO - joeynmt.training - 	Hypothesis: Het verwachten in winter en contracten in summer .
2025-05-24 14:35:12,087 - INFO - joeynmt.training - Example #4
2025-05-24 14:35:12,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:35:12,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:35:12,087 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'sli@@', 'ste', 'sli@@', 'm', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'se', ',', 'die', 'de', 'laatste', '2@@', '5', 'jaar', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:35:12,087 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:35:12,087 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:35:12,087 - INFO - joeynmt.training - 	Hypothesis: De volgende sliste slim zal ik een rapid fase , die de laatste 25 jaar de laatste 25 jaar .
2025-05-24 14:35:17,112 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     1.886358, Batch Acc: 0.467085, Tokens per Sec:    14418, Lr: 0.000300
2025-05-24 14:35:22,134 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     1.806007, Batch Acc: 0.471342, Tokens per Sec:    14728, Lr: 0.000300
2025-05-24 14:35:27,251 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     1.750696, Batch Acc: 0.474776, Tokens per Sec:    14527, Lr: 0.000300
2025-05-24 14:35:32,564 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     1.833915, Batch Acc: 0.471370, Tokens per Sec:    13816, Lr: 0.000300
2025-05-24 14:35:37,835 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     1.873753, Batch Acc: 0.472129, Tokens per Sec:    13810, Lr: 0.000300
2025-05-24 14:35:37,836 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:35:37,836 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:35:48,900 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.09, ppl:   8.08, acc:   0.42, generation: 11.0522[sec], evaluation: 0.0000[sec]
2025-05-24 14:35:48,901 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:35:48,983 - INFO - joeynmt.helpers - delete models/bpe_2k/2500.ckpt
2025-05-24 14:35:48,987 - INFO - joeynmt.training - Example #0
2025-05-24 14:35:48,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:35:48,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:35:48,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'deze', 'twee', 'jaar', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'zo', '&apos;', 'n', '4@@', '8', 'jaar', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'groot@@', 'te', 'van', 'de', 'z@@', 'aten', 'van', 'de', 'lo@@', 'ed', '4@@', '8', 'jaar', 'heeft', 'heeft', 'ge@@', 'we@@', 'est', '.', '</s>']
2025-05-24 14:35:48,988 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:35:48,988 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:35:48,988 - INFO - joeynmt.training - 	Hypothesis: Ik heb deze twee jaar zo demonstreren zo &apos; n 48 jaar dat de arctische ijs de laatste drie miljoen jaar heeft de grootte van de zaten van de loed 48 jaar heeft heeft geweest .
2025-05-24 14:35:48,988 - INFO - joeynmt.training - Example #1
2025-05-24 14:35:48,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:35:48,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:35:48,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'int', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'van', 'dit', 'de@@', 'el@@', 'te', 'niet', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'n@@', 'e', 'van', 'de', 'ij@@', 's', 'niet', 'de', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:35:48,989 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:35:48,989 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:35:48,989 - INFO - joeynmt.training - 	Hypothesis: Maar dit begint de serieuze van dit deelte niet omdat het niet de thickne van de ijs niet de ijs van het ijs .
2025-05-24 14:35:48,989 - INFO - joeynmt.training - Example #2
2025-05-24 14:35:48,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:35:48,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:35:48,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'is', 'in', 'een', 'z@@', 'in', ',', 'het', 'ver@@', 'b@@', 'ru@@', 'ru@@', 'g', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:35:48,989 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:35:48,989 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:35:48,989 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs is in een zin , het verbrurug van de wereldklimaat .
2025-05-24 14:35:48,989 - INFO - joeynmt.training - Example #3
2025-05-24 14:35:48,989 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:35:48,989 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:35:48,989 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht@@', 's@@', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:35:48,990 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:35:48,990 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:35:48,990 - INFO - joeynmt.training - 	Hypothesis: Het verwachtsin winter en contracten in summer .
2025-05-24 14:35:48,990 - INFO - joeynmt.training - Example #4
2025-05-24 14:35:48,990 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:35:48,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:35:48,991 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'sli@@', 'm', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'i@@', 'me', 'zal', 'een', 'r@@', 'ap@@', 'i@@', 'eren', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:35:48,991 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:35:48,991 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:35:48,991 - INFO - joeynmt.training - 	Hypothesis: De volgende slim zal ik een rapime zal een rapieren van wat er gebeurde de laatste 25 jaar .
2025-05-24 14:35:54,231 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     1.921419, Batch Acc: 0.476731, Tokens per Sec:    13707, Lr: 0.000300
2025-05-24 14:35:59,458 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     1.910133, Batch Acc: 0.480084, Tokens per Sec:    14178, Lr: 0.000300
2025-05-24 14:36:04,616 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.884356, Batch Acc: 0.482763, Tokens per Sec:    14386, Lr: 0.000300
2025-05-24 14:36:09,680 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.872709, Batch Acc: 0.492041, Tokens per Sec:    14787, Lr: 0.000300
2025-05-24 14:36:14,928 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.913496, Batch Acc: 0.485546, Tokens per Sec:    13957, Lr: 0.000300
2025-05-24 14:36:14,928 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:36:14,928 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:36:25,890 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.06, ppl:   7.84, acc:   0.42, generation: 10.9477[sec], evaluation: 0.0000[sec]
2025-05-24 14:36:25,890 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:36:25,971 - INFO - joeynmt.helpers - delete models/bpe_2k/3000.ckpt
2025-05-24 14:36:25,977 - INFO - joeynmt.training - Example #0
2025-05-24 14:36:25,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:36:25,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:36:25,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'st', 'ik', 'deze', 'twee', 'dia', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'zo', '&apos;', 'n', 'k@@', 'ti@@', 'sch', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'groot@@', 'ste', 'van', 'de', 'oor@@', 'lo@@', 'g', '4@@', '8', 'jaar', 'heeft', 'ge@@', 'we@@', 'est', '.', '</s>']
2025-05-24 14:36:25,978 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:36:25,978 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:36:25,978 - INFO - joeynmt.training - 	Hypothesis: Last ik deze twee dia zo demonstreren die de arctische ijs zo &apos; n ktisch , die de meeste drie miljoen jaar heeft de grootste van de oorlog 48 jaar heeft geweest .
2025-05-24 14:36:25,978 - INFO - joeynmt.training - Example #1
2025-05-24 14:36:25,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:36:25,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:36:25,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'onnen', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'van', 'deze', 'be@@', 'pa@@', 'al@@', 'de', 'probleem', 'omdat', 'het', 'niet', 'zien', 'zien', ',', 'want', 'het', 'niet', 'zien', 'zien', '.', '</s>']
2025-05-24 14:36:25,979 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:36:25,979 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:36:25,979 - INFO - joeynmt.training - 	Hypothesis: Maar dit begonnen de serieuze van deze bepaalde probleem omdat het niet zien zien , want het niet zien zien .
2025-05-24 14:36:25,979 - INFO - joeynmt.training - Example #2
2025-05-24 14:36:25,979 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:36:25,979 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:36:25,979 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'li@@', 'ma@@', 'at@@', 'schapp@@', 'el@@', 'ijke', 'k@@', 'li@@', 'ma@@', 'at@@', 'schapp@@', 'en', '.', '</s>']
2025-05-24 14:36:25,979 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:36:25,979 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:36:25,979 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs klimaatschappelijke klimaatschappen .
2025-05-24 14:36:25,979 - INFO - joeynmt.training - Example #3
2025-05-24 14:36:25,980 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:36:25,980 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:36:25,980 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:36:25,980 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:36:25,980 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:36:25,980 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in summer .
2025-05-24 14:36:25,980 - INFO - joeynmt.training - Example #4
2025-05-24 14:36:25,980 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:36:25,980 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:36:25,980 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', ',', 'zal', 'een', 'r@@', 'ap@@', 'i@@', '-@@', 'kra@@', 'cht', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:36:25,981 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:36:25,981 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:36:25,981 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik een rapid laten zien , zal een rapi-kracht van wat er gebeurde de de laatste 25 jaar .
2025-05-24 14:36:31,101 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.048488, Batch Acc: 0.487029, Tokens per Sec:    14100, Lr: 0.000300
2025-05-24 14:36:36,186 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.829624, Batch Acc: 0.490421, Tokens per Sec:    14404, Lr: 0.000300
2025-05-24 14:36:41,275 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.706964, Batch Acc: 0.491913, Tokens per Sec:    14657, Lr: 0.000300
2025-05-24 14:36:46,420 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.715014, Batch Acc: 0.490514, Tokens per Sec:    14057, Lr: 0.000300
2025-05-24 14:36:51,530 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.072497, Batch Acc: 0.498221, Tokens per Sec:    14687, Lr: 0.000300
2025-05-24 14:36:51,530 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:36:51,530 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:37:00,043 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.03, ppl:   7.61, acc:   0.43, generation: 8.5025[sec], evaluation: 0.0000[sec]
2025-05-24 14:37:00,043 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:37:00,127 - INFO - joeynmt.helpers - delete models/bpe_2k/3500.ckpt
2025-05-24 14:37:00,134 - INFO - joeynmt.training - Example #0
2025-05-24 14:37:00,134 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:37:00,134 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:37:00,134 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'ik', 'deze', 'twee', 'dia', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sch', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'groot@@', 'ste', 'van', 'de', 'lo@@', 'gen', '4@@', '8', 'jaar', 'heeft', 'ge@@', 'we@@', 'est', '.', '</s>']
2025-05-24 14:37:00,134 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:37:00,134 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:37:00,134 - INFO - joeynmt.training - 	Hypothesis: Laat ik deze twee dia zo dat de arctisch demonstreren die de arctisch ijs , die de meeste drie miljoen jaar heeft de grootste van de logen 48 jaar heeft geweest .
2025-05-24 14:37:00,134 - INFO - joeynmt.training - Example #1
2025-05-24 14:37:00,134 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:37:00,134 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:37:00,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'onder@@', 'onder@@', 'onder@@', 'wer@@', 'p', 'van', 'dit', 'speci@@', 'f@@', 'iek', 'niet', 'te', 'laten', 'zien', 'omdat', 'het', 'd@@', 'oo@@', 'd@@', 's@@', '-@@', 'probleem', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:37:00,135 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:37:00,135 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:37:00,135 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderonderonderonderwerp van dit specifiek niet te laten zien omdat het doods-probleem van de ijs .
2025-05-24 14:37:00,135 - INFO - joeynmt.training - Example #2
2025-05-24 14:37:00,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:37:00,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:37:00,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'ree@@', 'g', ',', 'in', 'een', 'gev@@', 'oel', 'van', 'de', 'glob@@', 'ale', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:37:00,136 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:37:00,136 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:37:00,136 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kreeg , in een gevoel van de globale klimaatsysteem .
2025-05-24 14:37:00,136 - INFO - joeynmt.training - Example #3
2025-05-24 14:37:00,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:37:00,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:37:00,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'het', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:37:00,136 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:37:00,137 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:37:00,137 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in het winter en contracten in summer .
2025-05-24 14:37:00,137 - INFO - joeynmt.training - Example #4
2025-05-24 14:37:00,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:37:00,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:37:00,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'voor@@', 'uit@@', 'gan@@', 'g', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:37:00,137 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:37:00,137 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:37:00,137 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal je een rapid zal een rapid fast-vooruitgang van wat er gebeurd is .
2025-05-24 14:37:05,138 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.833561, Batch Acc: 0.498705, Tokens per Sec:    14544, Lr: 0.000300
2025-05-24 14:37:10,223 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.962768, Batch Acc: 0.502982, Tokens per Sec:    14643, Lr: 0.000300
2025-05-24 14:37:15,316 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     1.881783, Batch Acc: 0.495428, Tokens per Sec:    14346, Lr: 0.000300
2025-05-24 14:37:20,332 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.957953, Batch Acc: 0.501260, Tokens per Sec:    14641, Lr: 0.000300
2025-05-24 14:37:25,353 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.079912, Batch Acc: 0.503568, Tokens per Sec:    14374, Lr: 0.000300
2025-05-24 14:37:25,353 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:37:25,353 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:37:36,069 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.44, acc:   0.44, generation: 10.7031[sec], evaluation: 0.0000[sec]
2025-05-24 14:37:36,070 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:37:36,149 - INFO - joeynmt.helpers - delete models/bpe_2k/4000.ckpt
2025-05-24 14:37:36,155 - INFO - joeynmt.training - Example #0
2025-05-24 14:37:36,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:37:36,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:37:36,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'deze', 'twee', 'dia', 'zo', '&apos;', 'n', 'twee', 'sli@@', 'des', 'zo', '&apos;', 'n', 'twee', 'keer', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'meest', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'z@@', 'in@@', 'nen', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:37:36,159 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:37:36,159 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:37:36,159 - INFO - joeynmt.training - 	Hypothesis: Laat deze twee dia zo &apos; n twee slides zo &apos; n twee keer dat de arctische ijs de laatste drie miljoen jaar de meest drie miljoen jaar heeft de zinnen 40 procent .
2025-05-24 14:37:36,159 - INFO - joeynmt.training - Example #1
2025-05-24 14:37:36,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:37:36,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:37:36,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'probleem', 'van', 'deze', 'speci@@', 'f@@', 'iek', 'probleem', 'omdat', 'het', 'het', 'd@@', 'oel', 'is', 'van', 'de', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:37:36,159 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:37:36,159 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:37:36,160 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieuze probleem van deze specifiek probleem omdat het het doel is van de ijs van het ijs .
2025-05-24 14:37:36,160 - INFO - joeynmt.training - Example #2
2025-05-24 14:37:36,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:37:36,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:37:36,160 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'ver@@', 'b@@', 'oel', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:37:36,160 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:37:36,160 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:37:36,160 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kap is , in een zin , het verboel van de wereldklimaatsysteem .
2025-05-24 14:37:36,160 - INFO - joeynmt.training - Example #3
2025-05-24 14:37:36,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:37:36,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:37:36,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht@@', 'ingen', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'het', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:37:36,161 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:37:36,161 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:37:36,161 - INFO - joeynmt.training - 	Hypothesis: Het verwachtingen in winter en contracten in het zomer .
2025-05-24 14:37:36,161 - INFO - joeynmt.training - Example #4
2025-05-24 14:37:36,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:37:36,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:37:36,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'i@@', 'er@@', 'kra@@', 'cht', 'zal', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'de', 'dan', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:37:36,162 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:37:36,162 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:37:36,162 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapierkracht zal zien van wat er gebeurde dan 25 jaar .
2025-05-24 14:37:41,343 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.603003, Batch Acc: 0.505941, Tokens per Sec:    14027, Lr: 0.000300
2025-05-24 14:37:46,733 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     1.622957, Batch Acc: 0.503461, Tokens per Sec:    13917, Lr: 0.000300
2025-05-24 14:37:52,002 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.779768, Batch Acc: 0.503800, Tokens per Sec:    13488, Lr: 0.000300
2025-05-24 14:37:57,279 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     1.780172, Batch Acc: 0.507239, Tokens per Sec:    13510, Lr: 0.000300
2025-05-24 14:38:02,605 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.724003, Batch Acc: 0.504836, Tokens per Sec:    13787, Lr: 0.000300
2025-05-24 14:38:02,605 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:38:02,606 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:38:13,578 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.31, acc:   0.44, generation: 10.9590[sec], evaluation: 0.0000[sec]
2025-05-24 14:38:13,578 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:38:13,663 - INFO - joeynmt.helpers - delete models/bpe_2k/4500.ckpt
2025-05-24 14:38:13,669 - INFO - joeynmt.training - Example #0
2025-05-24 14:38:13,670 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:38:13,670 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:38:13,670 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'dit', 'jaar', 'ver@@', 'b@@', 'onden', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'c@@', 'ij@@', 'f@@', 'ers', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'ge@@', 'we@@', 'est', 'van', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'is', 'de', 'ver@@', 'z@@', 'am@@', 'el@@', 'ing', ',', 'heeft', 'een', 'aantal', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:38:13,670 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:38:13,671 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:38:13,671 - INFO - joeynmt.training - 	Hypothesis: Laat dit jaar verbonden ik deze twee dia &apos; s zo &apos; n cijfers die de meeste drie miljoen jaar de geweest van de afgelopen drie miljoen jaar is de verzameling , heeft een aantal 48 procent .
2025-05-24 14:38:13,671 - INFO - joeynmt.training - Example #1
2025-05-24 14:38:13,671 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:38:13,671 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:38:13,671 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'us', 'van', 'deze', 'bij@@', 'zonder', 'probleem', ',', 'want', 'het', 'is', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'e', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:38:13,671 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:38:13,671 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:38:13,671 - INFO - joeynmt.training - 	Hypothesis: Maar deze begreep de serieus van deze bijzonder probleem , want het is het niet de thicknee van de ijs .
2025-05-24 14:38:13,672 - INFO - joeynmt.training - Example #2
2025-05-24 14:38:13,672 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:38:13,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:38:13,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 't@@', 'ief', 'gev@@', 'aar@@', 'lijk', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:38:13,672 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:38:13,672 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:38:13,672 - INFO - joeynmt.training - 	Hypothesis: De arctief gevaarlijk is , in een zin , de klimaatsysteem .
2025-05-24 14:38:13,672 - INFO - joeynmt.training - Example #3
2025-05-24 14:38:13,672 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:38:13,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:38:13,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:38:13,673 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:38:13,673 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:38:13,673 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:38:13,673 - INFO - joeynmt.training - Example #4
2025-05-24 14:38:13,673 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:38:13,673 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:38:13,673 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'voor@@', 'uit@@', 'gan@@', 'g', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:38:13,674 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:38:13,674 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:38:13,674 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid zijn vooruitgang van wat er gebeurde de de de laatste 25 jaar .
2025-05-24 14:38:18,970 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.763592, Batch Acc: 0.508443, Tokens per Sec:    13551, Lr: 0.000300
2025-05-24 14:38:24,284 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.697219, Batch Acc: 0.511875, Tokens per Sec:    13799, Lr: 0.000300
2025-05-24 14:38:29,491 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.778537, Batch Acc: 0.513864, Tokens per Sec:    14169, Lr: 0.000300
2025-05-24 14:38:34,627 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.692738, Batch Acc: 0.513771, Tokens per Sec:    14535, Lr: 0.000300
2025-05-24 14:38:39,808 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.687691, Batch Acc: 0.517632, Tokens per Sec:    13989, Lr: 0.000300
2025-05-24 14:38:39,808 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:38:39,808 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:38:49,802 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.16, acc:   0.45, generation: 9.9824[sec], evaluation: 0.0000[sec]
2025-05-24 14:38:49,802 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:38:49,876 - INFO - joeynmt.helpers - delete models/bpe_2k/5000.ckpt
2025-05-24 14:38:49,881 - INFO - joeynmt.training - Example #0
2025-05-24 14:38:49,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:38:49,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:38:49,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'oor', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ma@@', 'at@@', 'en@@', '-@@', 'ij@@', 's', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'is', 'de', 'z@@', 'om@@', 'ing', 'van', 'de', 'l@@', 'on@@', 'k', 'van', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:38:49,882 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:38:49,882 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:38:49,882 - INFO - joeynmt.training - 	Hypothesis: Voor deze twee dia &apos; s zo &apos; n twee dia &apos; s zo &apos; n maaten-ijs , die voor de laatste drie miljoen jaar is de grootte van de laatste drie miljoen jaar is de zoming van de lonk van 40 % .
2025-05-24 14:38:49,882 - INFO - joeynmt.training - Example #1
2025-05-24 14:38:49,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:38:49,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:38:49,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie', 'van', 'deze', 'bij@@', 'zonder', 'probleem', 'van', 'deze', 'bij@@', 'zonder', 'probleem', 'omdat', 'het', 'd@@', 'rij@@', 's', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'n@@', 'is', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:38:49,882 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:38:49,883 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:38:49,883 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serie van deze bijzonder probleem van deze bijzonder probleem omdat het drijs niet de thicknis van de ijs .
2025-05-24 14:38:49,883 - INFO - joeynmt.training - Example #2
2025-05-24 14:38:49,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:38:49,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:38:49,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'ti@@', 'en@@', 's', 'is', ',', 'de', 'be@@', 'per@@', 'kt', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:38:49,883 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:38:49,883 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:38:49,883 - INFO - joeynmt.training - 	Hypothesis: De arctische ktiens is , de beperkt van de wereldklimaat .
2025-05-24 14:38:49,883 - INFO - joeynmt.training - Example #3
2025-05-24 14:38:49,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:38:49,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:38:49,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:38:49,884 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:38:49,884 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:38:49,884 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:38:49,884 - INFO - joeynmt.training - Example #4
2025-05-24 14:38:49,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:38:49,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:38:49,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'de', 'is', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:38:49,884 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:38:49,884 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:38:49,884 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurde is in de laatste 25 jaar .
2025-05-24 14:38:54,995 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.625282, Batch Acc: 0.512167, Tokens per Sec:    13817, Lr: 0.000300
2025-05-24 14:39:00,111 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.853672, Batch Acc: 0.512921, Tokens per Sec:    13973, Lr: 0.000300
2025-05-24 14:39:05,284 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.604314, Batch Acc: 0.520309, Tokens per Sec:    14331, Lr: 0.000300
2025-05-24 14:39:10,405 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.808837, Batch Acc: 0.514335, Tokens per Sec:    14075, Lr: 0.000300
2025-05-24 14:39:14,541 - INFO - joeynmt.training - Epoch   2: total training loss 7225.01
2025-05-24 14:39:14,541 - INFO - joeynmt.training - EPOCH 3
2025-05-24 14:39:15,462 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.737907, Batch Acc: 0.546191, Tokens per Sec:    14865, Lr: 0.000300
2025-05-24 14:39:15,463 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:39:15,463 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:39:24,726 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.11, acc:   0.45, generation: 9.2515[sec], evaluation: 0.0000[sec]
2025-05-24 14:39:24,726 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:39:24,803 - INFO - joeynmt.helpers - delete models/bpe_2k/5500.ckpt
2025-05-24 14:39:24,809 - INFO - joeynmt.training - Example #0
2025-05-24 14:39:24,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:39:24,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:39:24,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'st', 'het', 'jaar', 'to@@', 'on@@', 'de', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ma@@', 'at@@', 'te', 'k@@', 'ie@@', 'ten', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', 'van', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'g@@', 'ers', ',', 'heeft', 'de', 'groot@@', 'te', 'van', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:39:24,809 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:39:24,809 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:39:24,809 - INFO - joeynmt.training - 	Hypothesis: Last het jaar toonde deze twee dia &apos; s zo &apos; n maatte kieten die de arctische ktische kap van de afgelopen drie miljoen jaar is de grootte van de logers , heeft de grootte van 40 procent .
2025-05-24 14:39:24,809 - INFO - joeynmt.training - Example #1
2025-05-24 14:39:24,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:39:24,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:39:24,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'at@@', 'ten', 'van', 'dit', 'bij@@', 'zonder', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 's', 'de', 'dr@@', 'ic@@', 'k@@', 'a@@', 'k@@', 'ne@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:39:24,810 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:39:24,810 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:39:24,810 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderschatten van dit bijzonder probleem omdat het niet de thicknes de drickaknes van het ijs .
2025-05-24 14:39:24,810 - INFO - joeynmt.training - Example #2
2025-05-24 14:39:24,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:39:24,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:39:24,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'oo@@', 'p', 'is', ',', 'in', 'een', 'gev@@', 'oel', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'op@@', 'pen', 'van', 'de', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:39:24,811 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:39:24,811 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:39:24,811 - INFO - joeynmt.training - 	Hypothesis: De arctische koop is , in een gevoel van de wereldkoppen van de klimaat .
2025-05-24 14:39:24,811 - INFO - joeynmt.training - Example #3
2025-05-24 14:39:24,811 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:39:24,811 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:39:24,811 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'su@@', 'mm@@', 'er', '.', '</s>']
2025-05-24 14:39:24,811 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:39:24,812 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:39:24,812 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in summer .
2025-05-24 14:39:24,812 - INFO - joeynmt.training - Example #4
2025-05-24 14:39:24,812 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:39:24,812 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:39:24,812 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:39:24,812 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:39:24,812 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:39:24,812 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurd is gebeurd gebeurd is .
2025-05-24 14:39:29,847 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.794815, Batch Acc: 0.530046, Tokens per Sec:    14048, Lr: 0.000300
2025-05-24 14:39:35,185 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.642168, Batch Acc: 0.536218, Tokens per Sec:    13751, Lr: 0.000300
2025-05-24 14:39:40,353 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.649551, Batch Acc: 0.534368, Tokens per Sec:    14742, Lr: 0.000300
2025-05-24 14:39:45,554 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.659169, Batch Acc: 0.535450, Tokens per Sec:    14591, Lr: 0.000300
2025-05-24 14:39:50,581 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.627285, Batch Acc: 0.538664, Tokens per Sec:    14412, Lr: 0.000300
2025-05-24 14:39:50,582 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:39:50,582 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:39:59,554 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.82, acc:   0.46, generation: 8.9617[sec], evaluation: 0.0000[sec]
2025-05-24 14:39:59,555 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:39:59,631 - INFO - joeynmt.helpers - delete models/bpe_2k/6000.ckpt
2025-05-24 14:39:59,636 - INFO - joeynmt.training - Example #0
2025-05-24 14:39:59,637 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:39:59,637 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:39:59,637 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ma@@', 'at@@', 'schapp@@', 'el@@', 'ijke', 'gev@@', 'al', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'heeft', 'de', 'groot@@', 'te', 'van', 'de', 'z@@', 'in@@', 'tu@@', 'i@@', 'gen', 'van', 'de', 'lo@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:39:59,637 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:39:59,637 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:39:59,637 - INFO - joeynmt.training - 	Hypothesis: Laat deze twee dia &apos; s zo &apos; n twee dia &apos; s zo &apos; n maatschappelijke geval , die voor de laatste drie miljoen jaar heeft de grootte van de zintuigen van de loger 40 procent .
2025-05-24 14:39:59,637 - INFO - joeynmt.training - Example #1
2025-05-24 14:39:59,637 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:39:59,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:39:59,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'van', 'de', 'ser@@', 'i@@', 'ou@@', 's@@', 'heid', 'van', 'deze', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:39:59,638 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:39:59,638 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:39:59,638 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten van de seriousheid van deze specifieke probleem omdat het niet de thickness van het ijs .
2025-05-24 14:39:59,638 - INFO - joeynmt.training - Example #2
2025-05-24 14:39:59,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:39:59,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:39:59,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'k@@', 'or@@', 'te', 'k@@', 'aar@@', 't', ',', 'de', 'ver@@', 'b@@', 'ru@@', 'i@@', 'kt', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:39:59,639 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:39:59,639 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:39:59,639 - INFO - joeynmt.training - 	Hypothesis: De arctisch korte kaart , de verbruikt van de wereldklimaatsysteem .
2025-05-24 14:39:59,639 - INFO - joeynmt.training - Example #3
2025-05-24 14:39:59,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:39:59,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:39:59,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:39:59,639 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:39:59,640 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:39:59,640 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:39:59,640 - INFO - joeynmt.training - Example #4
2025-05-24 14:39:59,640 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:39:59,640 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:39:59,640 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'i@@', 'p', 'een', 'r@@', 'ap@@', 'i@@', 'p@@', 'pen', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:39:59,640 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:39:59,640 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:39:59,640 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik een rapip een rapippen zijn van wat er gebeurde de de laatste 25 jaar .
2025-05-24 14:40:04,738 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.697415, Batch Acc: 0.535113, Tokens per Sec:    14022, Lr: 0.000300
2025-05-24 14:40:09,776 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.615686, Batch Acc: 0.534743, Tokens per Sec:    14824, Lr: 0.000300
2025-05-24 14:40:14,942 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.557865, Batch Acc: 0.540353, Tokens per Sec:    14305, Lr: 0.000300
2025-05-24 14:40:20,048 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.579870, Batch Acc: 0.535983, Tokens per Sec:    14705, Lr: 0.000300
2025-05-24 14:40:25,045 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.739723, Batch Acc: 0.530881, Tokens per Sec:    14442, Lr: 0.000300
2025-05-24 14:40:25,045 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:40:25,045 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:40:33,766 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.78, acc:   0.46, generation: 8.7081[sec], evaluation: 0.0000[sec]
2025-05-24 14:40:33,767 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:40:33,844 - INFO - joeynmt.helpers - delete models/bpe_2k/6500.ckpt
2025-05-24 14:40:33,849 - INFO - joeynmt.training - Example #0
2025-05-24 14:40:33,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:40:33,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:40:33,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'st', 'dit', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ma@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'ver@@', 'gel@@', 'ijk@@', 'ste', 'van', 'de', 'l@@', 'oe@@', 'd@@', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'de@@', 'el@@', 'te', ',', 'heeft', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:40:33,849 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:40:33,850 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:40:33,850 - INFO - joeynmt.training - 	Hypothesis: Last dit jaar schoed ik deze twee dia &apos; s zo &apos; n mastreren die de arctisch ijs kaart , die voor de laatste drie miljoen jaar de vergelijkste van de loedstaten , heeft gedeelte , heeft door 40 procent .
2025-05-24 14:40:33,850 - INFO - joeynmt.training - Example #1
2025-05-24 14:40:33,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:40:33,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:40:33,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'van', 'deze', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'oel', 'van', 'de', 'ij@@', 's', 'niet', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:40:33,850 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:40:33,850 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:40:33,850 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieuze van deze specifieke probleem omdat het niet de doel van de ijs niet van de ijs .
2025-05-24 14:40:33,850 - INFO - joeynmt.training - Example #2
2025-05-24 14:40:33,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:40:33,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:40:33,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'in', 'een', 'z@@', 'in', ',', 'de', 'ged@@', 'ur@@', 'ende', 'het', 'werel@@', 'd@@', 'wij@@', 'd', 'k@@', 'li@@', 'ma@@', 'at@@', 'schapp@@', 'ij', '.', '</s>']
2025-05-24 14:40:33,852 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:40:33,852 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:40:33,852 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kaart , in een zin , de gedurende het wereldwijd klimaatschappij .
2025-05-24 14:40:33,852 - INFO - joeynmt.training - Example #3
2025-05-24 14:40:33,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:40:33,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:40:33,852 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:40:33,852 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:40:33,852 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:40:33,852 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in de winter en contracten in de zomer .
2025-05-24 14:40:33,852 - INFO - joeynmt.training - Example #4
2025-05-24 14:40:33,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:40:33,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:40:33,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zal', 'een', 'r@@', 'ap@@', 'id', 'worden', 'voor@@', 'uit', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:40:33,853 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:40:33,853 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:40:33,853 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid zal een rapid worden vooruit de laatste 25 jaar .
2025-05-24 14:40:38,783 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.615317, Batch Acc: 0.537306, Tokens per Sec:    14544, Lr: 0.000300
2025-05-24 14:40:43,792 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.544596, Batch Acc: 0.535383, Tokens per Sec:    14870, Lr: 0.000300
2025-05-24 14:40:48,839 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.724740, Batch Acc: 0.540789, Tokens per Sec:    14748, Lr: 0.000300
2025-05-24 14:40:53,971 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.650571, Batch Acc: 0.541002, Tokens per Sec:    14167, Lr: 0.000300
2025-05-24 14:40:59,080 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.673520, Batch Acc: 0.542056, Tokens per Sec:    14254, Lr: 0.000300
2025-05-24 14:40:59,081 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:40:59,081 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:41:09,233 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.69, acc:   0.47, generation: 10.1409[sec], evaluation: 0.0000[sec]
2025-05-24 14:41:09,233 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:41:09,309 - INFO - joeynmt.helpers - delete models/bpe_2k/7000.ckpt
2025-05-24 14:41:09,314 - INFO - joeynmt.training - Example #0
2025-05-24 14:41:09,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:41:09,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:41:09,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'die', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'k@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'ze', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'h@@', 'oord', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:41:09,315 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:41:09,315 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:41:09,315 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoed ik die deze twee dia &apos; s zo &apos; n ktische ijs kaart , die voor de laatste drie miljoen jaar de grootte van de loze van de lagere 48 staten , heeft gehoord door 40 procent .
2025-05-24 14:41:09,315 - INFO - joeynmt.training - Example #1
2025-05-24 14:41:09,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:41:09,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:41:09,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'un@@', 't', 'de', 'het', 'ser@@', 'ie@@', '-@@', 'probleem', 'van', 'dit', 'speci@@', 'f@@', 'iek', 'omdat', 'het', 'het', 'niet', 'de', 'dr@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:41:09,315 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:41:09,315 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:41:09,315 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteunt de het serie-probleem van dit specifiek omdat het het niet de drickness van het ijs .
2025-05-24 14:41:09,316 - INFO - joeynmt.training - Example #2
2025-05-24 14:41:09,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:41:09,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:41:09,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'ver@@', 'der', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:41:09,316 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:41:09,316 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:41:09,316 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart in een gevoel , het verder van de wereldklimaatsysteem .
2025-05-24 14:41:09,316 - INFO - joeynmt.training - Example #3
2025-05-24 14:41:09,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:41:09,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:41:09,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:41:09,317 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:41:09,317 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:41:09,317 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:41:09,317 - INFO - joeynmt.training - Example #4
2025-05-24 14:41:09,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:41:09,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:41:09,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'id', 'zal', 'een', 'r@@', 'ap@@', 'id', 'voor@@', 'uit', 'wat', 'er', 'gebeur@@', 'd', 'is', ',', 'is', 'er', 'zo', '&apos;', 'n', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:41:09,317 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:41:09,317 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:41:09,317 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik een rapid zal een rapid vooruit wat er gebeurd is , is er zo &apos; n 25 jaar .
2025-05-24 14:41:14,334 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.446472, Batch Acc: 0.541072, Tokens per Sec:    14157, Lr: 0.000300
2025-05-24 14:41:19,339 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.629755, Batch Acc: 0.545365, Tokens per Sec:    14949, Lr: 0.000300
2025-05-24 14:41:24,343 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.543369, Batch Acc: 0.540604, Tokens per Sec:    14741, Lr: 0.000300
2025-05-24 14:41:29,369 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.754244, Batch Acc: 0.540939, Tokens per Sec:    14691, Lr: 0.000300
2025-05-24 14:41:34,305 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.796036, Batch Acc: 0.542640, Tokens per Sec:    14651, Lr: 0.000300
2025-05-24 14:41:34,306 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:41:34,306 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:41:45,005 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.62, acc:   0.47, generation: 10.6878[sec], evaluation: 0.0000[sec]
2025-05-24 14:41:45,005 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:41:45,079 - INFO - joeynmt.helpers - delete models/bpe_2k/7500.ckpt
2025-05-24 14:41:45,086 - INFO - joeynmt.training - Example #0
2025-05-24 14:41:45,086 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:41:45,086 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:41:45,086 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'ge@@', 'to@@', 'ond', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'gev@@', 'al', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'g@@', 'oo@@', 'ien', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'tot', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:41:45,087 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:41:45,087 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:41:45,087 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar getoond ik deze twee dia &apos; s zo dat de arctische ijs geval , die de meeste drie miljoen jaar de grootste van de gooien van de lagere 48 tot 40 procent .
2025-05-24 14:41:45,087 - INFO - joeynmt.training - Example #1
2025-05-24 14:41:45,087 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:41:45,087 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:41:45,087 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', '-@@', 'ex@@', 'tr@@', 'ie@@', '-@@', 'probleem', 'van', 'deze', 'bij@@', 'zonder', 'probleem', 'omdat', 'het', 'niet', 'de', 'dr@@', 'ie@@', 'der', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:41:45,087 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:41:45,087 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:41:45,087 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serie-extrie-probleem van deze bijzonder probleem omdat het niet de drieder van het ijs .
2025-05-24 14:41:45,087 - INFO - joeynmt.training - Example #2
2025-05-24 14:41:45,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:41:45,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:41:45,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'z@@', 'in', ',', 'de', 'ged@@', 're@@', 'ven', 'har@@', 't', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:41:45,088 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:41:45,088 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:41:45,088 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kaart in een zin , de gedreven hart van de wereldklimaatsysteem .
2025-05-24 14:41:45,088 - INFO - joeynmt.training - Example #3
2025-05-24 14:41:45,088 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:41:45,088 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:41:45,088 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:41:45,089 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:41:45,089 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:41:45,089 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:41:45,089 - INFO - joeynmt.training - Example #4
2025-05-24 14:41:45,089 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:41:45,089 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:41:45,089 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', 'er', 'er', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:41:45,089 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:41:45,089 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:41:45,089 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik je een rapid zal laten zien van wat er gebeurd is gebeurd gebeurd gebeurd is er er in de laatste 25 jaar .
2025-05-24 14:41:50,124 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.609137, Batch Acc: 0.542337, Tokens per Sec:    14304, Lr: 0.000300
2025-05-24 14:41:54,946 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.688898, Batch Acc: 0.542003, Tokens per Sec:    14769, Lr: 0.000300
2025-05-24 14:41:59,940 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.467046, Batch Acc: 0.542733, Tokens per Sec:    14450, Lr: 0.000300
2025-05-24 14:42:04,884 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.715611, Batch Acc: 0.543147, Tokens per Sec:    14729, Lr: 0.000300
2025-05-24 14:42:09,843 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.574482, Batch Acc: 0.537728, Tokens per Sec:    14795, Lr: 0.000300
2025-05-24 14:42:09,844 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:42:09,844 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:42:20,325 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.52, acc:   0.47, generation: 10.4704[sec], evaluation: 0.0000[sec]
2025-05-24 14:42:20,325 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:42:20,403 - INFO - joeynmt.helpers - delete models/bpe_2k/8000.ckpt
2025-05-24 14:42:20,407 - INFO - joeynmt.training - Example #0
2025-05-24 14:42:20,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:42:20,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:42:20,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'en@@', 'ste', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'ze', ',', 'heeft', 'met', 'een', 'sch@@', 'er@@', 'der@@', 'gel@@', 'ijke', ',', 'heeft', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:42:20,407 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:42:20,407 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:42:20,407 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zo demonstreren dat de arctienste kaart , die voor de laatste drie miljoen jaar is de grootte van de loze , heeft met een scherdergelijke , heeft door 40 procent .
2025-05-24 14:42:20,407 - INFO - joeynmt.training - Example #1
2025-05-24 14:42:20,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:42:20,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:42:20,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'u@@', 'w@@', 'w@@', 'oe@@', 'st@@', 'zijn', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'd@@', 'ic@@', 'k@@', 'ne@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieuwwoestzijn van dit specifieke probleem omdat het dicknesheid van het ijs .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - Example #2
2025-05-24 14:42:20,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:42:20,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:42:20,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'en@@', 'ste', 'k@@', 'aar@@', 't', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'ver@@', 'ra@@', 'ss@@', 'end', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Hypothesis: De arctienste kaart in een gevoel , het verrassend van de wereldklimaat .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - Example #3
2025-05-24 14:42:20,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:42:20,409 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:42:20,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:42:20,409 - INFO - joeynmt.training - Example #4
2025-05-24 14:42:20,410 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:42:20,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:42:20,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', ',', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:42:20,410 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:42:20,410 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:42:20,410 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik je een rapid laten zien wat er gebeurd is gebeurd gebeurd , de laatste 25 jaar .
2025-05-24 14:42:25,349 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.629682, Batch Acc: 0.541587, Tokens per Sec:    14750, Lr: 0.000300
2025-05-24 14:42:30,353 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.526595, Batch Acc: 0.550551, Tokens per Sec:    15018, Lr: 0.000300
2025-05-24 14:42:35,514 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.627957, Batch Acc: 0.545979, Tokens per Sec:    14403, Lr: 0.000300
2025-05-24 14:42:40,444 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.594005, Batch Acc: 0.546901, Tokens per Sec:    15071, Lr: 0.000300
2025-05-24 14:42:45,366 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.239477, Batch Acc: 0.546644, Tokens per Sec:    15075, Lr: 0.000300
2025-05-24 14:42:45,366 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:42:45,366 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:42:54,173 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.45, acc:   0.48, generation: 8.7935[sec], evaluation: 0.0000[sec]
2025-05-24 14:42:54,173 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:42:54,249 - INFO - joeynmt.helpers - delete models/bpe_2k/8500.ckpt
2025-05-24 14:42:54,254 - INFO - joeynmt.training - Example #0
2025-05-24 14:42:54,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:42:54,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:42:54,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:42:54,255 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:42:54,255 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:42:54,255 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoed ik deze twee dia &apos; s zo dat de arctisch ijs kaart , die voor de laatste drie miljoen jaar de grootte van de lagere 48 procent .
2025-05-24 14:42:54,255 - INFO - joeynmt.training - Example #1
2025-05-24 14:42:54,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:42:54,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:42:54,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'probleem', 'van', 'deze', 'bij@@', 'zonder', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Hypothesis: Maar dit begrip de serieuze probleem van deze bijzonder probleem omdat het niet de thickness van het ijs niet de ijs .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - Example #2
2025-05-24 14:42:54,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:42:54,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:42:54,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'z@@', 'in', ',', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kaart in een zin , de klimaatsysteem .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - Example #3
2025-05-24 14:42:54,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:42:54,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:42:54,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:42:54,256 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:42:54,257 - INFO - joeynmt.training - Example #4
2025-05-24 14:42:54,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:42:54,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:42:54,257 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:42:54,257 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:42:54,257 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:42:54,257 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurd is gebeurd gebeurd is .
2025-05-24 14:42:59,374 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.701741, Batch Acc: 0.548758, Tokens per Sec:    14277, Lr: 0.000300
2025-05-24 14:43:04,325 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.582338, Batch Acc: 0.548087, Tokens per Sec:    14905, Lr: 0.000300
2025-05-24 14:43:09,410 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.532747, Batch Acc: 0.545247, Tokens per Sec:    14609, Lr: 0.000300
2025-05-24 14:43:14,312 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.755363, Batch Acc: 0.554127, Tokens per Sec:    14519, Lr: 0.000300
2025-05-24 14:43:19,283 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.540379, Batch Acc: 0.555233, Tokens per Sec:    14641, Lr: 0.000300
2025-05-24 14:43:19,285 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:43:19,285 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:43:29,142 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.36, acc:   0.48, generation: 9.8459[sec], evaluation: 0.0000[sec]
2025-05-24 14:43:29,142 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:43:29,230 - INFO - joeynmt.helpers - delete models/bpe_2k/9000.ckpt
2025-05-24 14:43:29,235 - INFO - joeynmt.training - Example #0
2025-05-24 14:43:29,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:43:29,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:43:29,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'jaar', 'ge@@', 'to@@', 'ond', 'de', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:43:29,236 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:43:29,236 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:43:29,236 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zo dat de arctische ijs kaart , die voor de laatste drie miljoen jaar de grootste van de lagere 48 jaar getoond de 48 procent .
2025-05-24 14:43:29,236 - INFO - joeynmt.training - Example #1
2025-05-24 14:43:29,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:43:29,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:43:29,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'us', 'van', 'deze', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'n@@', 'oo@@', 'p', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:43:29,236 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:43:29,236 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:43:29,236 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieus van deze specifieke probleem omdat het niet de thicknoop van de ijs .
2025-05-24 14:43:29,237 - INFO - joeynmt.training - Example #2
2025-05-24 14:43:29,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:43:29,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:43:29,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'ged@@', 'ragen', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:43:29,237 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:43:29,237 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:43:29,237 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs kaart , in een zin , het gedragen van de wereldklimaatsysteem .
2025-05-24 14:43:29,237 - INFO - joeynmt.training - Example #3
2025-05-24 14:43:29,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:43:29,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:43:29,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:43:29,238 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:43:29,238 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:43:29,238 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:43:29,238 - INFO - joeynmt.training - Example #4
2025-05-24 14:43:29,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:43:29,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:43:29,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', 'gebeur@@', 'de', '.', '</s>']
2025-05-24 14:43:29,238 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:43:29,238 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:43:29,238 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid fastvooruit van wat er gebeurde de de laatste 25 jaar gebeurde .
2025-05-24 14:43:34,123 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.621323, Batch Acc: 0.553382, Tokens per Sec:    14790, Lr: 0.000300
2025-05-24 14:43:39,078 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.559522, Batch Acc: 0.546649, Tokens per Sec:    14701, Lr: 0.000300
2025-05-24 14:43:44,101 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.661614, Batch Acc: 0.550124, Tokens per Sec:    14238, Lr: 0.000300
2025-05-24 14:43:49,202 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.432571, Batch Acc: 0.554920, Tokens per Sec:    14665, Lr: 0.000300
2025-05-24 14:43:52,932 - INFO - joeynmt.training - Epoch   3: total training loss 6440.82
2025-05-24 14:43:52,932 - INFO - joeynmt.training - EPOCH 4
2025-05-24 14:43:54,403 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.419075, Batch Acc: 0.572770, Tokens per Sec:    13045, Lr: 0.000300
2025-05-24 14:43:54,403 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:43:54,403 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:44:05,594 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.32, acc:   0.49, generation: 11.1777[sec], evaluation: 0.0000[sec]
2025-05-24 14:44:05,595 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:44:05,669 - INFO - joeynmt.helpers - delete models/bpe_2k/9500.ckpt
2025-05-24 14:44:05,673 - INFO - joeynmt.training - Example #0
2025-05-24 14:44:05,673 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:44:05,673 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:44:05,673 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'is', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'de', 'oor@@', 'lo@@', 'ven', 'van', 'de', 'lo@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:44:05,673 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:44:05,673 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde twee dia &apos; s zo dat de arctische kap , die voor de laatste drie miljoen jaar is de afgelopen drie miljoen jaar de oorloven van de loger 40 procent .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - Example #1
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'probleem', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'er', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieuze probleem van dit specifieke probleem omdat het de dickniser van de ijs .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - Example #2
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'in', 'een', 'gev@@', 'oel', ',', 'de', 'be@@', 'per@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart , in een gevoel , de beperklimaatsysteem .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - Example #3
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:44:05,675 - INFO - joeynmt.training - Example #4
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:44:05,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'sn@@', 'el@@', '-@@', 'voor@@', 'uit@@', 'gan@@', 'g', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:44:05,677 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:44:05,677 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:44:05,677 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid snel-vooruitgang van wat er gebeurd is gebeurd gebeurd gebeurde de laatste 25 jaar .
2025-05-24 14:44:10,774 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.461099, Batch Acc: 0.569483, Tokens per Sec:    14052, Lr: 0.000300
2025-05-24 14:44:15,914 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.490466, Batch Acc: 0.578027, Tokens per Sec:    14566, Lr: 0.000300
2025-05-24 14:44:21,113 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.428384, Batch Acc: 0.567002, Tokens per Sec:    14580, Lr: 0.000300
2025-05-24 14:44:26,282 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     1.616303, Batch Acc: 0.571113, Tokens per Sec:    14357, Lr: 0.000300
2025-05-24 14:44:31,526 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     1.655562, Batch Acc: 0.571306, Tokens per Sec:    14057, Lr: 0.000300
2025-05-24 14:44:31,526 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:44:31,526 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:44:40,755 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.21, acc:   0.49, generation: 9.2104[sec], evaluation: 0.0000[sec]
2025-05-24 14:44:40,755 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:44:40,847 - INFO - joeynmt.helpers - delete models/bpe_2k/10000.ckpt
2025-05-24 14:44:40,853 - INFO - joeynmt.training - Example #0
2025-05-24 14:44:40,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:44:40,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:44:40,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'ge@@', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'ze', 'de', 'la@@', 'ger', '4@@', '8', '%', '.', '</s>']
2025-05-24 14:44:40,854 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:44:40,854 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:44:40,854 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar getoonde ik deze twee dia &apos; s zo dat de arctische ktische kap , die de meeste drie miljoen jaar de grootte van de loze de lager 48 % .
2025-05-24 14:44:40,854 - INFO - joeynmt.training - Example #1
2025-05-24 14:44:40,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:44:40,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:44:40,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'probleem', 'van', 'dit', 'be@@', 'pa@@', 'al@@', 'de', 'p@@', 'iek', 'omdat', 'het', 'het', 'het', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:44:40,854 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:44:40,854 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:44:40,855 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieuze probleem van dit bepaalde piek omdat het het het niet de dicknisheid van het ijs .
2025-05-24 14:44:40,855 - INFO - joeynmt.training - Example #2
2025-05-24 14:44:40,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:44:40,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:44:40,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'f@@', 'ers', ',', 'in', 'een', 'z@@', 'in', ',', 'de', 'be@@', 'per@@', 'kt', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:44:40,855 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:44:40,855 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:44:40,855 - INFO - joeynmt.training - 	Hypothesis: De arctische ijfers , in een zin , de beperkt van de wereldklimaatsysteem .
2025-05-24 14:44:40,855 - INFO - joeynmt.training - Example #3
2025-05-24 14:44:40,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:44:40,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:44:40,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:44:40,856 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:44:40,856 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:44:40,856 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in de winter en contracten in zomer .
2025-05-24 14:44:40,857 - INFO - joeynmt.training - Example #4
2025-05-24 14:44:40,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:44:40,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:44:40,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'sn@@', 'el@@', '-@@', 'voor@@', 'uit', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:44:40,857 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:44:40,857 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:44:40,857 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid snel-vooruit wat er gebeurde de de de laatste 25 jaar .
2025-05-24 14:44:46,100 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     1.489835, Batch Acc: 0.572284, Tokens per Sec:    13400, Lr: 0.000300
2025-05-24 14:44:51,250 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     1.472252, Batch Acc: 0.565229, Tokens per Sec:    14054, Lr: 0.000300
2025-05-24 14:44:56,203 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     1.567069, Batch Acc: 0.570880, Tokens per Sec:    14981, Lr: 0.000300
2025-05-24 14:45:01,199 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     1.541517, Batch Acc: 0.570399, Tokens per Sec:    14415, Lr: 0.000300
2025-05-24 14:45:06,142 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     1.474973, Batch Acc: 0.568631, Tokens per Sec:    14960, Lr: 0.000300
2025-05-24 14:45:06,142 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:45:06,142 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:45:16,205 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.19, acc:   0.49, generation: 10.0485[sec], evaluation: 0.0000[sec]
2025-05-24 14:45:16,205 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:45:16,285 - INFO - joeynmt.helpers - delete models/bpe_2k/10500.ckpt
2025-05-24 14:45:16,290 - INFO - joeynmt.training - Example #0
2025-05-24 14:45:16,290 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:45:16,290 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:45:16,290 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'ti@@', 'sch', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'ste', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'is', 'de', 'lo@@', 'wer', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:45:16,291 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:45:16,291 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:45:16,291 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde twee dia &apos; s zo dat de arctische ktisch ijs ktisch , die de laatste drie miljoen jaar is de grootste van de laatste drie miljoen jaar is de lower 40 procent .
2025-05-24 14:45:16,291 - INFO - joeynmt.training - Example #1
2025-05-24 14:45:16,291 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:45:16,291 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:45:16,291 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'ie', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'r@@', 'om@@', 'm@@', 'ate', 'niet', '.', '</s>']
2025-05-24 14:45:16,292 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:45:16,292 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:45:16,292 - INFO - joeynmt.training - 	Hypothesis: Maar dit begrip de serie van dit specifieke probleem omdat het niet de dickrommate niet .
2025-05-24 14:45:16,292 - INFO - joeynmt.training - Example #2
2025-05-24 14:45:16,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:45:16,292 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:45:16,292 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'keer', 'is', 'in', 'een', 'z@@', 'in', ',', 'de', 'ged@@', 'ur@@', 'ende', 'har@@', 't', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:45:16,292 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:45:16,292 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:45:16,292 - INFO - joeynmt.training - 	Hypothesis: De arctische keer is in een zin , de gedurende hart van de wereldwijde klimaatsysteem .
2025-05-24 14:45:16,292 - INFO - joeynmt.training - Example #3
2025-05-24 14:45:16,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:45:16,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:45:16,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:45:16,293 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:45:16,293 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:45:16,293 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in zomer .
2025-05-24 14:45:16,294 - INFO - joeynmt.training - Example #4
2025-05-24 14:45:16,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:45:16,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:45:16,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:45:16,294 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:45:16,294 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:45:16,294 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurd is gebeurd gebeurd is .
2025-05-24 14:45:21,280 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.543401, Batch Acc: 0.564928, Tokens per Sec:    14517, Lr: 0.000300
2025-05-24 14:45:26,397 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.405546, Batch Acc: 0.567411, Tokens per Sec:    14346, Lr: 0.000300
2025-05-24 14:45:31,403 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.694684, Batch Acc: 0.566842, Tokens per Sec:    14396, Lr: 0.000300
2025-05-24 14:45:36,580 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.453409, Batch Acc: 0.568416, Tokens per Sec:    14297, Lr: 0.000300
2025-05-24 14:45:41,831 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.531763, Batch Acc: 0.569189, Tokens per Sec:    14332, Lr: 0.000300
2025-05-24 14:45:41,831 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:45:41,831 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:45:51,525 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.12, acc:   0.49, generation: 9.6827[sec], evaluation: 0.0000[sec]
2025-05-24 14:45:51,526 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:45:51,604 - INFO - joeynmt.helpers - delete models/bpe_2k/11000.ckpt
2025-05-24 14:45:51,610 - INFO - joeynmt.training - Example #0
2025-05-24 14:45:51,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:45:51,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:45:51,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'p', ',', 'die', 'de', 'mee@@', 'ste', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'wer', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:45:51,611 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:45:51,611 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:45:51,611 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zo dat de arctische ijs , die de arctische ijp , die de meeste van de laatste drie miljoen jaar de grootte van de lower 48 procent .
2025-05-24 14:45:51,611 - INFO - joeynmt.training - Example #1
2025-05-24 14:45:51,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:45:51,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:45:51,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie', 'van', 'deze', 'be@@', 'pa@@', 'al@@', 'de', 'probleem', ',', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'per@@', 'k', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:45:51,612 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:45:51,612 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:45:51,612 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serie van deze bepaalde probleem , omdat het niet de thicknisperk van het ijs .
2025-05-24 14:45:51,612 - INFO - joeynmt.training - Example #2
2025-05-24 14:45:51,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:45:51,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:45:51,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'ged@@', 'ru@@', 'kt', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:45:51,612 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:45:51,612 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart , in een zin , het gedrukt van de wereldklimaatsysteem .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - Example #3
2025-05-24 14:45:51,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:45:51,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:45:51,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - Example #4
2025-05-24 14:45:51,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:45:51,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:45:51,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'sn@@', 'el@@', 'le', 'voor@@', 'uit@@', 'gan@@', 'g', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', ',', 'is', 'er', 'dan', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:45:51,613 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid snelle vooruitgang van wat er gebeurd is , is er dan de laatste 25 jaar .
2025-05-24 14:45:56,703 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.452044, Batch Acc: 0.571260, Tokens per Sec:    14368, Lr: 0.000300
2025-05-24 14:46:01,944 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.464943, Batch Acc: 0.574443, Tokens per Sec:    13981, Lr: 0.000300
2025-05-24 14:46:06,997 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.546769, Batch Acc: 0.568637, Tokens per Sec:    14066, Lr: 0.000300
2025-05-24 14:46:12,110 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.580054, Batch Acc: 0.564233, Tokens per Sec:    14441, Lr: 0.000300
2025-05-24 14:46:17,184 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.487698, Batch Acc: 0.570186, Tokens per Sec:    14636, Lr: 0.000300
2025-05-24 14:46:17,184 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:46:17,184 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:46:27,150 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.09, acc:   0.50, generation: 9.9523[sec], evaluation: 0.0000[sec]
2025-05-24 14:46:27,150 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:46:27,229 - INFO - joeynmt.helpers - delete models/bpe_2k/11500.ckpt
2025-05-24 14:46:27,234 - INFO - joeynmt.training - Example #0
2025-05-24 14:46:27,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:46:27,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:46:27,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'z@@', 'in@@', 'ni@@', 'g', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:46:27,234 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:46:27,234 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:46:27,235 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoed ik deze twee dia &apos; s zo dat de arctische ijs kaart , die voor de meeste drie miljoen jaar de grootte van de zinnig van de lager 40 procent .
2025-05-24 14:46:27,235 - INFO - joeynmt.training - Example #1
2025-05-24 14:46:27,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:46:27,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:46:27,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'ie', 'van', 'dit', 'bij@@', 'zonder', 'probleem', 'te', 'be@@', 'stu@@', 'deren', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:46:27,235 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:46:27,235 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:46:27,235 - INFO - joeynmt.training - 	Hypothesis: Maar dit begrip de serie van dit bijzonder probleem te bestuderen omdat het niet de thicknisheid van het ijs .
2025-05-24 14:46:27,235 - INFO - joeynmt.training - Example #2
2025-05-24 14:46:27,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:46:27,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:46:27,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'ged@@', 'ur@@', 'ende', 'het', 'har@@', 't', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'schapp@@', 'ij', '.', '</s>']
2025-05-24 14:46:27,237 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:46:27,237 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:46:27,237 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart in een gevoel , het gedurende het hart van de wereldwijde klimaatschappij .
2025-05-24 14:46:27,237 - INFO - joeynmt.training - Example #3
2025-05-24 14:46:27,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:46:27,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:46:27,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'chten', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:46:27,237 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:46:27,237 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:46:27,237 - INFO - joeynmt.training - 	Hypothesis: Het verwachten in winter en contracten in zomer .
2025-05-24 14:46:27,237 - INFO - joeynmt.training - Example #4
2025-05-24 14:46:27,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:46:27,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:46:27,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'laten', 'zien', 'dat', 'ik', 'een', 'r@@', 'ap@@', 'id', 'sne@@', 'l', 'zal', 'zijn', 'van', 'de', 'laatste', '2@@', '5', 'jaar', 'gebeur@@', 'de', '.', '</s>']
2025-05-24 14:46:27,238 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:46:27,238 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:46:27,238 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik laten zien dat ik een rapid snel zal zijn van de laatste 25 jaar gebeurde .
2025-05-24 14:46:32,380 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.357320, Batch Acc: 0.567531, Tokens per Sec:    14147, Lr: 0.000300
2025-05-24 14:46:37,577 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.514953, Batch Acc: 0.570185, Tokens per Sec:    14147, Lr: 0.000300
2025-05-24 14:46:42,798 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.419785, Batch Acc: 0.569085, Tokens per Sec:    14021, Lr: 0.000300
2025-05-24 14:46:48,015 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.711700, Batch Acc: 0.572689, Tokens per Sec:    14377, Lr: 0.000300
2025-05-24 14:46:53,144 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.526312, Batch Acc: 0.570755, Tokens per Sec:    14431, Lr: 0.000300
2025-05-24 14:46:53,145 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:46:53,145 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:47:03,750 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.11, acc:   0.49, generation: 10.5928[sec], evaluation: 0.0000[sec]
2025-05-24 14:47:03,830 - INFO - joeynmt.helpers - delete models/bpe_2k/12000.ckpt
2025-05-24 14:47:03,835 - INFO - joeynmt.training - Example #0
2025-05-24 14:47:03,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:47:03,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:47:03,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', '-@@', 'a@@', 'p', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'tot', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:47:03,836 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:47:03,836 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:47:03,836 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar jaar schoed ik deze twee dia &apos; s zo dat de arctische ijs-ap , die de laatste drie miljoen jaar de grootte van de lager 48 van de lager 48 tot 40 % .
2025-05-24 14:47:03,836 - INFO - joeynmt.training - Example #1
2025-05-24 14:47:03,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:47:03,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:47:03,837 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'ei@@', 'den', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'probleem', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'n@@', 'is', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:47:03,837 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:47:03,837 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:47:03,837 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheiden de serieuze probleem van dit specifieke probleem omdat het niet de thicknis van het ijs .
2025-05-24 14:47:03,837 - INFO - joeynmt.training - Example #2
2025-05-24 14:47:03,837 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:47:03,837 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:47:03,837 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', 'is', ',', 'de', 'har@@', 't@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:47:03,837 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:47:03,837 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:47:03,838 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart is , de hartklimaatsysteem van de wereldklimaatsysteem .
2025-05-24 14:47:03,838 - INFO - joeynmt.training - Example #3
2025-05-24 14:47:03,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:47:03,838 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:47:03,838 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:47:03,838 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:47:03,838 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:47:03,838 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in zomer .
2025-05-24 14:47:03,838 - INFO - joeynmt.training - Example #4
2025-05-24 14:47:03,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:47:03,838 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:47:03,838 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:47:03,839 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:47:03,839 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:47:03,839 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurde de laatste 25 jaar gebeurde de de laatste 25 jaar .
2025-05-24 14:47:08,953 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.401113, Batch Acc: 0.569650, Tokens per Sec:    14170, Lr: 0.000300
2025-05-24 14:47:14,085 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.495746, Batch Acc: 0.569973, Tokens per Sec:    13962, Lr: 0.000300
2025-05-24 14:47:19,253 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.612852, Batch Acc: 0.569783, Tokens per Sec:    13912, Lr: 0.000300
2025-05-24 14:47:24,330 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.591288, Batch Acc: 0.570169, Tokens per Sec:    14452, Lr: 0.000300
2025-05-24 14:47:29,415 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.703898, Batch Acc: 0.575772, Tokens per Sec:    14651, Lr: 0.000300
2025-05-24 14:47:29,415 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:47:29,415 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:47:38,936 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.01, acc:   0.50, generation: 9.5073[sec], evaluation: 0.0000[sec]
2025-05-24 14:47:38,936 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:47:39,018 - INFO - joeynmt.helpers - delete models/bpe_2k/12500.ckpt
2025-05-24 14:47:39,024 - INFO - joeynmt.training - Example #0
2025-05-24 14:47:39,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:47:39,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:47:39,024 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La@@', 'at', 'jaar', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'h@@', 'oord', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:47:39,024 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:47:39,024 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:47:39,024 - INFO - joeynmt.training - 	Hypothesis: Laat jaar jaar toonde ik deze twee dia &apos; s zien dat de arctische ijs de arctische ijs van de laatste drie miljoen jaar de grootte van de loger 48 staten , heeft gehoord door 40 procent .
2025-05-24 14:47:39,024 - INFO - joeynmt.training - Example #1
2025-05-24 14:47:39,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:47:39,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:47:39,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'ei@@', 'den', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'deze', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:47:39,025 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:47:39,025 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:47:39,025 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderscheiden de seriënt van deze specifieke probleem omdat het niet de thickness van het ijs .
2025-05-24 14:47:39,025 - INFO - joeynmt.training - Example #2
2025-05-24 14:47:39,025 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:47:39,025 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:47:39,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'z@@', 'in', ',', 'het', 'ged@@', 'ur@@', 'ende', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:47:39,026 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:47:39,026 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:47:39,026 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart in een zin , het gedurende het wereldklimaat .
2025-05-24 14:47:39,026 - INFO - joeynmt.training - Example #3
2025-05-24 14:47:39,026 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:47:39,026 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:47:39,026 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:47:39,026 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:47:39,026 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:47:39,027 - INFO - joeynmt.training - 	Hypothesis: Het verwagens en contracten in zomer .
2025-05-24 14:47:39,027 - INFO - joeynmt.training - Example #4
2025-05-24 14:47:39,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:47:39,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:47:39,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', 'gebeur@@', 'de', 'er', 'gebeur@@', 'de', '.', '</s>']
2025-05-24 14:47:39,027 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:47:39,027 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:47:39,027 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurde de laatste 25 jaar gebeurde er gebeurde .
2025-05-24 14:47:44,233 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.746332, Batch Acc: 0.570843, Tokens per Sec:    13732, Lr: 0.000300
2025-05-24 14:47:49,341 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.325958, Batch Acc: 0.572830, Tokens per Sec:    14371, Lr: 0.000300
2025-05-24 14:47:54,389 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.511015, Batch Acc: 0.573783, Tokens per Sec:    14473, Lr: 0.000300
2025-05-24 14:47:59,498 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.637309, Batch Acc: 0.573142, Tokens per Sec:    14569, Lr: 0.000300
2025-05-24 14:48:04,602 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.611735, Batch Acc: 0.571812, Tokens per Sec:    14465, Lr: 0.000300
2025-05-24 14:48:04,603 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:48:04,603 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:48:15,056 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.93, acc:   0.50, generation: 10.4400[sec], evaluation: 0.0000[sec]
2025-05-24 14:48:15,057 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:48:15,138 - INFO - joeynmt.helpers - delete models/bpe_2k/13000.ckpt
2025-05-24 14:48:15,143 - INFO - joeynmt.training - Example #0
2025-05-24 14:48:15,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:48:15,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:48:15,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'die', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'or@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:48:15,143 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:48:15,143 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:48:15,143 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s die demonstreren die de arctische ijs korp , die voor de meeste drie miljoen jaar de grootste van de lagere 48 procent .
2025-05-24 14:48:15,143 - INFO - joeynmt.training - Example #1
2025-05-24 14:48:15,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:48:15,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:48:15,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'ste@@', 'un@@', 't', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:48:15,145 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:48:15,145 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:48:15,145 - INFO - joeynmt.training - 	Hypothesis: Maar deze ondersteunt de serieus van dit specifieke probleem omdat het niet de thickness van het ijs .
2025-05-24 14:48:15,145 - INFO - joeynmt.training - Example #2
2025-05-24 14:48:15,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:48:15,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:48:15,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'ver@@', 'spre@@', 'i@@', 'ding', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:48:15,146 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:48:15,146 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:48:15,146 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap in een gevoel , het verspreiding van de wereldklimaat .
2025-05-24 14:48:15,146 - INFO - joeynmt.training - Example #3
2025-05-24 14:48:15,146 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:48:15,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:48:15,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'an@@', 'den', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:48:15,146 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:48:15,146 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:48:15,146 - INFO - joeynmt.training - 	Hypothesis: Het verwaanden in de winter en contracten in zomer .
2025-05-24 14:48:15,147 - INFO - joeynmt.training - Example #4
2025-05-24 14:48:15,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:48:15,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:48:15,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'voor@@', 'uit@@', 'gan@@', 'g', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:48:15,147 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:48:15,147 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:48:15,147 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid zijn vooruitgang van wat er gebeurde de de laatste 25 jaar .
2025-05-24 14:48:20,349 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.684274, Batch Acc: 0.572855, Tokens per Sec:    13839, Lr: 0.000300
2025-05-24 14:48:25,462 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.443200, Batch Acc: 0.577312, Tokens per Sec:    13866, Lr: 0.000300
2025-05-24 14:48:30,569 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.758105, Batch Acc: 0.564424, Tokens per Sec:    14575, Lr: 0.000300
2025-05-24 14:48:35,736 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.549731, Batch Acc: 0.576733, Tokens per Sec:    13885, Lr: 0.000300
2025-05-24 14:48:39,119 - INFO - joeynmt.training - Epoch   4: total training loss 6013.55
2025-05-24 14:48:39,119 - INFO - joeynmt.training - EPOCH 5
2025-05-24 14:48:40,934 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     1.330766, Batch Acc: 0.599714, Tokens per Sec:    14272, Lr: 0.000300
2025-05-24 14:48:40,934 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:48:40,934 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:48:50,379 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.89, acc:   0.50, generation: 9.4318[sec], evaluation: 0.0000[sec]
2025-05-24 14:48:50,379 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:48:50,459 - INFO - joeynmt.helpers - delete models/bpe_2k/13500.ckpt
2025-05-24 14:48:50,465 - INFO - joeynmt.training - Example #0
2025-05-24 14:48:50,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:48:50,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:48:50,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'la@@', 'g@@', 'ere', 'k@@', 'a@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', '%', 'ge@@', 'st@@', 'aten', ',', 'heeft', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:48:50,465 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:48:50,465 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:48:50,465 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zo dat de arctische lagere kap , die voor de meeste drie miljoen jaar de grootte van de lager 48 % gestaten , heeft door 40 procent .
2025-05-24 14:48:50,465 - INFO - joeynmt.training - Example #1
2025-05-24 14:48:50,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:48:50,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:48:50,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'ep@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'a@@', 'der', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:48:50,466 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:48:50,466 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:48:50,466 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serie van dit specifieke probleem omdat het de diepsheid van het ijs niet de dickader van het ijs .
2025-05-24 14:48:50,466 - INFO - joeynmt.training - Example #2
2025-05-24 14:48:50,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:48:50,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:48:50,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'de', 'k@@', 'li@@', 'ma@@', 'at', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:48:50,466 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:48:50,466 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:48:50,466 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in een zin , de klimaat van de wereldklimaat .
2025-05-24 14:48:50,467 - INFO - joeynmt.training - Example #3
2025-05-24 14:48:50,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:48:50,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:48:50,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'gens', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:48:50,467 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:48:50,467 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:48:50,467 - INFO - joeynmt.training - 	Hypothesis: Het verwagens in winter en contracten in de zomer .
2025-05-24 14:48:50,467 - INFO - joeynmt.training - Example #4
2025-05-24 14:48:50,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:48:50,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:48:50,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'voor@@', 'uit@@', 'gan@@', 'g', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:48:50,468 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:48:50,468 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:48:50,468 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid een rapid vooruitgang van wat er gebeurd is gebeurd gebeurd is .
2025-05-24 14:48:55,564 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     1.357345, Batch Acc: 0.591463, Tokens per Sec:    14041, Lr: 0.000300
2025-05-24 14:49:00,651 - INFO - joeynmt.training - Epoch   5, Step:    16200, Batch Loss:     1.339042, Batch Acc: 0.592290, Tokens per Sec:    14339, Lr: 0.000300
2025-05-24 14:49:05,748 - INFO - joeynmt.training - Epoch   5, Step:    16300, Batch Loss:     1.399554, Batch Acc: 0.592347, Tokens per Sec:    14355, Lr: 0.000300
2025-05-24 14:49:10,838 - INFO - joeynmt.training - Epoch   5, Step:    16400, Batch Loss:     1.443025, Batch Acc: 0.591858, Tokens per Sec:    14206, Lr: 0.000300
2025-05-24 14:49:15,923 - INFO - joeynmt.training - Epoch   5, Step:    16500, Batch Loss:     1.429714, Batch Acc: 0.592491, Tokens per Sec:    14432, Lr: 0.000300
2025-05-24 14:49:15,924 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:49:15,924 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:49:26,049 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.93, acc:   0.51, generation: 10.1123[sec], evaluation: 0.0000[sec]
2025-05-24 14:49:26,139 - INFO - joeynmt.helpers - delete models/bpe_2k/14500.ckpt
2025-05-24 14:49:26,144 - INFO - joeynmt.training - Example #0
2025-05-24 14:49:26,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:49:26,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:49:26,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'zien', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'gev@@', 'o@@', 'el@@', 'ens', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'af@@', 'gel@@', 'open', '3', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'gel@@', 'ijk@@', 'er@@', 'aten', ',', 'heeft', 'ge@@', 'de@@', 'el@@', 'den', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:49:26,145 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:49:26,145 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:49:26,145 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar zien ik deze twee dia &apos; s zodat de arctische ijs de gevoelens van de laatste drie miljoen jaar de grootste van de afgelopen 3 miljoen jaar de grootte van de gelijkeraten , heeft gedeelden door 40 procent .
2025-05-24 14:49:26,145 - INFO - joeynmt.training - Example #1
2025-05-24 14:49:26,146 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:49:26,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:49:26,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'ei@@', 'den', 'de', 'ser@@', 'ie', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:49:26,146 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:49:26,146 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:49:26,146 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderscheiden de serie van dit specifieke probleem omdat het de dickness van het ijs niet de dickness van het ijs .
2025-05-24 14:49:26,146 - INFO - joeynmt.training - Example #2
2025-05-24 14:49:26,146 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:49:26,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:49:26,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'z@@', 'in', ',', 'het', 'ver@@', 'b@@', 'and', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:49:26,146 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:49:26,146 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:49:26,147 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart in een zin , het verband van het wereldklimaatsysteem .
2025-05-24 14:49:26,147 - INFO - joeynmt.training - Example #3
2025-05-24 14:49:26,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:49:26,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:49:26,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'and@@', 's', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:49:26,147 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:49:26,147 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:49:26,147 - INFO - joeynmt.training - 	Hypothesis: Het verwaands in de winter en contracten in zomer .
2025-05-24 14:49:26,147 - INFO - joeynmt.training - Example #4
2025-05-24 14:49:26,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:49:26,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:49:26,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'sn@@', 'el@@', 'le', 'voor@@', 'uit@@', 'komt', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:49:26,148 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:49:26,148 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:49:26,148 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid een rapid snelle vooruitkomt van wat er gebeurd is .
2025-05-24 14:49:31,270 - INFO - joeynmt.training - Epoch   5, Step:    16600, Batch Loss:     1.306926, Batch Acc: 0.588123, Tokens per Sec:    13952, Lr: 0.000300
2025-05-24 14:49:36,398 - INFO - joeynmt.training - Epoch   5, Step:    16700, Batch Loss:     1.366452, Batch Acc: 0.594254, Tokens per Sec:    14426, Lr: 0.000300
2025-05-24 14:49:41,617 - INFO - joeynmt.training - Epoch   5, Step:    16800, Batch Loss:     1.441872, Batch Acc: 0.588741, Tokens per Sec:    14420, Lr: 0.000300
2025-05-24 14:49:46,749 - INFO - joeynmt.training - Epoch   5, Step:    16900, Batch Loss:     1.356196, Batch Acc: 0.587989, Tokens per Sec:    13951, Lr: 0.000300
2025-05-24 14:49:51,883 - INFO - joeynmt.training - Epoch   5, Step:    17000, Batch Loss:     1.450618, Batch Acc: 0.590503, Tokens per Sec:    14426, Lr: 0.000300
2025-05-24 14:49:51,883 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:49:51,883 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:50:01,616 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.88, acc:   0.50, generation: 9.7203[sec], evaluation: 0.0000[sec]
2025-05-24 14:50:01,617 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:50:01,699 - INFO - joeynmt.helpers - delete models/bpe_2k/14000.ckpt
2025-05-24 14:50:01,705 - INFO - joeynmt.training - Example #0
2025-05-24 14:50:01,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:50:01,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:50:01,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'af@@', 'gel@@', 'open', '&apos;', 'n', 'drie', 'miljoen', 'jaar', ',', 'die', 'de', 'meest', 'meest', 'meest', 'meest', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'l@@', 'en@@', 'g@@', 'de', '4@@', '8', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:50:01,705 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde deze twee dia &apos; s zo &apos; n afgelopen &apos; n drie miljoen jaar , die de meest meest meest meest drie miljoen jaar de grootste van de lengde 48 van de lagere 48 procent .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - Example #1
2025-05-24 14:50:01,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:50:01,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:50:01,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie', 'van', 'dit', 'bij@@', 'zonder', 'probleem', '.', '</s>']
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Hypothesis: Maar deze begreep de serie van dit bijzonder probleem .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - Example #2
2025-05-24 14:50:01,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:50:01,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:50:01,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'ged@@', 'ur@@', 'ende', 'het', 'werel@@', 'd@@', 'k@@', 'om', 'systeem', 'te', 'k@@', 'ie@@', 'm@@', 'systeem', '.', '</s>']
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in een zin , het gedurende het wereldkom systeem te kiemsysteem .
2025-05-24 14:50:01,706 - INFO - joeynmt.training - Example #3
2025-05-24 14:50:01,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:50:01,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:50:01,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'grot@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:50:01,708 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:50:01,708 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:50:01,708 - INFO - joeynmt.training - 	Hypothesis: Het vergrott in de winter en contracten in de zomer .
2025-05-24 14:50:01,708 - INFO - joeynmt.training - Example #4
2025-05-24 14:50:01,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:50:01,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:50:01,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'sn@@', 'el@@', 'le', 'l@@', 'er', 'van', 'wat', 'er', 'gebeur@@', 'de', 'er', 'gebeur@@', 'de', '.', '</s>']
2025-05-24 14:50:01,709 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:50:01,709 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:50:01,709 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid een rapid snelle ler van wat er gebeurde er gebeurde .
2025-05-24 14:50:06,851 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     1.430666, Batch Acc: 0.589902, Tokens per Sec:    14024, Lr: 0.000300
2025-05-24 14:50:11,937 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.391040, Batch Acc: 0.587597, Tokens per Sec:    14456, Lr: 0.000300
2025-05-24 14:50:17,123 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     1.288515, Batch Acc: 0.589490, Tokens per Sec:    14427, Lr: 0.000300
2025-05-24 14:50:22,264 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.597906, Batch Acc: 0.588647, Tokens per Sec:    14550, Lr: 0.000300
2025-05-24 14:50:27,358 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.498351, Batch Acc: 0.588187, Tokens per Sec:    14482, Lr: 0.000300
2025-05-24 14:50:27,358 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:50:27,358 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:50:36,468 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.85, acc:   0.50, generation: 9.0966[sec], evaluation: 0.0000[sec]
2025-05-24 14:50:36,468 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:50:36,544 - INFO - joeynmt.helpers - delete models/bpe_2k/15000.ckpt
2025-05-24 14:50:36,550 - INFO - joeynmt.training - Example #0
2025-05-24 14:50:36,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:50:36,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:50:36,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'h@@', 'oord', 'door', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:50:36,551 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:50:36,551 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:50:36,551 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoed ik deze twee dia &apos; s zo &apos; n afgelopen drie miljoen jaar de grootste kaart , die voor de meeste drie miljoen jaar de grootte van de lager 48 staten , heeft gehoord door 40 % .
2025-05-24 14:50:36,551 - INFO - joeynmt.training - Example #1
2025-05-24 14:50:36,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:50:36,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:50:36,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'ei@@', 'd@@', 's@@', 'zijn', 'van', 'dit', 'onder@@', 'sch@@', 'ei@@', 'ding', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:50:36,552 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:50:36,552 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:50:36,552 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderscheidszijn van dit onderscheiding van dit probleem omdat het niet de thickness van het ijs .
2025-05-24 14:50:36,552 - INFO - joeynmt.training - Example #2
2025-05-24 14:50:36,552 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:50:36,552 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:50:36,552 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'het', 'be@@', 'staat', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:50:36,553 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:50:36,553 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:50:36,553 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart , het bestaat van het wereldklimaat .
2025-05-24 14:50:36,553 - INFO - joeynmt.training - Example #3
2025-05-24 14:50:36,553 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:50:36,553 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:50:36,553 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'and@@', 's', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:50:36,553 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:50:36,553 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:50:36,554 - INFO - joeynmt.training - 	Hypothesis: Het expands winter en contracten in zomer .
2025-05-24 14:50:36,554 - INFO - joeynmt.training - Example #4
2025-05-24 14:50:36,554 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:50:36,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:50:36,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'vor@@', 'm', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:50:36,554 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:50:36,554 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:50:36,554 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik je een rapid fast-vorm van wat er gebeurd is in de afgelopen 25 jaar .
2025-05-24 14:50:41,728 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.560084, Batch Acc: 0.586345, Tokens per Sec:    13955, Lr: 0.000300
2025-05-24 14:50:46,811 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.226730, Batch Acc: 0.589024, Tokens per Sec:    14433, Lr: 0.000300
2025-05-24 14:50:51,848 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.433400, Batch Acc: 0.587706, Tokens per Sec:    14497, Lr: 0.000300
2025-05-24 14:50:56,918 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.541609, Batch Acc: 0.588394, Tokens per Sec:    14724, Lr: 0.000300
2025-05-24 14:51:02,007 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.473485, Batch Acc: 0.588119, Tokens per Sec:    14964, Lr: 0.000300
2025-05-24 14:51:02,008 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:51:02,008 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:51:10,975 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.80, acc:   0.51, generation: 8.9531[sec], evaluation: 0.0000[sec]
2025-05-24 14:51:10,975 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:51:11,053 - INFO - joeynmt.helpers - delete models/bpe_2k/15500.ckpt
2025-05-24 14:51:11,061 - INFO - joeynmt.training - Example #0
2025-05-24 14:51:11,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:51:11,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:51:11,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'beetje', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'c@@', 'ti@@', 'sche', 'c@@', 'ti@@', 'sch', 'is', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'h@@', 'oord', '.', '</s>']
2025-05-24 14:51:11,061 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:51:11,061 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:51:11,061 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde deze twee dia &apos; s zo &apos; n beetje de arctische ctische ctisch is , die voor de laatste drie miljoen jaar de grootte van de lager 48 staten , heeft gehoord .
2025-05-24 14:51:11,061 - INFO - joeynmt.training - Example #1
2025-05-24 14:51:11,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:51:11,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:51:11,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'er@@', 'en@@', 's@@', 'ou@@', 'de', 'probleem', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oel', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:51:11,063 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:51:11,063 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:51:11,063 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serierensoude probleem van dit specifieke probleem omdat het de doel niet de dickness van het ijs .
2025-05-24 14:51:11,063 - INFO - joeynmt.training - Example #2
2025-05-24 14:51:11,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:51:11,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:51:11,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'k@@', 'aar@@', 't', 'in', 'een', 'z@@', 'in', ',', 'het', 'ver@@', 'z@@', 'in', ',', 'het', 'str@@', 'al@@', 'ing', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:51:11,064 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:51:11,064 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:51:11,064 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kaart in een zin , het verzin , het straling van het wereldklimaat .
2025-05-24 14:51:11,064 - INFO - joeynmt.training - Example #3
2025-05-24 14:51:11,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:51:11,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:51:11,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht@@', 'ten', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:51:11,064 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:51:11,064 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:51:11,065 - INFO - joeynmt.training - 	Hypothesis: Het verwachtten in de winter en contracten in de zomer .
2025-05-24 14:51:11,065 - INFO - joeynmt.training - Example #4
2025-05-24 14:51:11,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:51:11,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:51:11,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'van', 'de', 'laatste', '2@@', '5', 'jaar', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:51:11,066 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:51:11,066 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:51:11,066 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik je een rapid van de laatste 25 jaar vooruit van wat er gebeurde de laatste 25 jaar .
2025-05-24 14:51:16,162 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.465172, Batch Acc: 0.587792, Tokens per Sec:    14377, Lr: 0.000300
2025-05-24 14:51:21,253 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.397420, Batch Acc: 0.587383, Tokens per Sec:    15005, Lr: 0.000300
2025-05-24 14:51:26,437 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.321927, Batch Acc: 0.593811, Tokens per Sec:    14605, Lr: 0.000300
2025-05-24 14:51:31,564 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.528301, Batch Acc: 0.591412, Tokens per Sec:    14464, Lr: 0.000300
2025-05-24 14:51:36,693 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.507102, Batch Acc: 0.586947, Tokens per Sec:    14148, Lr: 0.000300
2025-05-24 14:51:36,694 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:51:36,694 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:51:47,008 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.83, acc:   0.50, generation: 10.3028[sec], evaluation: 0.0000[sec]
2025-05-24 14:51:47,086 - INFO - joeynmt.helpers - delete models/bpe_2k/16500.ckpt
2025-05-24 14:51:47,091 - INFO - joeynmt.training - Example #0
2025-05-24 14:51:47,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:51:47,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:51:47,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'dia', '&apos;', 'n', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'oo@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'ger', '4@@', '8', '%', '.', '</s>']
2025-05-24 14:51:47,092 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:51:47,092 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:51:47,093 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zo &apos; n dia &apos; n arctische ijs koop , die voor de meeste drie miljoen jaar de grootste van de lager 48 % .
2025-05-24 14:51:47,093 - INFO - joeynmt.training - Example #1
2025-05-24 14:51:47,093 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:51:47,093 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:51:47,093 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'i@@', 'er@@', 'van', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'er', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 14:51:47,093 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:51:47,093 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:51:47,093 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de seriervan van dit probleem omdat het de dickniser niet tonen .
2025-05-24 14:51:47,093 - INFO - joeynmt.training - Example #2
2025-05-24 14:51:47,093 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:51:47,094 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:51:47,094 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'ged@@', 'ru@@', 'g', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:51:47,094 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:51:47,094 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:51:47,094 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart , in een zin , het gedrug van het wereldklimaat .
2025-05-24 14:51:47,094 - INFO - joeynmt.training - Example #3
2025-05-24 14:51:47,094 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:51:47,094 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:51:47,094 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:51:47,094 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:51:47,094 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:51:47,094 - INFO - joeynmt.training - 	Hypothesis: Het verwachtt in winter en contracten in zomer .
2025-05-24 14:51:47,094 - INFO - joeynmt.training - Example #4
2025-05-24 14:51:47,094 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:51:47,095 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:51:47,095 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'laten', 'zien', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:51:47,095 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:51:47,095 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:51:47,095 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid laten zien van wat er gebeurde de laatste 25 jaar .
2025-05-24 14:51:52,264 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.487109, Batch Acc: 0.589736, Tokens per Sec:    14216, Lr: 0.000300
2025-05-24 14:51:57,292 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.433149, Batch Acc: 0.588502, Tokens per Sec:    14854, Lr: 0.000300
2025-05-24 14:52:02,382 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.572065, Batch Acc: 0.586647, Tokens per Sec:    14140, Lr: 0.000300
2025-05-24 14:52:07,421 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.454389, Batch Acc: 0.590496, Tokens per Sec:    14358, Lr: 0.000300
2025-05-24 14:52:12,567 - INFO - joeynmt.training - Epoch   5, Step:    19000, Batch Loss:     1.473964, Batch Acc: 0.587497, Tokens per Sec:    14412, Lr: 0.000300
2025-05-24 14:52:12,567 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:52:12,568 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:52:21,371 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.73, acc:   0.51, generation: 8.7908[sec], evaluation: 0.0000[sec]
2025-05-24 14:52:21,371 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:52:21,449 - INFO - joeynmt.helpers - delete models/bpe_2k/16000.ckpt
2025-05-24 14:52:21,455 - INFO - joeynmt.training - Example #0
2025-05-24 14:52:21,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:52:21,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:52:21,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'oo@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:52:21,455 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:52:21,455 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:52:21,455 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde deze twee dia &apos; s zien dat de arctische ijs koop , die voor de meeste drie miljoen jaar de grootste drie miljoen jaar de grootte van de lagere 48 procent .
2025-05-24 14:52:21,455 - INFO - joeynmt.training - Example #1
2025-05-24 14:52:21,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:52:21,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:52:21,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'u@@', 'ze', 'van', 'dit', 'onder@@', 'ste@@', 'un@@', 't', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:52:21,456 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:52:21,456 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:52:21,456 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serieuze van dit ondersteunt omdat het de dickness van het ijs niet de dickness van het ijs .
2025-05-24 14:52:21,456 - INFO - joeynmt.training - Example #2
2025-05-24 14:52:21,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:52:21,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:52:21,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'be@@', 'per@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:52:21,456 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:52:21,457 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:52:21,457 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in een zin , het beperklimaatsysteem .
2025-05-24 14:52:21,457 - INFO - joeynmt.training - Example #3
2025-05-24 14:52:21,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:52:21,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:52:21,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:52:21,458 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:52:21,458 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:52:21,458 - INFO - joeynmt.training - 	Hypothesis: Het expands in winter en contracten in zomer .
2025-05-24 14:52:21,458 - INFO - joeynmt.training - Example #4
2025-05-24 14:52:21,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:52:21,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:52:21,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', '.', '</s>']
2025-05-24 14:52:21,459 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:52:21,459 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:52:21,459 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid een rapid vooruit van wat er gebeurd is gebeurd .
2025-05-24 14:52:26,615 - INFO - joeynmt.training - Epoch   5, Step:    19100, Batch Loss:     1.540781, Batch Acc: 0.593741, Tokens per Sec:    13843, Lr: 0.000300
2025-05-24 14:52:31,734 - INFO - joeynmt.training - Epoch   5, Step:    19200, Batch Loss:     1.369466, Batch Acc: 0.588219, Tokens per Sec:    13909, Lr: 0.000300
2025-05-24 14:52:36,919 - INFO - joeynmt.training - Epoch   5, Step:    19300, Batch Loss:     1.489563, Batch Acc: 0.590509, Tokens per Sec:    14284, Lr: 0.000300
2025-05-24 14:52:42,105 - INFO - joeynmt.training - Epoch   5, Step:    19400, Batch Loss:     1.440254, Batch Acc: 0.587714, Tokens per Sec:    13968, Lr: 0.000300
2025-05-24 14:52:47,216 - INFO - joeynmt.training - Epoch   5, Step:    19500, Batch Loss:     1.463765, Batch Acc: 0.591159, Tokens per Sec:    13863, Lr: 0.000300
2025-05-24 14:52:47,216 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:52:47,216 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:52:56,649 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.80, acc:   0.51, generation: 9.4179[sec], evaluation: 0.0000[sec]
2025-05-24 14:52:56,726 - INFO - joeynmt.helpers - delete models/bpe_2k/17000.ckpt
2025-05-24 14:52:56,732 - INFO - joeynmt.training - Example #0
2025-05-24 14:52:56,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:52:56,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:52:56,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'ca@@', 'p', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'k@@', 'la@@', 'p', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', '.', '</s>']
2025-05-24 14:52:56,733 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:52:56,733 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:52:56,733 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoed ik deze twee dia &apos; s zo dat de arctisch ijs cap , die de laatste drie miljoen jaar de grootste klap van de lagere 48 % van de lagere 48 % .
2025-05-24 14:52:56,733 - INFO - joeynmt.training - Example #1
2025-05-24 14:52:56,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriënt van dit specifieke probleem omdat het het niet de thickness van het ijs .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - Example #2
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs cap is , in zekere zin , het hart van het wereldklimaat .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - Example #3
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:52:56,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'der', 'en', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:52:56,734 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:52:56,735 - INFO - joeynmt.training - 	Hypothesis: Het verder en de winter en contracten in zomer .
2025-05-24 14:52:56,735 - INFO - joeynmt.training - Example #4
2025-05-24 14:52:56,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:52:56,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:52:56,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'zal', 'een', 'r@@', 'ap@@', 'id', 'zal', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'van', 'wat', 'er', 'is', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:52:56,735 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:52:56,735 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:52:56,735 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie zal een rapid zal een rapid zijn van wat er is gebeurd is .
2025-05-24 14:53:01,875 - INFO - joeynmt.training - Epoch   5, Step:    19600, Batch Loss:     1.365693, Batch Acc: 0.587463, Tokens per Sec:    14303, Lr: 0.000300
2025-05-24 14:53:06,995 - INFO - joeynmt.training - Epoch   5, Step:    19700, Batch Loss:     1.462876, Batch Acc: 0.591522, Tokens per Sec:    14128, Lr: 0.000300
2025-05-24 14:53:12,109 - INFO - joeynmt.training - Epoch   5, Step:    19800, Batch Loss:     1.389246, Batch Acc: 0.591510, Tokens per Sec:    14135, Lr: 0.000300
2025-05-24 14:53:17,188 - INFO - joeynmt.training - Epoch   5, Step:    19900, Batch Loss:     1.347798, Batch Acc: 0.597173, Tokens per Sec:    14794, Lr: 0.000300
2025-05-24 14:53:19,796 - INFO - joeynmt.training - Epoch   5: total training loss 5710.36
2025-05-24 14:53:19,796 - INFO - joeynmt.training - EPOCH 6
2025-05-24 14:53:22,368 - INFO - joeynmt.training - Epoch   6, Step:    20000, Batch Loss:     1.264425, Batch Acc: 0.617874, Tokens per Sec:    13857, Lr: 0.000300
2025-05-24 14:53:22,370 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:53:22,370 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:53:31,270 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.72, acc:   0.51, generation: 8.8863[sec], evaluation: 0.0000[sec]
2025-05-24 14:53:31,270 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:53:31,353 - INFO - joeynmt.helpers - delete models/bpe_2k/17500.ckpt
2025-05-24 14:53:31,358 - INFO - joeynmt.training - Example #0
2025-05-24 14:53:31,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:53:31,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:53:31,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'le@@', 'ed', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'sk@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', ',', 'wat', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'om@@', 'stan@@', 'dig@@', 'heid', 'van', 'de', 'la@@', 'ger', '4@@', '8', '%', '.', '</s>']
2025-05-24 14:53:31,359 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:53:31,359 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:53:31,360 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar leed ik deze twee dia &apos; s zo dat de arctische ijsktische ijs cap , wat voor de meeste drie miljoen jaar de omstandigheid van de lager 48 % .
2025-05-24 14:53:31,360 - INFO - joeynmt.training - Example #1
2025-05-24 14:53:31,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:53:31,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:53:31,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:53:31,360 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:53:31,360 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:53:31,360 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriënt van dit specifieke probleem omdat het de dickness van het ijs niet van de ijs .
2025-05-24 14:53:31,360 - INFO - joeynmt.training - Example #2
2025-05-24 14:53:31,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:53:31,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:53:31,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'be@@', 'st@@', 'rij@@', 'den', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:53:31,361 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:53:31,361 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:53:31,361 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in een gevoel , het bestrijden van de wereldwijde klimaatsysteem .
2025-05-24 14:53:31,361 - INFO - joeynmt.training - Example #3
2025-05-24 14:53:31,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:53:31,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:53:31,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:53:31,361 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:53:31,361 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:53:31,361 - INFO - joeynmt.training - 	Hypothesis: Het verwachtten in de zomer en contracten in zomer .
2025-05-24 14:53:31,361 - INFO - joeynmt.training - Example #4
2025-05-24 14:53:31,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:53:31,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:53:31,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'in', 'de', 'laatste', '2@@', '5', 'jaar', 'gebeur@@', 'de', 'er', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:53:31,362 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:53:31,362 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:53:31,362 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid zal zijn van wat er in de laatste 25 jaar gebeurde er in de afgelopen 25 jaar .
2025-05-24 14:53:36,487 - INFO - joeynmt.training - Epoch   6, Step:    20100, Batch Loss:     1.176036, Batch Acc: 0.615135, Tokens per Sec:    13864, Lr: 0.000300
2025-05-24 14:53:41,588 - INFO - joeynmt.training - Epoch   6, Step:    20200, Batch Loss:     1.248445, Batch Acc: 0.608830, Tokens per Sec:    14442, Lr: 0.000300
2025-05-24 14:53:46,731 - INFO - joeynmt.training - Epoch   6, Step:    20300, Batch Loss:     1.503973, Batch Acc: 0.602592, Tokens per Sec:    14496, Lr: 0.000300
2025-05-24 14:53:51,847 - INFO - joeynmt.training - Epoch   6, Step:    20400, Batch Loss:     1.295553, Batch Acc: 0.609919, Tokens per Sec:    14606, Lr: 0.000300
2025-05-24 14:53:56,925 - INFO - joeynmt.training - Epoch   6, Step:    20500, Batch Loss:     1.540211, Batch Acc: 0.610955, Tokens per Sec:    14387, Lr: 0.000300
2025-05-24 14:53:56,926 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:53:56,926 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:54:05,753 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.72, acc:   0.51, generation: 8.8154[sec], evaluation: 0.0000[sec]
2025-05-24 14:54:05,754 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:54:05,834 - INFO - joeynmt.helpers - delete models/bpe_2k/18500.ckpt
2025-05-24 14:54:05,839 - INFO - joeynmt.training - Example #0
2025-05-24 14:54:05,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:54:05,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:54:05,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', ',', 'dat', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'lo@@', 'wer', '4@@', '8', '%', 'van', 'de', 'lo@@', 'wer', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:54:05,840 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:54:05,840 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:54:05,840 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zo dat de arctische ijs kap , dat de meeste drie miljoen jaar de grootte van de lower 48 % van de lower 40 % .
2025-05-24 14:54:05,840 - INFO - joeynmt.training - Example #1
2025-05-24 14:54:05,840 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:54:05,840 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:54:05,840 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'u@@', 'w@@', 'is', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 14:54:05,840 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:54:05,840 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:54:05,840 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serieuwis van dit specifieke probleem omdat het niet de thickness van het ijs van het ijs niet laten zien .
2025-05-24 14:54:05,840 - INFO - joeynmt.training - Example #2
2025-05-24 14:54:05,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:54:05,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:54:05,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'aar@@', 't', 'is', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'ged@@', 'ur@@', 'ende', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:54:05,841 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:54:05,841 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:54:05,841 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kaart is in een gevoel , het gedurende hart van het wereldklimaat .
2025-05-24 14:54:05,841 - INFO - joeynmt.training - Example #3
2025-05-24 14:54:05,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:54:05,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:54:05,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'ru@@', 'i@@', 'ken', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:54:05,842 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:54:05,842 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:54:05,842 - INFO - joeynmt.training - 	Hypothesis: Het verbruiken in de zomer en contracten in de zomer .
2025-05-24 14:54:05,842 - INFO - joeynmt.training - Example #4
2025-05-24 14:54:05,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:54:05,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:54:05,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:54:05,842 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:54:05,842 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:54:05,842 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik een rapid een rapid vooruit van wat er gebeurde de de laatste 25 jaar .
2025-05-24 14:54:10,911 - INFO - joeynmt.training - Epoch   6, Step:    20600, Batch Loss:     1.688937, Batch Acc: 0.603624, Tokens per Sec:    14022, Lr: 0.000300
2025-05-24 14:54:16,036 - INFO - joeynmt.training - Epoch   6, Step:    20700, Batch Loss:     1.366111, Batch Acc: 0.606091, Tokens per Sec:    14756, Lr: 0.000300
2025-05-24 14:54:21,212 - INFO - joeynmt.training - Epoch   6, Step:    20800, Batch Loss:     1.501258, Batch Acc: 0.607640, Tokens per Sec:    13908, Lr: 0.000300
2025-05-24 14:54:26,383 - INFO - joeynmt.training - Epoch   6, Step:    20900, Batch Loss:     1.514278, Batch Acc: 0.611768, Tokens per Sec:    14059, Lr: 0.000300
2025-05-24 14:54:31,579 - INFO - joeynmt.training - Epoch   6, Step:    21000, Batch Loss:     1.371418, Batch Acc: 0.601910, Tokens per Sec:    14148, Lr: 0.000300
2025-05-24 14:54:31,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:54:31,580 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:54:41,580 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.69, acc:   0.51, generation: 9.9879[sec], evaluation: 0.0000[sec]
2025-05-24 14:54:41,580 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:54:41,662 - INFO - joeynmt.helpers - delete models/bpe_2k/18000.ckpt
2025-05-24 14:54:41,668 - INFO - joeynmt.training - Example #0
2025-05-24 14:54:41,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:54:41,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:54:41,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'laten', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'gev@@', 'al', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:54:41,669 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:54:41,669 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:54:41,669 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s laten zien dat de arctische ijs geval , die de meeste drie miljoen jaar de grootste van de lager 48 jaar de grootte van de lager 40 procent .
2025-05-24 14:54:41,669 - INFO - joeynmt.training - Example #1
2025-05-24 14:54:41,670 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:54:41,670 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:54:41,670 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 14:54:41,670 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:54:41,670 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:54:41,670 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriënt van dit specifieke probleem omdat het niet de thickness van het ijs niet van de ijs .
2025-05-24 14:54:41,670 - INFO - joeynmt.training - Example #2
2025-05-24 14:54:41,670 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:54:41,670 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:54:41,670 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'ged@@', 'ur@@', 'ende', 'het', 'hoof@@', 'd@@', 'k@@', 'op@@', 'a@@', 'al@@', 'k@@', 'ter@@', 't', '.', '</s>']
2025-05-24 14:54:41,671 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:54:41,671 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:54:41,671 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in zekere zin het gedurende het hoofdkopaalktert .
2025-05-24 14:54:41,671 - INFO - joeynmt.training - Example #3
2025-05-24 14:54:41,671 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:54:41,671 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:54:41,671 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:54:41,672 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:54:41,672 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:54:41,672 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in zomer .
2025-05-24 14:54:41,672 - INFO - joeynmt.training - Example #4
2025-05-24 14:54:41,672 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:54:41,672 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:54:41,672 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'm@@', 'ul@@', 'er', 'een', 'r@@', 'ap@@', 'm@@', 'ul@@', 'er', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:54:41,672 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:54:41,672 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:54:41,672 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik een rapmuler een rapmuler van wat er gebeurd is gebeurd gebeurd is .
2025-05-24 14:54:47,006 - INFO - joeynmt.training - Epoch   6, Step:    21100, Batch Loss:     1.414600, Batch Acc: 0.603380, Tokens per Sec:    13383, Lr: 0.000300
2025-05-24 14:54:52,142 - INFO - joeynmt.training - Epoch   6, Step:    21200, Batch Loss:     1.427502, Batch Acc: 0.603389, Tokens per Sec:    14113, Lr: 0.000300
2025-05-24 14:54:57,230 - INFO - joeynmt.training - Epoch   6, Step:    21300, Batch Loss:     1.157169, Batch Acc: 0.605045, Tokens per Sec:    14671, Lr: 0.000300
2025-05-24 14:55:02,265 - INFO - joeynmt.training - Epoch   6, Step:    21400, Batch Loss:     1.292500, Batch Acc: 0.607839, Tokens per Sec:    14646, Lr: 0.000300
2025-05-24 14:55:07,239 - INFO - joeynmt.training - Epoch   6, Step:    21500, Batch Loss:     1.446825, Batch Acc: 0.604670, Tokens per Sec:    14725, Lr: 0.000300
2025-05-24 14:55:07,239 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:55:07,240 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:55:16,119 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.51, generation: 8.8668[sec], evaluation: 0.0000[sec]
2025-05-24 14:55:16,197 - INFO - joeynmt.helpers - delete models/bpe_2k/19500.ckpt
2025-05-24 14:55:16,203 - INFO - joeynmt.training - Example #0
2025-05-24 14:55:16,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:55:16,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:55:16,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', '-@@', 'k@@', 'a@@', 'p', '&apos;', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:55:16,203 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:55:16,203 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:55:16,204 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zo dat de arctische ijs-kap &apos; , die de laatste drie miljoen jaar de grootte van de lagere 48 % van de lagere 40 % .
2025-05-24 14:55:16,204 - INFO - joeynmt.training - Example #1
2025-05-24 14:55:16,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:55:16,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:55:16,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ou@@', 'ten', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 14:55:16,204 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:55:16,204 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:55:16,204 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriouten van dit specifieke probleem omdat het de thickness van het ijs niet tonen .
2025-05-24 14:55:16,204 - INFO - joeynmt.training - Example #2
2025-05-24 14:55:16,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:55:16,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:55:16,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'b@@', 'ouw@@', 'kun@@', 'st', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'lim@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:55:16,205 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:55:16,206 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:55:16,206 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het bouwkunst van de wereldklimmaat .
2025-05-24 14:55:16,206 - INFO - joeynmt.training - Example #3
2025-05-24 14:55:16,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:55:16,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:55:16,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'dt', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:55:16,206 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:55:16,206 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:55:16,206 - INFO - joeynmt.training - 	Hypothesis: Het verbreidt in winter en contracten in zomer .
2025-05-24 14:55:16,206 - INFO - joeynmt.training - Example #4
2025-05-24 14:55:16,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:55:16,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:55:16,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:55:16,207 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:55:16,207 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:55:16,207 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid een rapid fast-vooruit van wat er gebeurd is in de afgelopen 25 jaar .
2025-05-24 14:55:21,243 - INFO - joeynmt.training - Epoch   6, Step:    21600, Batch Loss:     1.404438, Batch Acc: 0.603405, Tokens per Sec:    14800, Lr: 0.000300
2025-05-24 14:55:26,402 - INFO - joeynmt.training - Epoch   6, Step:    21700, Batch Loss:     1.565957, Batch Acc: 0.602283, Tokens per Sec:    14299, Lr: 0.000300
2025-05-24 14:55:31,647 - INFO - joeynmt.training - Epoch   6, Step:    21800, Batch Loss:     1.514249, Batch Acc: 0.601466, Tokens per Sec:    14025, Lr: 0.000300
2025-05-24 14:55:36,872 - INFO - joeynmt.training - Epoch   6, Step:    21900, Batch Loss:     1.383516, Batch Acc: 0.605319, Tokens per Sec:    13846, Lr: 0.000300
2025-05-24 14:55:42,002 - INFO - joeynmt.training - Epoch   6, Step:    22000, Batch Loss:     1.241103, Batch Acc: 0.603146, Tokens per Sec:    14500, Lr: 0.000300
2025-05-24 14:55:42,002 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:55:42,002 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:55:51,849 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.66, acc:   0.51, generation: 9.8333[sec], evaluation: 0.0000[sec]
2025-05-24 14:55:51,849 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:55:51,927 - INFO - joeynmt.helpers - delete models/bpe_2k/19000.ckpt
2025-05-24 14:55:51,932 - INFO - joeynmt.training - Example #0
2025-05-24 14:55:51,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:55:51,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:55:51,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', '.', '</s>']
2025-05-24 14:55:51,933 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:55:51,933 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:55:51,933 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zien dat de arctische ijs cap , die de meeste drie miljoen jaar de grootste van de lagere 48 % .
2025-05-24 14:55:51,933 - INFO - joeynmt.training - Example #1
2025-05-24 14:55:51,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:55:51,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:55:51,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'le', 'van', 'dit', 'be@@', 'pa@@', 'al@@', 'de', 'probleem', 'omdat', 'het', 'de', 'th@@', 'ic@@', 'k@@', 'n@@', 'is', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:55:51,934 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:55:51,934 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:55:51,934 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriële van dit bepaalde probleem omdat het de thicknis van het ijs .
2025-05-24 14:55:51,934 - INFO - joeynmt.training - Example #2
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'ver@@', 'le@@', 'den', 'har@@', 't', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:55:51,935 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:55:51,935 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:55:51,935 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in een zin , het verleden hart van de wereldklimaatsysteem .
2025-05-24 14:55:51,935 - INFO - joeynmt.training - Example #3
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'grot@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:55:51,935 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:55:51,935 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:55:51,935 - INFO - joeynmt.training - 	Hypothesis: Het vergrott in winter en contracten in de zomer .
2025-05-24 14:55:51,935 - INFO - joeynmt.training - Example #4
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:55:51,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'jullie', 'een', 'r@@', 'ap@@', 'i@@', 'id', 'zal', 'een', 'r@@', 'ap@@', 'i@@', 'id', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:55:51,937 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:55:51,937 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:55:51,937 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat jullie een rapiid zal een rapiid van wat er gebeurde de de afgelopen 25 jaar .
2025-05-24 14:55:56,860 - INFO - joeynmt.training - Epoch   6, Step:    22100, Batch Loss:     1.550205, Batch Acc: 0.597006, Tokens per Sec:    14559, Lr: 0.000300
2025-05-24 14:56:01,940 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.283121, Batch Acc: 0.604485, Tokens per Sec:    14386, Lr: 0.000300
2025-05-24 14:56:06,828 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.366420, Batch Acc: 0.603506, Tokens per Sec:    14827, Lr: 0.000300
2025-05-24 14:56:11,953 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.457925, Batch Acc: 0.600395, Tokens per Sec:    14811, Lr: 0.000300
2025-05-24 14:56:17,024 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.324554, Batch Acc: 0.608189, Tokens per Sec:    14309, Lr: 0.000300
2025-05-24 14:56:17,024 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:56:17,024 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:56:26,784 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.62, acc:   0.52, generation: 9.7448[sec], evaluation: 0.0000[sec]
2025-05-24 14:56:26,784 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:56:26,862 - INFO - joeynmt.helpers - delete models/bpe_2k/20000.ckpt
2025-05-24 14:56:26,867 - INFO - joeynmt.training - Example #0
2025-05-24 14:56:26,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:56:26,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:56:26,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', 'is', 'ge@@', 'we@@', 'est', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:56:26,868 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:56:26,868 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:56:26,868 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien dat de arctische kap , die de meeste drie miljoen jaar de grootste van de lagere 48 % is geweest van de lagere 40 procent .
2025-05-24 14:56:26,868 - INFO - joeynmt.training - Example #1
2025-05-24 14:56:26,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:56:26,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:56:26,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:56:26,868 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:56:26,868 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:56:26,869 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriënt van dit specifieke probleem omdat het het niet de thickness van het ijs .
2025-05-24 14:56:26,869 - INFO - joeynmt.training - Example #2
2025-05-24 14:56:26,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:56:26,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:56:26,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', 'de', 'k@@', 'li@@', 'ma@@', 'at', 'systeem', '.', '</s>']
2025-05-24 14:56:26,869 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:56:26,869 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:56:26,869 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in een zin de klimaat systeem .
2025-05-24 14:56:26,869 - INFO - joeynmt.training - Example #3
2025-05-24 14:56:26,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:56:26,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:56:26,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'an@@', 'den', 'in', 'het', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:56:26,870 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:56:26,870 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:56:26,870 - INFO - joeynmt.training - 	Hypothesis: Het verwaanden in het winter en contracten in de zomer .
2025-05-24 14:56:26,870 - INFO - joeynmt.training - Example #4
2025-05-24 14:56:26,870 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:56:26,870 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:56:26,870 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'van', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:56:26,870 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:56:26,870 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:56:26,870 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid zal zijn van wat er gebeurde van de laatste 25 jaar .
2025-05-24 14:56:32,044 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.387293, Batch Acc: 0.604976, Tokens per Sec:    14107, Lr: 0.000300
2025-05-24 14:56:37,484 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.556048, Batch Acc: 0.608158, Tokens per Sec:    13528, Lr: 0.000300
2025-05-24 14:56:42,719 - INFO - joeynmt.training - Epoch   6, Step:    22800, Batch Loss:     1.343987, Batch Acc: 0.604634, Tokens per Sec:    14448, Lr: 0.000300
2025-05-24 14:56:47,873 - INFO - joeynmt.training - Epoch   6, Step:    22900, Batch Loss:     1.398729, Batch Acc: 0.609921, Tokens per Sec:    14215, Lr: 0.000300
2025-05-24 14:56:52,921 - INFO - joeynmt.training - Epoch   6, Step:    23000, Batch Loss:     1.347233, Batch Acc: 0.604823, Tokens per Sec:    14639, Lr: 0.000300
2025-05-24 14:56:52,921 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:56:52,921 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:57:02,055 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.52, generation: 9.1190[sec], evaluation: 0.0000[sec]
2025-05-24 14:57:02,055 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:57:02,134 - INFO - joeynmt.helpers - delete models/bpe_2k/20500.ckpt
2025-05-24 14:57:02,140 - INFO - joeynmt.training - Example #0
2025-05-24 14:57:02,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:57:02,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:57:02,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', '&apos;', 'zo', '&apos;', 'n', 'beetje', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:57:02,141 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:57:02,141 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:57:02,141 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s &apos; zo &apos; n beetje de arctische ijs van de laatste drie miljoen jaar de grootste deel van de laatste drie miljoen jaar de grootte van de lagere 40 % .
2025-05-24 14:57:02,141 - INFO - joeynmt.training - Example #1
2025-05-24 14:57:02,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:57:02,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:57:02,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'ei@@', 'ding', 'van', 'het', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 14:57:02,142 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:57:02,142 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:57:02,142 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheiding van het seriënt van dit specifieke probleem omdat het de thickness van het ijs laat zien .
2025-05-24 14:57:02,142 - INFO - joeynmt.training - Example #2
2025-05-24 14:57:02,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:57:02,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:57:02,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'de', 'k@@', 'li@@', 'ma@@', 'at', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 14:57:02,142 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:57:02,143 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:57:02,143 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in zekere zin de klimaat van het wereldklimaat .
2025-05-24 14:57:02,143 - INFO - joeynmt.training - Example #3
2025-05-24 14:57:02,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:57:02,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:57:02,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'grot@@', 'en@@', 'de@@', 'els', 'in', 'het', 'sp@@', 'r@@', 'ingen', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:57:02,143 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:57:02,143 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:57:02,143 - INFO - joeynmt.training - 	Hypothesis: Het vergrotendeels in het springen en contracten in de zomer .
2025-05-24 14:57:02,143 - INFO - joeynmt.training - Example #4
2025-05-24 14:57:02,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:57:02,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:57:02,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:57:02,144 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:57:02,144 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:57:02,144 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien een rapid zal zijn van wat er gebeurde de laatste 25 jaar .
2025-05-24 14:57:07,325 - INFO - joeynmt.training - Epoch   6, Step:    23100, Batch Loss:     1.350227, Batch Acc: 0.599981, Tokens per Sec:    13959, Lr: 0.000300
2025-05-24 14:57:12,435 - INFO - joeynmt.training - Epoch   6, Step:    23200, Batch Loss:     1.378135, Batch Acc: 0.602860, Tokens per Sec:    14236, Lr: 0.000300
2025-05-24 14:57:17,496 - INFO - joeynmt.training - Epoch   6, Step:    23300, Batch Loss:     1.236845, Batch Acc: 0.604384, Tokens per Sec:    14191, Lr: 0.000300
2025-05-24 14:57:22,557 - INFO - joeynmt.training - Epoch   6, Step:    23400, Batch Loss:     1.341460, Batch Acc: 0.599333, Tokens per Sec:    14345, Lr: 0.000300
2025-05-24 14:57:27,634 - INFO - joeynmt.training - Epoch   6, Step:    23500, Batch Loss:     1.417256, Batch Acc: 0.596836, Tokens per Sec:    14109, Lr: 0.000300
2025-05-24 14:57:27,634 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:57:27,634 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:57:36,527 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.59, acc:   0.52, generation: 8.8810[sec], evaluation: 0.0000[sec]
2025-05-24 14:57:36,527 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:57:36,604 - INFO - joeynmt.helpers - delete models/bpe_2k/21500.ckpt
2025-05-24 14:57:36,609 - INFO - joeynmt.training - Example #0
2025-05-24 14:57:36,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:57:36,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:57:36,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'enen', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', '-@@', 'ij@@', 's@@', '-@@', 'ij@@', 's@@', '-@@', 'ij@@', 's@@', 'eren', ',', 'dat', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', '.', '</s>']
2025-05-24 14:57:36,609 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:57:36,609 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:57:36,609 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoenen ik deze twee dia &apos; s zo dat de arctische ijs-ijs-ijs-ijseren , dat voor de meeste drie miljoen jaar de grootte van de lagere 48 % .
2025-05-24 14:57:36,612 - INFO - joeynmt.training - Example #1
2025-05-24 14:57:36,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:57:36,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:57:36,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'i@@', 'ver@@', 's', 'van', 'dit', 'be@@', 'pa@@', 'al@@', 'de', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:57:36,612 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:57:36,612 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:57:36,612 - INFO - joeynmt.training - 	Hypothesis: Maar deze begrip de serivers van dit bepaalde probleem omdat het niet de thicknisheid van het ijs van het ijs .
2025-05-24 14:57:36,612 - INFO - joeynmt.training - Example #2
2025-05-24 14:57:36,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:57:36,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:57:36,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sch', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'het', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:57:36,613 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:57:36,613 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:57:36,613 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs cap is , het hart van het wereldwijde klimaatsysteem .
2025-05-24 14:57:36,613 - INFO - joeynmt.training - Example #3
2025-05-24 14:57:36,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:57:36,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:57:36,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'en', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:57:36,614 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:57:36,614 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:57:36,614 - INFO - joeynmt.training - 	Hypothesis: Het verbreien in de zomer en contracten in zomer .
2025-05-24 14:57:36,614 - INFO - joeynmt.training - Example #4
2025-05-24 14:57:36,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:57:36,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:57:36,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'een', 'sn@@', 'el@@', 'le', 'sn@@', 'el@@', 'le', 'zal', 'zijn', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 14:57:36,614 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:57:36,614 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:57:36,614 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie een snelle snelle zal zijn vooruit van wat er gebeurd is .
2025-05-24 14:57:41,693 - INFO - joeynmt.training - Epoch   6, Step:    23600, Batch Loss:     1.308942, Batch Acc: 0.602574, Tokens per Sec:    13898, Lr: 0.000300
2025-05-24 14:57:46,762 - INFO - joeynmt.training - Epoch   6, Step:    23700, Batch Loss:     1.323118, Batch Acc: 0.608117, Tokens per Sec:    14588, Lr: 0.000300
2025-05-24 14:57:51,878 - INFO - joeynmt.training - Epoch   6, Step:    23800, Batch Loss:     1.551918, Batch Acc: 0.596893, Tokens per Sec:    14293, Lr: 0.000300
2025-05-24 14:57:56,971 - INFO - joeynmt.training - Epoch   6, Step:    23900, Batch Loss:     1.333132, Batch Acc: 0.607396, Tokens per Sec:    14839, Lr: 0.000300
2025-05-24 14:57:59,173 - INFO - joeynmt.training - Epoch   6: total training loss 5495.54
2025-05-24 14:57:59,173 - INFO - joeynmt.training - EPOCH 7
2025-05-24 14:58:02,108 - INFO - joeynmt.training - Epoch   7, Step:    24000, Batch Loss:     1.319511, Batch Acc: 0.617067, Tokens per Sec:    13974, Lr: 0.000300
2025-05-24 14:58:02,109 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:58:02,109 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:58:11,791 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.59, acc:   0.52, generation: 9.6665[sec], evaluation: 0.0000[sec]
2025-05-24 14:58:11,792 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:58:11,867 - INFO - joeynmt.helpers - delete models/bpe_2k/21000.ckpt
2025-05-24 14:58:11,874 - INFO - joeynmt.training - Example #0
2025-05-24 14:58:11,874 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:58:11,874 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:58:11,874 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'was', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'sk@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'sp@@', 'ro@@', 'ken', 'met', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 14:58:11,874 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:58:11,874 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:58:11,874 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar was ik deze twee dia &apos; s zo dat de arctische ijsktische ijs , die voor de meeste drie miljoen jaar de grootte van de lagere 48 staten , heeft gesproken met 40 procent .
2025-05-24 14:58:11,874 - INFO - joeynmt.training - Example #1
2025-05-24 14:58:11,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:58:11,875 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:58:11,875 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'th@@', 'ic@@', 'k@@', 'ni@@', 'p@@', 'per@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 14:58:11,875 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:58:11,875 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:58:11,875 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriënt van dit specifieke probleem omdat het thicknippers van het ijs .
2025-05-24 14:58:11,875 - INFO - joeynmt.training - Example #2
2025-05-24 14:58:11,875 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:58:11,875 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:58:11,875 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'en@@', 'ste', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'het', 'be@@', 'st@@', 'rij@@', 'den', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:58:11,876 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:58:11,876 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:58:11,876 - INFO - joeynmt.training - 	Hypothesis: De arctienste ijs kap is , het bestrijden van het wereldklimaatsysteem .
2025-05-24 14:58:11,876 - INFO - joeynmt.training - Example #3
2025-05-24 14:58:11,876 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:58:11,876 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:58:11,876 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'wa@@', 'cht@@', 'ingen', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:58:11,877 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:58:11,877 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:58:11,877 - INFO - joeynmt.training - 	Hypothesis: Het verwachtingen in winter en contracten in zomer .
2025-05-24 14:58:11,877 - INFO - joeynmt.training - Example #4
2025-05-24 14:58:11,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:58:11,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:58:11,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'van', 'de', 'volgende', 'dia', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', 'gebeur@@', 'de', '.', '</s>']
2025-05-24 14:58:11,877 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:58:11,877 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:58:11,877 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid van de volgende dia zal zijn van wat er gebeurde de laatste 25 jaar gebeurde .
2025-05-24 14:58:17,040 - INFO - joeynmt.training - Epoch   7, Step:    24100, Batch Loss:     1.345113, Batch Acc: 0.627073, Tokens per Sec:    14233, Lr: 0.000300
2025-05-24 14:58:22,145 - INFO - joeynmt.training - Epoch   7, Step:    24200, Batch Loss:     1.185600, Batch Acc: 0.624061, Tokens per Sec:    14527, Lr: 0.000300
2025-05-24 14:58:27,320 - INFO - joeynmt.training - Epoch   7, Step:    24300, Batch Loss:     1.351526, Batch Acc: 0.618065, Tokens per Sec:    14129, Lr: 0.000300
2025-05-24 14:58:32,419 - INFO - joeynmt.training - Epoch   7, Step:    24400, Batch Loss:     1.280384, Batch Acc: 0.624221, Tokens per Sec:    14453, Lr: 0.000300
2025-05-24 14:58:37,599 - INFO - joeynmt.training - Epoch   7, Step:    24500, Batch Loss:     1.323890, Batch Acc: 0.622562, Tokens per Sec:    14254, Lr: 0.000300
2025-05-24 14:58:37,599 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:58:37,599 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:58:47,037 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.55, acc:   0.52, generation: 9.4250[sec], evaluation: 0.0000[sec]
2025-05-24 14:58:47,037 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:58:47,117 - INFO - joeynmt.helpers - delete models/bpe_2k/22000.ckpt
2025-05-24 14:58:47,123 - INFO - joeynmt.training - Example #0
2025-05-24 14:58:47,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:58:47,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:58:47,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'sk@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', 'heeft', 'ge@@', 'st@@', 'eld', '.', '</s>']
2025-05-24 14:58:47,123 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:58:47,123 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:58:47,123 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien dat demonstreren dat de arctische ijsktische ijs , die de meeste drie miljoen jaar de grootte van de lagere 48 % heeft gesteld .
2025-05-24 14:58:47,123 - INFO - joeynmt.training - Example #1
2025-05-24 14:58:47,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:58:47,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:58:47,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'be@@', 'pa@@', 'al@@', 'd', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'n@@', 'is', 'van', 'het', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 14:58:47,125 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:58:47,125 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:58:47,125 - INFO - joeynmt.training - 	Hypothesis: Maar dit begrip de seriënt van dit bepaald probleem omdat het niet de dicknis van het ijs niet .
2025-05-24 14:58:47,125 - INFO - joeynmt.training - Example #2
2025-05-24 14:58:47,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:58:47,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:58:47,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', '-@@', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'be@@', 'per@@', 'kt', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:58:47,126 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:58:47,126 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:58:47,126 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs-kap is , in een zin , het beperkt van het wereldklimaatsysteem .
2025-05-24 14:58:47,126 - INFO - joeynmt.training - Example #3
2025-05-24 14:58:47,126 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:58:47,126 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:58:47,126 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'uit@@', 'bre@@', 'iden', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:58:47,126 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:58:47,126 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:58:47,126 - INFO - joeynmt.training - 	Hypothesis: Het uitbreiden in winter en contracten in de zomer .
2025-05-24 14:58:47,127 - INFO - joeynmt.training - Example #4
2025-05-24 14:58:47,127 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:58:47,127 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:58:47,127 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zal', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:58:47,127 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:58:47,127 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:58:47,127 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik jullie een rapid zal een rapid zijn van wat er gebeurd is in de laatste 25 jaar .
2025-05-24 14:58:52,465 - INFO - joeynmt.training - Epoch   7, Step:    24600, Batch Loss:     1.448485, Batch Acc: 0.616762, Tokens per Sec:    13681, Lr: 0.000300
2025-05-24 14:58:57,543 - INFO - joeynmt.training - Epoch   7, Step:    24700, Batch Loss:     1.252827, Batch Acc: 0.618334, Tokens per Sec:    13806, Lr: 0.000300
2025-05-24 14:59:02,652 - INFO - joeynmt.training - Epoch   7, Step:    24800, Batch Loss:     1.370239, Batch Acc: 0.616897, Tokens per Sec:    14100, Lr: 0.000300
2025-05-24 14:59:07,776 - INFO - joeynmt.training - Epoch   7, Step:    24900, Batch Loss:     1.289488, Batch Acc: 0.616285, Tokens per Sec:    14521, Lr: 0.000300
2025-05-24 14:59:12,906 - INFO - joeynmt.training - Epoch   7, Step:    25000, Batch Loss:     1.362460, Batch Acc: 0.616239, Tokens per Sec:    14387, Lr: 0.000300
2025-05-24 14:59:12,906 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:59:12,906 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:59:22,242 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.56, acc:   0.52, generation: 9.3213[sec], evaluation: 0.0000[sec]
2025-05-24 14:59:22,321 - INFO - joeynmt.helpers - delete models/bpe_2k/22500.ckpt
2025-05-24 14:59:22,326 - INFO - joeynmt.training - Example #0
2025-05-24 14:59:22,326 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:59:22,326 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:59:22,326 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'me@@', 'est@@', 'al', 'zien', 'de', 'groot@@', 'ste', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'we@@', 'est', 'ge@@', 'ge@@', 'we@@', 'est', 'van', '4@@', '0', '%', '.', '</s>']
2025-05-24 14:59:22,327 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:59:22,327 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:59:22,327 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar ik deze twee dia &apos; s zien dat de arctische ijs de meestal zien de grootste van de laatste drie miljoen jaar de grootte van de lager 48 staten , heeft geweest gegeweest van 40 % .
2025-05-24 14:59:22,327 - INFO - joeynmt.training - Example #1
2025-05-24 14:59:22,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:59:22,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:59:22,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'ei@@', 'den', 'de', 'ser@@', 'i@@', 'ver@@', 's', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'th@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 14:59:22,328 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:59:22,328 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:59:22,328 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheiden de serivers van dit specifieke probleem omdat het de thicknisheid van het ijs niet .
2025-05-24 14:59:22,328 - INFO - joeynmt.training - Example #2
2025-05-24 14:59:22,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:59:22,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:59:22,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'arti@@', 'kel', 'k@@', 'a@@', 'p', 'is', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'be@@', 'k@@', 'end', 'har@@', 't', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at', 'systeem', '.', '</s>']
2025-05-24 14:59:22,328 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:59:22,328 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:59:22,328 - INFO - joeynmt.training - 	Hypothesis: De artikel kap is in een gevoel , het bekend hart van de wereldklimaat systeem .
2025-05-24 14:59:22,329 - INFO - joeynmt.training - Example #3
2025-05-24 14:59:22,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:59:22,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:59:22,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'an@@', 'der@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:59:22,330 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:59:22,330 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:59:22,330 - INFO - joeynmt.training - 	Hypothesis: Het expandert in de winter en contracten in de zomer .
2025-05-24 14:59:22,330 - INFO - joeynmt.training - Example #4
2025-05-24 14:59:22,330 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:59:22,330 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:59:22,330 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'zal', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'is', 'gebeur@@', 'd', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:59:22,330 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:59:22,330 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:59:22,330 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie zal een rapid zijn van wat er gebeurde is gebeurd in de laatste 25 jaar .
2025-05-24 14:59:27,487 - INFO - joeynmt.training - Epoch   7, Step:    25100, Batch Loss:     1.352659, Batch Acc: 0.619689, Tokens per Sec:    14096, Lr: 0.000300
2025-05-24 14:59:32,608 - INFO - joeynmt.training - Epoch   7, Step:    25200, Batch Loss:     1.240249, Batch Acc: 0.616151, Tokens per Sec:    14627, Lr: 0.000300
2025-05-24 14:59:37,697 - INFO - joeynmt.training - Epoch   7, Step:    25300, Batch Loss:     1.111103, Batch Acc: 0.617275, Tokens per Sec:    14064, Lr: 0.000300
2025-05-24 14:59:42,852 - INFO - joeynmt.training - Epoch   7, Step:    25400, Batch Loss:     1.128410, Batch Acc: 0.616616, Tokens per Sec:    14155, Lr: 0.000300
2025-05-24 14:59:47,946 - INFO - joeynmt.training - Epoch   7, Step:    25500, Batch Loss:     1.214093, Batch Acc: 0.616777, Tokens per Sec:    14248, Lr: 0.000300
2025-05-24 14:59:47,946 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:59:47,946 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:59:57,206 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.53, acc:   0.52, generation: 9.2458[sec], evaluation: 0.0000[sec]
2025-05-24 14:59:57,208 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 14:59:57,290 - INFO - joeynmt.helpers - delete models/bpe_2k/23000.ckpt
2025-05-24 14:59:57,296 - INFO - joeynmt.training - Example #0
2025-05-24 14:59:57,296 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 14:59:57,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 14:59:57,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'procent', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 14:59:57,296 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 14:59:57,296 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 14:59:57,296 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien dat de arctische kap , die de laatste drie miljoen jaar de grootte van de lager 48 procent van de lager 48 procent .
2025-05-24 14:59:57,296 - INFO - joeynmt.training - Example #1
2025-05-24 14:59:57,297 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 14:59:57,297 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 14:59:57,297 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'ei@@', 'den', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'de', '.', '</s>']
2025-05-24 14:59:57,297 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 14:59:57,297 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 14:59:57,297 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderscheiden de seriënt van dit specifieke probleem omdat het niet de dickness van het ijs niet de .
2025-05-24 14:59:57,297 - INFO - joeynmt.training - Example #2
2025-05-24 14:59:57,297 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 14:59:57,297 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 14:59:57,297 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', 'de', 'gev@@', 'oel', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 14:59:57,297 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 14:59:57,298 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 14:59:57,298 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in een zin de gevoel van de wereldklimaatsysteem .
2025-05-24 14:59:57,298 - INFO - joeynmt.training - Example #3
2025-05-24 14:59:57,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 14:59:57,298 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 14:59:57,298 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 14:59:57,298 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 14:59:57,298 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 14:59:57,298 - INFO - joeynmt.training - 	Hypothesis: Het expands in winter en contracten in zomer .
2025-05-24 14:59:57,298 - INFO - joeynmt.training - Example #4
2025-05-24 14:59:57,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 14:59:57,298 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 14:59:57,298 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'ik', 'jullie', 'een', 'sn@@', 'el@@', 'le', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'is', 'gebeur@@', 'd', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 14:59:57,299 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 14:59:57,299 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 14:59:57,299 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat ik jullie een snelle snelle voorwaarts is gebeurd in de laatste 25 jaar .
2025-05-24 15:00:02,403 - INFO - joeynmt.training - Epoch   7, Step:    25600, Batch Loss:     1.412633, Batch Acc: 0.616758, Tokens per Sec:    14275, Lr: 0.000300
2025-05-24 15:00:07,561 - INFO - joeynmt.training - Epoch   7, Step:    25700, Batch Loss:     1.313245, Batch Acc: 0.613480, Tokens per Sec:    14399, Lr: 0.000300
2025-05-24 15:00:12,989 - INFO - joeynmt.training - Epoch   7, Step:    25800, Batch Loss:     1.343244, Batch Acc: 0.617672, Tokens per Sec:    13487, Lr: 0.000300
2025-05-24 15:00:18,175 - INFO - joeynmt.training - Epoch   7, Step:    25900, Batch Loss:     1.365891, Batch Acc: 0.615774, Tokens per Sec:    14402, Lr: 0.000300
2025-05-24 15:00:23,351 - INFO - joeynmt.training - Epoch   7, Step:    26000, Batch Loss:     1.302073, Batch Acc: 0.616994, Tokens per Sec:    14080, Lr: 0.000300
2025-05-24 15:00:23,351 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:00:23,351 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:00:33,103 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.51, acc:   0.52, generation: 9.7381[sec], evaluation: 0.0000[sec]
2025-05-24 15:00:33,103 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:00:33,193 - INFO - joeynmt.helpers - delete models/bpe_2k/23500.ckpt
2025-05-24 15:00:33,200 - INFO - joeynmt.training - Example #0
2025-05-24 15:00:33,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:00:33,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:00:33,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'met', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 15:00:33,200 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:00:33,200 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:00:33,200 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien dat de arctische kap kaart , die voor de meeste drie miljoen jaar de grootte van de lager 48 staten , met de grootte van de lager 40 procent .
2025-05-24 15:00:33,200 - INFO - joeynmt.training - Example #1
2025-05-24 15:00:33,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:00:33,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:00:33,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'ei@@', 'den', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'het', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:00:33,201 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:00:33,201 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:00:33,201 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheiden de seriënt van dit specifieke probleem omdat het het dickness van het ijs van het ijs .
2025-05-24 15:00:33,201 - INFO - joeynmt.training - Example #2
2025-05-24 15:00:33,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:00:33,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:00:33,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'ver@@', 'le@@', 'den', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:00:33,202 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:00:33,202 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:00:33,202 - INFO - joeynmt.training - 	Hypothesis: De arctische ijscap is , in zekere zin , het verleden van het wereldwijde klimaatsysteem .
2025-05-24 15:00:33,202 - INFO - joeynmt.training - Example #3
2025-05-24 15:00:33,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:00:33,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:00:33,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'der', 'in', 'de', 'w@@', 'on@@', 'teren', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:00:33,203 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:00:33,203 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:00:33,203 - INFO - joeynmt.training - 	Hypothesis: Het verder in de wonteren en contracten in zomer .
2025-05-24 15:00:33,203 - INFO - joeynmt.training - Example #4
2025-05-24 15:00:33,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:00:33,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:00:33,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'voor@@', 'uit', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:00:33,203 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:00:33,203 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:00:33,203 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie een rapid fast-vooruit van wat er gebeurd is in de laatste 25 jaar .
2025-05-24 15:00:38,389 - INFO - joeynmt.training - Epoch   7, Step:    26100, Batch Loss:     1.413678, Batch Acc: 0.613416, Tokens per Sec:    14015, Lr: 0.000300
2025-05-24 15:00:43,503 - INFO - joeynmt.training - Epoch   7, Step:    26200, Batch Loss:     1.216635, Batch Acc: 0.616473, Tokens per Sec:    14252, Lr: 0.000300
2025-05-24 15:00:48,528 - INFO - joeynmt.training - Epoch   7, Step:    26300, Batch Loss:     1.348801, Batch Acc: 0.616136, Tokens per Sec:    14471, Lr: 0.000300
2025-05-24 15:00:53,544 - INFO - joeynmt.training - Epoch   7, Step:    26400, Batch Loss:     1.262542, Batch Acc: 0.608046, Tokens per Sec:    14807, Lr: 0.000300
2025-05-24 15:00:58,709 - INFO - joeynmt.training - Epoch   7, Step:    26500, Batch Loss:     1.401308, Batch Acc: 0.613682, Tokens per Sec:    14084, Lr: 0.000300
2025-05-24 15:00:58,709 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:00:58,709 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:01:08,020 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.49, acc:   0.52, generation: 9.2994[sec], evaluation: 0.0000[sec]
2025-05-24 15:01:08,020 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:01:08,106 - INFO - joeynmt.helpers - delete models/bpe_2k/24000.ckpt
2025-05-24 15:01:08,112 - INFO - joeynmt.training - Example #0
2025-05-24 15:01:08,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:01:08,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:01:08,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'laten', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'tot', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:01:08,112 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:01:08,112 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:01:08,112 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s laten zien dat de arctische kap , die voor de meeste drie miljoen jaar de grootte van de lager 48 tot 48 procent .
2025-05-24 15:01:08,112 - INFO - joeynmt.training - Example #1
2025-05-24 15:01:08,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:01:08,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:01:08,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'ei@@', 'dt', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'probleem', ',', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ni@@', 'e', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:01:08,114 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:01:08,114 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:01:08,114 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderscheidt de seriënt van dit probleem , omdat het niet de thicknie van het ijs .
2025-05-24 15:01:08,114 - INFO - joeynmt.training - Example #2
2025-05-24 15:01:08,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:01:08,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:01:08,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', ',', 'in', 'een', 'z@@', 'in', ',', 'het', 'z@@', 'in', ',', 'het', 'bij@@', 'voorbeeld', 'het', 'be@@', 'per@@', 'kt', 'systeem', '.', '</s>']
2025-05-24 15:01:08,115 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:01:08,115 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:01:08,115 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het zin , het bijvoorbeeld het beperkt systeem .
2025-05-24 15:01:08,115 - INFO - joeynmt.training - Example #3
2025-05-24 15:01:08,115 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:01:08,115 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:01:08,115 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'onden', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:01:08,116 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:01:08,116 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:01:08,116 - INFO - joeynmt.training - 	Hypothesis: Het verbonden in winter en contracten in zomer .
2025-05-24 15:01:08,116 - INFO - joeynmt.training - Example #4
2025-05-24 15:01:08,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:01:08,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:01:08,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'zien', 'dat', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'vor@@', 'en', 'van', 'wat', 'er', 'gebeur@@', 'de', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:01:08,116 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:01:08,116 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:01:08,117 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat zien dat ik jullie een rapid fast-voren van wat er gebeurde in de afgelopen 25 jaar .
2025-05-24 15:01:13,334 - INFO - joeynmt.training - Epoch   7, Step:    26600, Batch Loss:     1.272211, Batch Acc: 0.618601, Tokens per Sec:    13948, Lr: 0.000300
2025-05-24 15:01:18,500 - INFO - joeynmt.training - Epoch   7, Step:    26700, Batch Loss:     1.314329, Batch Acc: 0.614152, Tokens per Sec:    14366, Lr: 0.000300
2025-05-24 15:01:23,813 - INFO - joeynmt.training - Epoch   7, Step:    26800, Batch Loss:     1.400080, Batch Acc: 0.614269, Tokens per Sec:    13882, Lr: 0.000300
2025-05-24 15:01:28,915 - INFO - joeynmt.training - Epoch   7, Step:    26900, Batch Loss:     1.417105, Batch Acc: 0.613832, Tokens per Sec:    14091, Lr: 0.000300
2025-05-24 15:01:33,859 - INFO - joeynmt.training - Epoch   7, Step:    27000, Batch Loss:     1.272965, Batch Acc: 0.615012, Tokens per Sec:    14824, Lr: 0.000300
2025-05-24 15:01:33,859 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:01:33,859 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:01:43,276 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.48, acc:   0.52, generation: 9.4047[sec], evaluation: 0.0000[sec]
2025-05-24 15:01:43,277 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:01:43,358 - INFO - joeynmt.helpers - delete models/bpe_2k/25000.ckpt
2025-05-24 15:01:43,363 - INFO - joeynmt.training - Example #0
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'laten', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', '.', '</s>']
2025-05-24 15:01:43,363 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:01:43,363 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:01:43,363 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s laten zien dat de arctische ijs cap , die voor de meeste drie miljoen jaar de grootte van de lagere 48 % .
2025-05-24 15:01:43,363 - INFO - joeynmt.training - Example #1
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'stel@@', 'ling', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', ',', 'want', 'het', 'laat', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:01:43,363 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:01:43,363 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:01:43,363 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstelling de seriënt van dit specifieke probleem , want het laat niet de thickness van het ijs .
2025-05-24 15:01:43,363 - INFO - joeynmt.training - Example #2
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:01:43,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:01:43,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'b@@', 'ar@@', 't@@', 's@@', 'systeem', '.', '</s>']
2025-05-24 15:01:43,365 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:01:43,365 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:01:43,365 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in zekere zin , het bartssysteem .
2025-05-24 15:01:43,365 - INFO - joeynmt.training - Example #3
2025-05-24 15:01:43,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:01:43,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:01:43,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'ru@@', 'i@@', 'm', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:01:43,365 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:01:43,365 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:01:43,365 - INFO - joeynmt.training - 	Hypothesis: Het verbruim en contracten in zomer .
2025-05-24 15:01:43,366 - INFO - joeynmt.training - Example #4
2025-05-24 15:01:43,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:01:43,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:01:43,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'laten', 'zien', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'vor@@', '-@@', 'vor@@', 'ming', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:01:43,366 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:01:43,366 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:01:43,366 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat laten zien een rapid fast-vor-vorming van wat er gebeurde de de afgelopen 25 jaar .
2025-05-24 15:01:48,560 - INFO - joeynmt.training - Epoch   7, Step:    27100, Batch Loss:     1.247331, Batch Acc: 0.611104, Tokens per Sec:    14101, Lr: 0.000300
2025-05-24 15:01:53,787 - INFO - joeynmt.training - Epoch   7, Step:    27200, Batch Loss:     1.377820, Batch Acc: 0.613359, Tokens per Sec:    14264, Lr: 0.000300
2025-05-24 15:01:58,972 - INFO - joeynmt.training - Epoch   7, Step:    27300, Batch Loss:     1.412714, Batch Acc: 0.616308, Tokens per Sec:    14696, Lr: 0.000300
2025-05-24 15:02:04,121 - INFO - joeynmt.training - Epoch   7, Step:    27400, Batch Loss:     1.520642, Batch Acc: 0.610989, Tokens per Sec:    14377, Lr: 0.000300
2025-05-24 15:02:09,149 - INFO - joeynmt.training - Epoch   7, Step:    27500, Batch Loss:     1.311650, Batch Acc: 0.615464, Tokens per Sec:    14504, Lr: 0.000300
2025-05-24 15:02:09,149 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:02:09,149 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:02:18,771 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.46, acc:   0.52, generation: 9.6072[sec], evaluation: 0.0000[sec]
2025-05-24 15:02:18,771 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:02:18,853 - INFO - joeynmt.helpers - delete models/bpe_2k/24500.ckpt
2025-05-24 15:02:18,859 - INFO - joeynmt.training - Example #0
2025-05-24 15:02:18,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:02:18,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:02:18,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'gev@@', 'al', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'h@@', 'oord', '.', '</s>']
2025-05-24 15:02:18,860 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:02:18,860 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:02:18,861 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zo dat de arctische ijs geval , die de meeste drie miljoen jaar de grootte van de lager 48 staten , heeft gehoord .
2025-05-24 15:02:18,861 - INFO - joeynmt.training - Example #1
2025-05-24 15:02:18,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:02:18,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:02:18,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'at@@', 'ie@@', 'u@@', 'w@@', 's', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 15:02:18,861 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:02:18,861 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:02:18,861 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstatieuws van dit specifieke probleem omdat het niet de thickness van het ijs niet laat zien .
2025-05-24 15:02:18,861 - INFO - joeynmt.training - Example #2
2025-05-24 15:02:18,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:02:18,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:02:18,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'str@@', 'oo@@', 'm@@', 'systeem', '.', '</s>']
2025-05-24 15:02:18,862 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:02:18,862 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:02:18,862 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in zekere zin het stroomsysteem .
2025-05-24 15:02:18,862 - INFO - joeynmt.training - Example #3
2025-05-24 15:02:18,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:02:18,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:02:18,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'iden', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:02:18,862 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:02:18,862 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:02:18,862 - INFO - joeynmt.training - 	Hypothesis: Het verbreiden in winter en contracten in de zomer .
2025-05-24 15:02:18,862 - INFO - joeynmt.training - Example #4
2025-05-24 15:02:18,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:02:18,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:02:18,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'vor@@', 'en', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:02:18,864 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:02:18,864 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:02:18,864 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat jullie een rapid fast-voren van wat er gebeurde de de afgelopen 25 jaar .
2025-05-24 15:02:23,961 - INFO - joeynmt.training - Epoch   7, Step:    27600, Batch Loss:     1.354265, Batch Acc: 0.615443, Tokens per Sec:    14052, Lr: 0.000300
2025-05-24 15:02:29,101 - INFO - joeynmt.training - Epoch   7, Step:    27700, Batch Loss:     1.307541, Batch Acc: 0.616699, Tokens per Sec:    14150, Lr: 0.000300
2025-05-24 15:02:34,275 - INFO - joeynmt.training - Epoch   7, Step:    27800, Batch Loss:     1.310952, Batch Acc: 0.619486, Tokens per Sec:    14459, Lr: 0.000300
2025-05-24 15:02:39,413 - INFO - joeynmt.training - Epoch   7, Step:    27900, Batch Loss:     1.301801, Batch Acc: 0.610749, Tokens per Sec:    13991, Lr: 0.000300
2025-05-24 15:02:41,138 - INFO - joeynmt.training - Epoch   7: total training loss 5320.70
2025-05-24 15:02:41,138 - INFO - joeynmt.training - EPOCH 8
2025-05-24 15:02:44,386 - INFO - joeynmt.training - Epoch   8, Step:    28000, Batch Loss:     1.282169, Batch Acc: 0.636285, Tokens per Sec:    15718, Lr: 0.000300
2025-05-24 15:02:44,386 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:02:44,386 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:02:53,754 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.45, acc:   0.53, generation: 9.3574[sec], evaluation: 0.0000[sec]
2025-05-24 15:02:53,755 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:02:53,829 - INFO - joeynmt.helpers - delete models/bpe_2k/25500.ckpt
2025-05-24 15:02:53,835 - INFO - joeynmt.training - Example #0
2025-05-24 15:02:53,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:02:53,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:02:53,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'aar@@', 't', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'ze@@', 'gd', '.', '</s>']
2025-05-24 15:02:53,835 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:02:53,835 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:02:53,835 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien dat de arctische kaart , die voor de meeste drie miljoen jaar de grootste deel van de lager 48 staten , heeft gezegd .
2025-05-24 15:02:53,835 - INFO - joeynmt.training - Example #1
2025-05-24 15:02:53,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:02:53,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:02:53,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'stel@@', 'ling', 'de', 'de', 'ser@@', 'i@@', 'ë@@', 'le', 'probleem', 'omdat', 'het', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 15:02:53,835 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:02:53,835 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:02:53,835 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstelling de de seriële probleem omdat het de thickness van het ijs niet tonen .
2025-05-24 15:02:53,836 - INFO - joeynmt.training - Example #2
2025-05-24 15:02:53,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:02:53,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:02:53,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'en@@', '-@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'ver@@', 'b@@', 'and', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:02:53,836 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:02:53,836 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:02:53,836 - INFO - joeynmt.training - 	Hypothesis: De arctien-cap is , in zekere zin , het verband hart van het wereldwijde klimaatsysteem .
2025-05-24 15:02:53,836 - INFO - joeynmt.training - Example #3
2025-05-24 15:02:53,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:02:53,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:02:53,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'b@@', 'b@@', 'roe@@', 'dt', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:02:53,837 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:02:53,837 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:02:53,837 - INFO - joeynmt.training - 	Hypothesis: Het dubbroedt in winter en contracten in de zomer .
2025-05-24 15:02:53,837 - INFO - joeynmt.training - Example #4
2025-05-24 15:02:53,837 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:02:53,837 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:02:53,837 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'ton@@', 'en', 'dat', 'je', 'een', 'sn@@', 'el@@', 'le', 'voor@@', 'uit', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:02:53,837 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:02:53,837 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:02:53,837 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik tonen dat je een snelle vooruit wat er gebeurde de de laatste 25 jaar .
2025-05-24 15:02:58,819 - INFO - joeynmt.training - Epoch   8, Step:    28100, Batch Loss:     1.215340, Batch Acc: 0.635118, Tokens per Sec:    14373, Lr: 0.000300
2025-05-24 15:03:03,805 - INFO - joeynmt.training - Epoch   8, Step:    28200, Batch Loss:     1.462612, Batch Acc: 0.630470, Tokens per Sec:    14825, Lr: 0.000300
2025-05-24 15:03:08,823 - INFO - joeynmt.training - Epoch   8, Step:    28300, Batch Loss:     1.256526, Batch Acc: 0.633056, Tokens per Sec:    14762, Lr: 0.000300
2025-05-24 15:03:13,906 - INFO - joeynmt.training - Epoch   8, Step:    28400, Batch Loss:     1.166205, Batch Acc: 0.631398, Tokens per Sec:    14561, Lr: 0.000300
2025-05-24 15:03:18,892 - INFO - joeynmt.training - Epoch   8, Step:    28500, Batch Loss:     1.367151, Batch Acc: 0.627959, Tokens per Sec:    14846, Lr: 0.000300
2025-05-24 15:03:18,892 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:03:18,892 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:03:28,165 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.47, acc:   0.52, generation: 9.2611[sec], evaluation: 0.0000[sec]
2025-05-24 15:03:28,245 - INFO - joeynmt.helpers - delete models/bpe_2k/26000.ckpt
2025-05-24 15:03:28,251 - INFO - joeynmt.training - Example #0
2025-05-24 15:03:28,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:03:28,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:03:28,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'we@@', 'est', '.', '</s>']
2025-05-24 15:03:28,252 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:03:28,252 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:03:28,252 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien dat de arctische kap , die voor de meeste drie miljoen jaar de grootte van de lagere 48 staten , heeft geweest .
2025-05-24 15:03:28,252 - INFO - joeynmt.training - Example #1
2025-05-24 15:03:28,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:03:28,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:03:28,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'ei@@', 'dt', 'het', 'ser@@', 'i@@', 'ë@@', 'le', 'probleem', ',', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 15:03:28,252 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:03:28,252 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:03:28,252 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheidt het seriële probleem , omdat het niet de thickness van het ijs niet tonen .
2025-05-24 15:03:28,253 - INFO - joeynmt.training - Example #2
2025-05-24 15:03:28,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:03:28,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:03:28,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'ged@@', 'ru@@', 'kt', 'het', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:03:28,253 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:03:28,253 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:03:28,253 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in een gevoel , het gedrukt het hart van het wereldwijde klimaatsysteem .
2025-05-24 15:03:28,253 - INFO - joeynmt.training - Example #3
2025-05-24 15:03:28,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:03:28,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:03:28,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:03:28,254 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:03:28,254 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:03:28,254 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in zomer .
2025-05-24 15:03:28,254 - INFO - joeynmt.training - Example #4
2025-05-24 15:03:28,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:03:28,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:03:28,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'ton@@', 'en', 'dat', 'je', 'een', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'is', 'van', 'wat', 'er', 'gebeur@@', 'de', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:03:28,254 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:03:28,254 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:03:28,254 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik tonen dat je een snelle voorwaarts is van wat er gebeurde in de afgelopen 25 jaar .
2025-05-24 15:03:33,379 - INFO - joeynmt.training - Epoch   8, Step:    28600, Batch Loss:     1.299707, Batch Acc: 0.635412, Tokens per Sec:    14151, Lr: 0.000300
2025-05-24 15:03:38,384 - INFO - joeynmt.training - Epoch   8, Step:    28700, Batch Loss:     1.224688, Batch Acc: 0.631923, Tokens per Sec:    14205, Lr: 0.000300
2025-05-24 15:03:43,483 - INFO - joeynmt.training - Epoch   8, Step:    28800, Batch Loss:     1.396040, Batch Acc: 0.629473, Tokens per Sec:    14177, Lr: 0.000300
2025-05-24 15:03:48,615 - INFO - joeynmt.training - Epoch   8, Step:    28900, Batch Loss:     1.285593, Batch Acc: 0.625795, Tokens per Sec:    14308, Lr: 0.000300
2025-05-24 15:03:53,619 - INFO - joeynmt.training - Epoch   8, Step:    29000, Batch Loss:     1.379690, Batch Acc: 0.628872, Tokens per Sec:    14924, Lr: 0.000300
2025-05-24 15:03:53,620 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:03:53,620 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:04:02,993 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.47, acc:   0.52, generation: 9.3589[sec], evaluation: 0.0000[sec]
2025-05-24 15:04:03,089 - INFO - joeynmt.helpers - delete models/bpe_2k/26500.ckpt
2025-05-24 15:04:03,096 - INFO - joeynmt.training - Example #0
2025-05-24 15:04:03,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:04:03,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:04:03,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'h@@', 'on@@', 'k', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 15:04:03,098 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:04:03,098 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:04:03,098 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien dat de arctische ijs cap , die voor de meeste drie miljoen jaar de grootste deel van de lager 48 staten , heeft gehonk door 40 procent .
2025-05-24 15:04:03,098 - INFO - joeynmt.training - Example #1
2025-05-24 15:04:03,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:04:03,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:04:03,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'ei@@', 'ding', 'van', 'de', 'ser@@', 'i@@', 'ster@@', 's', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 15:04:03,099 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:04:03,099 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:04:03,099 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderscheiding van de seristers van dit probleem omdat het de thickness van het ijs laat zien .
2025-05-24 15:04:03,099 - INFO - joeynmt.training - Example #2
2025-05-24 15:04:03,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:04:03,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:04:03,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'ver@@', 'z@@', 'am@@', 'el@@', 'ing', 'van', 'de', 'glob@@', 'ale', 'k@@', 'li@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 15:04:03,100 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:04:03,100 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:04:03,100 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in zekere zin , het verzameling van de globale klimaat .
2025-05-24 15:04:03,100 - INFO - joeynmt.training - Example #3
2025-05-24 15:04:03,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:04:03,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:04:03,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:04:03,101 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:04:03,101 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:04:03,101 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in zomer .
2025-05-24 15:04:03,101 - INFO - joeynmt.training - Example #4
2025-05-24 15:04:03,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:04:03,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:04:03,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'laten', 'zien', 'dat', 'ik', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:04:03,102 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:04:03,102 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:04:03,102 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat laten zien dat ik een rapid zal zijn van wat er gebeurde de laatste 25 jaar .
2025-05-24 15:04:09,029 - INFO - joeynmt.training - Epoch   8, Step:    29100, Batch Loss:     1.186976, Batch Acc: 0.627491, Tokens per Sec:    12364, Lr: 0.000300
2025-05-24 15:04:14,788 - INFO - joeynmt.training - Epoch   8, Step:    29200, Batch Loss:     1.294132, Batch Acc: 0.625684, Tokens per Sec:    12692, Lr: 0.000300
2025-05-24 15:04:19,996 - INFO - joeynmt.training - Epoch   8, Step:    29300, Batch Loss:     1.325987, Batch Acc: 0.620882, Tokens per Sec:    13980, Lr: 0.000300
2025-05-24 15:04:25,011 - INFO - joeynmt.training - Epoch   8, Step:    29400, Batch Loss:     1.259880, Batch Acc: 0.626330, Tokens per Sec:    14527, Lr: 0.000300
2025-05-24 15:04:30,036 - INFO - joeynmt.training - Epoch   8, Step:    29500, Batch Loss:     1.156960, Batch Acc: 0.625408, Tokens per Sec:    14991, Lr: 0.000300
2025-05-24 15:04:30,036 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:04:30,036 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:04:39,120 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.43, acc:   0.53, generation: 9.0715[sec], evaluation: 0.0000[sec]
2025-05-24 15:04:39,121 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:04:39,195 - INFO - joeynmt.helpers - delete models/bpe_2k/27000.ckpt
2025-05-24 15:04:39,201 - INFO - joeynmt.training - Example #0
2025-05-24 15:04:39,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:04:39,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:04:39,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'groot@@', 'ste', 'e@@', 'eu@@', 'wi@@', 'g', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', '.', '</s>']
2025-05-24 15:04:39,202 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:04:39,202 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:04:39,202 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien dat de arctische ijs de grootste eeuwig , die voor de meeste drie miljoen jaar de grootte van de lagere 48 % .
2025-05-24 15:04:39,202 - INFO - joeynmt.training - Example #1
2025-05-24 15:04:39,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:04:39,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:04:39,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'e@@', 'id', 'van', 'dit', 'ser@@', 'i@@', 'ou@@', 'ten', ',', 'omdat', 'het', 'de', 'di@@', 'en@@', 'st', 'van', 'dit', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 15:04:39,202 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:04:39,202 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:04:39,202 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheid van dit seriouten , omdat het de dienst van dit ijs laat zien .
2025-05-24 15:04:39,202 - INFO - joeynmt.training - Example #2
2025-05-24 15:04:39,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:04:39,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:04:39,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'en@@', 'ste', 'ij@@', 's', 'z@@', 'in', ',', 'het', 'be@@', 'per@@', 'k@@', 'ende', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:04:39,203 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:04:39,203 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:04:39,203 - INFO - joeynmt.training - 	Hypothesis: De arctienste ijs zin , het beperkende hart van het wereldklimaatsysteem .
2025-05-24 15:04:39,203 - INFO - joeynmt.training - Example #3
2025-05-24 15:04:39,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:04:39,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:04:39,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'ru@@', 'i@@', 'm', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:04:39,204 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:04:39,204 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:04:39,204 - INFO - joeynmt.training - 	Hypothesis: Het verbruim en contracten in zomer .
2025-05-24 15:04:39,204 - INFO - joeynmt.training - Example #4
2025-05-24 15:04:39,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:04:39,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:04:39,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'zal', 'een', 'sne@@', 'l', 'laten', 'zien', 'van', 'wat', 'er', 'is', 'gebeur@@', 'd', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:04:39,204 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:04:39,204 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:04:39,204 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie zal een snel laten zien van wat er is gebeurd in de laatste 25 jaar .
2025-05-24 15:04:44,356 - INFO - joeynmt.training - Epoch   8, Step:    29600, Batch Loss:     1.185841, Batch Acc: 0.628243, Tokens per Sec:    14082, Lr: 0.000300
2025-05-24 15:04:49,362 - INFO - joeynmt.training - Epoch   8, Step:    29700, Batch Loss:     1.278384, Batch Acc: 0.624642, Tokens per Sec:    14707, Lr: 0.000300
2025-05-24 15:04:54,310 - INFO - joeynmt.training - Epoch   8, Step:    29800, Batch Loss:     1.350756, Batch Acc: 0.627151, Tokens per Sec:    14429, Lr: 0.000300
2025-05-24 15:04:59,381 - INFO - joeynmt.training - Epoch   8, Step:    29900, Batch Loss:     1.290102, Batch Acc: 0.626721, Tokens per Sec:    14784, Lr: 0.000300
2025-05-24 15:05:04,330 - INFO - joeynmt.training - Epoch   8, Step:    30000, Batch Loss:     1.250467, Batch Acc: 0.628562, Tokens per Sec:    14973, Lr: 0.000300
2025-05-24 15:05:04,331 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:05:04,331 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:05:13,699 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.44, acc:   0.52, generation: 9.3550[sec], evaluation: 0.0000[sec]
2025-05-24 15:05:13,777 - INFO - joeynmt.helpers - delete models/bpe_2k/28500.ckpt
2025-05-24 15:05:13,783 - INFO - joeynmt.training - Example #0
2025-05-24 15:05:13,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:05:13,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:05:13,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:05:13,783 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:05:13,784 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:05:13,784 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar de ik deze twee dia &apos; s zien dat de arctische ijs die de arctische ijs , die voor de meeste drie miljoen jaar de grootte van de lagere 48 procent .
2025-05-24 15:05:13,784 - INFO - joeynmt.training - Example #1
2025-05-24 15:05:13,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:05:13,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:05:13,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'sch@@', 'e@@', 'f', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'ker', 'niet', 'de', 'd@@', 'oo@@', 'g', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:05:13,784 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:05:13,784 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:05:13,784 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderschef de seriënt van dit specifieke probleem omdat het de dicker niet de doog van het ijs .
2025-05-24 15:05:13,784 - INFO - joeynmt.training - Example #2
2025-05-24 15:05:13,785 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:05:13,785 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:05:13,785 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'ver@@', 'le@@', 'den', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:05:13,785 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in zekere zin , het verleden hart van het globale klimaatsysteem .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - Example #3
2025-05-24 15:05:13,786 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:05:13,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:05:13,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in de zomer .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - Example #4
2025-05-24 15:05:13,786 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:05:13,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:05:13,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'sne@@', 'l', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:05:13,786 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie een rapid snel zal zijn van wat er gebeurde de de laatste 25 jaar .
2025-05-24 15:05:18,954 - INFO - joeynmt.training - Epoch   8, Step:    30100, Batch Loss:     1.141147, Batch Acc: 0.626653, Tokens per Sec:    13648, Lr: 0.000300
2025-05-24 15:05:24,071 - INFO - joeynmt.training - Epoch   8, Step:    30200, Batch Loss:     1.296256, Batch Acc: 0.624983, Tokens per Sec:    14557, Lr: 0.000300
2025-05-24 15:05:29,152 - INFO - joeynmt.training - Epoch   8, Step:    30300, Batch Loss:     1.252069, Batch Acc: 0.623152, Tokens per Sec:    14295, Lr: 0.000300
2025-05-24 15:05:34,429 - INFO - joeynmt.training - Epoch   8, Step:    30400, Batch Loss:     1.300664, Batch Acc: 0.624331, Tokens per Sec:    13858, Lr: 0.000300
2025-05-24 15:05:39,414 - INFO - joeynmt.training - Epoch   8, Step:    30500, Batch Loss:     1.472055, Batch Acc: 0.619980, Tokens per Sec:    14516, Lr: 0.000300
2025-05-24 15:05:39,414 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:05:39,414 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:05:49,020 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.43, acc:   0.53, generation: 9.5923[sec], evaluation: 0.0000[sec]
2025-05-24 15:05:49,020 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:05:49,099 - INFO - joeynmt.helpers - delete models/bpe_2k/29000.ckpt
2025-05-24 15:05:49,104 - INFO - joeynmt.training - Example #0
2025-05-24 15:05:49,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:05:49,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:05:49,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'k@@', 'a@@', 'p', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:05:49,105 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:05:49,105 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:05:49,105 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien dat de arctische ijs de kap , die voor de meeste drie miljoen jaar de grootte van de lagere 48 procent .
2025-05-24 15:05:49,105 - INFO - joeynmt.training - Example #1
2025-05-24 15:05:49,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:05:49,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:05:49,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'di@@', 'k@@', 'ke', 'van', 'het', 'ij@@', 's', 'niet', 'de', 'di@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:05:49,106 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:05:49,106 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:05:49,106 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriënt van dit specifieke probleem omdat het dikke van het ijs niet de diickness van het ijs .
2025-05-24 15:05:49,106 - INFO - joeynmt.training - Example #2
2025-05-24 15:05:49,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:05:49,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:05:49,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'oo@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'be@@', 'st@@', 'rij@@', 'den', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:05:49,107 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:05:49,107 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:05:49,107 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs koop is , in zekere zin , het bestrijden van het wereldklimaatsysteem .
2025-05-24 15:05:49,107 - INFO - joeynmt.training - Example #3
2025-05-24 15:05:49,107 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:05:49,107 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:05:49,107 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'iden', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:05:49,108 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:05:49,108 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:05:49,108 - INFO - joeynmt.training - 	Hypothesis: Het verbreiden in de zomer en contracten in zomer .
2025-05-24 15:05:49,108 - INFO - joeynmt.training - Example #4
2025-05-24 15:05:49,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:05:49,108 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:05:49,108 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'zal', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'voor@@', 'uit', 'wat', 'er', 'is', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 15:05:49,108 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:05:49,109 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:05:49,109 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik zal jullie een rapid fast-vooruit wat er is gebeurd is .
2025-05-24 15:05:54,192 - INFO - joeynmt.training - Epoch   8, Step:    30600, Batch Loss:     1.297202, Batch Acc: 0.623360, Tokens per Sec:    14188, Lr: 0.000300
2025-05-24 15:05:59,243 - INFO - joeynmt.training - Epoch   8, Step:    30700, Batch Loss:     1.420274, Batch Acc: 0.626778, Tokens per Sec:    14697, Lr: 0.000300
2025-05-24 15:06:04,215 - INFO - joeynmt.training - Epoch   8, Step:    30800, Batch Loss:     1.258150, Batch Acc: 0.618186, Tokens per Sec:    14794, Lr: 0.000300
2025-05-24 15:06:09,254 - INFO - joeynmt.training - Epoch   8, Step:    30900, Batch Loss:     1.413248, Batch Acc: 0.624208, Tokens per Sec:    14412, Lr: 0.000300
2025-05-24 15:06:14,273 - INFO - joeynmt.training - Epoch   8, Step:    31000, Batch Loss:     1.376762, Batch Acc: 0.627620, Tokens per Sec:    15296, Lr: 0.000300
2025-05-24 15:06:14,273 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:06:14,274 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:06:23,070 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.37, acc:   0.53, generation: 8.7837[sec], evaluation: 0.0000[sec]
2025-05-24 15:06:23,070 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:06:23,149 - INFO - joeynmt.helpers - delete models/bpe_2k/27500.ckpt
2025-05-24 15:06:23,155 - INFO - joeynmt.training - Example #0
2025-05-24 15:06:23,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:06:23,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:06:23,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'af@@', 'gel@@', 'open', '4@@', '8', '%', '.', '</s>']
2025-05-24 15:06:23,156 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:06:23,156 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:06:23,156 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zien dat de arctische ijs , die voor de meeste drie miljoen jaar de grootte van de afgelopen 48 % .
2025-05-24 15:06:23,156 - INFO - joeynmt.training - Example #1
2025-05-24 15:06:23,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:06:23,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:06:23,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'eren', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'het', 'de', 'di@@', 'en@@', 'st', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 15:06:23,157 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:06:23,157 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:06:23,157 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serieren van dit specifieke probleem omdat het het de dienst van het ijs niet tonen .
2025-05-24 15:06:23,157 - INFO - joeynmt.training - Example #2
2025-05-24 15:06:23,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:06:23,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:06:23,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'een', 'gev@@', 'oel', ',', 'het', 'be@@', 'st@@', 'rij@@', 'den', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:06:23,157 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:06:23,157 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:06:23,157 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in een gevoel , het bestrijden van het wereldwijde klimaatsysteem .
2025-05-24 15:06:23,157 - INFO - joeynmt.training - Example #3
2025-05-24 15:06:23,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:06:23,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:06:23,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'u@@', 'dt', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:06:23,158 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:06:23,158 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:06:23,158 - INFO - joeynmt.training - 	Hypothesis: Het verbreudt in de winter en contracten in de zomer .
2025-05-24 15:06:23,158 - INFO - joeynmt.training - Example #4
2025-05-24 15:06:23,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:06:23,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:06:23,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'voor@@', 'uit', 'van', 'wat', 'er', 'is', 'gebeur@@', 'd', 'gebeur@@', 'd', 'is', '.', '</s>']
2025-05-24 15:06:23,159 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:06:23,159 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:06:23,159 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik je een rapid fast-vooruit van wat er is gebeurd gebeurd is .
2025-05-24 15:06:28,185 - INFO - joeynmt.training - Epoch   8, Step:    31100, Batch Loss:     1.501043, Batch Acc: 0.624128, Tokens per Sec:    14243, Lr: 0.000300
2025-05-24 15:06:33,177 - INFO - joeynmt.training - Epoch   8, Step:    31200, Batch Loss:     1.307250, Batch Acc: 0.619456, Tokens per Sec:    14892, Lr: 0.000300
2025-05-24 15:06:38,140 - INFO - joeynmt.training - Epoch   8, Step:    31300, Batch Loss:     1.269444, Batch Acc: 0.621638, Tokens per Sec:    14992, Lr: 0.000300
2025-05-24 15:06:43,203 - INFO - joeynmt.training - Epoch   8, Step:    31400, Batch Loss:     1.297882, Batch Acc: 0.625420, Tokens per Sec:    14713, Lr: 0.000300
2025-05-24 15:06:48,225 - INFO - joeynmt.training - Epoch   8, Step:    31500, Batch Loss:     1.391649, Batch Acc: 0.623469, Tokens per Sec:    14599, Lr: 0.000300
2025-05-24 15:06:48,225 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:06:48,225 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:06:57,868 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.37, acc:   0.53, generation: 9.6292[sec], evaluation: 0.0000[sec]
2025-05-24 15:06:57,868 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:06:57,945 - INFO - joeynmt.helpers - delete models/bpe_2k/28000.ckpt
2025-05-24 15:06:57,950 - INFO - joeynmt.training - Example #0
2025-05-24 15:06:57,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:06:57,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:06:57,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'enen', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', '%', '.', '</s>']
2025-05-24 15:06:57,952 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:06:57,952 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:06:57,952 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoenen ik deze twee dia &apos; s zien dat de arctische ijs , die de meeste drie miljoen jaar de grootste miljoen jaar de grootte van de lagere 48 % .
2025-05-24 15:06:57,952 - INFO - joeynmt.training - Example #1
2025-05-24 15:06:57,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:06:57,952 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:06:57,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aan@@', 'de', 'ser@@', 'i@@', 'be@@', 'ker@@', 'ing', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'ker', 'niet', 'de', 'di@@', 'k@@', 'ker', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:06:57,952 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:06:57,952 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:06:57,953 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaande seribekering van dit specifieke probleem omdat het de dikker niet de dikker van het ijs .
2025-05-24 15:06:57,953 - INFO - joeynmt.training - Example #2
2025-05-24 15:06:57,953 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:06:57,953 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:06:57,953 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', '-@@', 'k@@', 'oo@@', 'p', 'is', 'het', 'be@@', 'per@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:06:57,953 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:06:57,953 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:06:57,953 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs-koop is het beperklimaatsysteem .
2025-05-24 15:06:57,953 - INFO - joeynmt.training - Example #3
2025-05-24 15:06:57,953 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:06:57,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:06:57,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'iden', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:06:57,954 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:06:57,954 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:06:57,954 - INFO - joeynmt.training - 	Hypothesis: Het verbreiden in de winter en contracten in zomer .
2025-05-24 15:06:57,954 - INFO - joeynmt.training - Example #4
2025-05-24 15:06:57,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:06:57,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:06:57,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'een', 'sn@@', 'el@@', 'le', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'is', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:06:57,955 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:06:57,955 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:06:57,955 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat een snelle snelle voorwaarts is van wat er gebeurde de de laatste 25 jaar .
2025-05-24 15:07:03,018 - INFO - joeynmt.training - Epoch   8, Step:    31600, Batch Loss:     1.394291, Batch Acc: 0.617940, Tokens per Sec:    14097, Lr: 0.000300
2025-05-24 15:07:08,092 - INFO - joeynmt.training - Epoch   8, Step:    31700, Batch Loss:     1.256568, Batch Acc: 0.621472, Tokens per Sec:    14287, Lr: 0.000300
2025-05-24 15:07:13,128 - INFO - joeynmt.training - Epoch   8, Step:    31800, Batch Loss:     1.294097, Batch Acc: 0.626713, Tokens per Sec:    14419, Lr: 0.000300
2025-05-24 15:07:18,163 - INFO - joeynmt.training - Epoch   8, Step:    31900, Batch Loss:     1.263872, Batch Acc: 0.624458, Tokens per Sec:    14514, Lr: 0.000300
2025-05-24 15:07:19,223 - INFO - joeynmt.training - Epoch   8: total training loss 5172.25
2025-05-24 15:07:19,224 - INFO - joeynmt.training - EPOCH 9
2025-05-24 15:07:23,147 - INFO - joeynmt.training - Epoch   9, Step:    32000, Batch Loss:     1.236750, Batch Acc: 0.641374, Tokens per Sec:    15010, Lr: 0.000300
2025-05-24 15:07:23,147 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:07:23,147 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:07:32,442 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.36, acc:   0.53, generation: 9.2798[sec], evaluation: 0.0000[sec]
2025-05-24 15:07:32,443 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:07:32,519 - INFO - joeynmt.helpers - delete models/bpe_2k/30000.ckpt
2025-05-24 15:07:32,525 - INFO - joeynmt.training - Example #0
2025-05-24 15:07:32,525 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:07:32,525 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:07:32,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'en', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:07:32,526 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:07:32,526 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:07:32,526 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zien dat de arctische ijen , die voor de meeste drie miljoen jaar de grootste deel van de lagere 48 procent .
2025-05-24 15:07:32,526 - INFO - joeynmt.training - Example #1
2025-05-24 15:07:32,526 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:07:32,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:07:32,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'un@@', 't', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 's', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'en@@', 'st', 'van', 'het', 'ij@@', 's', 'niet', 'de', 'di@@', 'en@@', 'st', '.', '</s>']
2025-05-24 15:07:32,527 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:07:32,527 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:07:32,527 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteunt de seriëns van dit specifieke probleem omdat het de dienst van het ijs niet de dienst .
2025-05-24 15:07:32,527 - INFO - joeynmt.training - Example #2
2025-05-24 15:07:32,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:07:32,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:07:32,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'grot@@', 't', 'is', 'het', 'be@@', 'er@@', 'ende', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:07:32,527 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:07:32,527 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:07:32,527 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs grott is het beerende hart van het wereldwijde klimaatsysteem .
2025-05-24 15:07:32,527 - INFO - joeynmt.training - Example #3
2025-05-24 15:07:32,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:07:32,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:07:32,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'an@@', 'den', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:07:32,528 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:07:32,528 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:07:32,528 - INFO - joeynmt.training - 	Hypothesis: Het expanden in de winter en contracten in de zomer .
2025-05-24 15:07:32,528 - INFO - joeynmt.training - Example #4
2025-05-24 15:07:32,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:07:32,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:07:32,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'vor@@', '-@@', 'voor@@', 'waar@@', 'ts', 'van', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:07:32,528 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:07:32,528 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:07:32,529 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat je een rapid fast-vor-voorwaarts van de afgelopen 25 jaar .
2025-05-24 15:07:37,596 - INFO - joeynmt.training - Epoch   9, Step:    32100, Batch Loss:     1.326064, Batch Acc: 0.646177, Tokens per Sec:    14204, Lr: 0.000300
2025-05-24 15:07:42,746 - INFO - joeynmt.training - Epoch   9, Step:    32200, Batch Loss:     1.216296, Batch Acc: 0.644330, Tokens per Sec:    14193, Lr: 0.000300
2025-05-24 15:07:47,843 - INFO - joeynmt.training - Epoch   9, Step:    32300, Batch Loss:     1.317596, Batch Acc: 0.643239, Tokens per Sec:    14717, Lr: 0.000300
2025-05-24 15:07:52,849 - INFO - joeynmt.training - Epoch   9, Step:    32400, Batch Loss:     1.251927, Batch Acc: 0.636489, Tokens per Sec:    14289, Lr: 0.000300
2025-05-24 15:07:57,887 - INFO - joeynmt.training - Epoch   9, Step:    32500, Batch Loss:     1.308297, Batch Acc: 0.637714, Tokens per Sec:    14548, Lr: 0.000300
2025-05-24 15:07:57,888 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:07:57,888 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:08:06,905 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.37, acc:   0.53, generation: 9.0034[sec], evaluation: 0.0000[sec]
2025-05-24 15:08:06,983 - INFO - joeynmt.helpers - delete models/bpe_2k/29500.ckpt
2025-05-24 15:08:06,991 - INFO - joeynmt.training - Example #0
2025-05-24 15:08:06,991 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:08:06,991 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:08:06,992 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 'enen', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'mee@@', 'ste', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', 'ge@@', 'we@@', 'est', ',', 'heeft', 'door', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 15:08:06,992 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:08:06,992 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:08:06,992 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schoenen ik deze twee dia &apos; s zien dat de arctische ijs de meeste van de laatste drie miljoen jaar de grootste deel van de lagere 48 procent geweest , heeft door 40 procent .
2025-05-24 15:08:06,992 - INFO - joeynmt.training - Example #1
2025-05-24 15:08:06,992 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:08:06,992 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:08:06,992 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'ste@@', 'un@@', 'de', 'ser@@', 'i@@', 'eren', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'en@@', 'st', 'van', 'het', 'ij@@', 's', 'niet', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 15:08:06,993 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:08:06,993 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:08:06,993 - INFO - joeynmt.training - 	Hypothesis: Maar deze ondersteunde serieren van dit specifieke probleem omdat het de dienst van het ijs niet van de ijs .
2025-05-24 15:08:06,993 - INFO - joeynmt.training - Example #2
2025-05-24 15:08:06,993 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:08:06,993 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:08:06,993 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'ver@@', 'le@@', 'den', 'har@@', 't', 'van', 'de', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:08:06,993 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:08:06,994 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:08:06,994 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in zekere zin , het verleden hart van de wereldklimaatsysteem .
2025-05-24 15:08:06,994 - INFO - joeynmt.training - Example #3
2025-05-24 15:08:06,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:08:06,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:08:06,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'de', 'w@@', 'are', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:08:06,995 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:08:06,995 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:08:06,995 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de ware en contracten in de zomer .
2025-05-24 15:08:06,995 - INFO - joeynmt.training - Example #4
2025-05-24 15:08:06,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:08:06,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:08:06,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'van', 'wat', 'er', 'gebeur@@', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:08:06,995 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:08:06,995 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:08:06,995 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat jullie een rapid van wat er gebeurde laatste 25 jaar .
2025-05-24 15:08:12,054 - INFO - joeynmt.training - Epoch   9, Step:    32600, Batch Loss:     1.222068, Batch Acc: 0.640279, Tokens per Sec:    14330, Lr: 0.000300
2025-05-24 15:08:17,171 - INFO - joeynmt.training - Epoch   9, Step:    32700, Batch Loss:     1.263196, Batch Acc: 0.636595, Tokens per Sec:    14486, Lr: 0.000300
2025-05-24 15:08:22,238 - INFO - joeynmt.training - Epoch   9, Step:    32800, Batch Loss:     1.184220, Batch Acc: 0.641685, Tokens per Sec:    14409, Lr: 0.000300
2025-05-24 15:08:27,307 - INFO - joeynmt.training - Epoch   9, Step:    32900, Batch Loss:     1.367337, Batch Acc: 0.635705, Tokens per Sec:    14303, Lr: 0.000300
2025-05-24 15:08:32,316 - INFO - joeynmt.training - Epoch   9, Step:    33000, Batch Loss:     1.459513, Batch Acc: 0.637885, Tokens per Sec:    14479, Lr: 0.000300
2025-05-24 15:08:32,316 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:08:32,317 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:08:41,316 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.40, acc:   0.53, generation: 8.9852[sec], evaluation: 0.0000[sec]
2025-05-24 15:08:41,395 - INFO - joeynmt.helpers - delete models/bpe_2k/30500.ckpt
2025-05-24 15:08:41,401 - INFO - joeynmt.training - Example #0
2025-05-24 15:08:41,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:08:41,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:08:41,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'o@@', 't', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'ap@@', 't', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', '%', '.', '</s>']
2025-05-24 15:08:41,401 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:08:41,401 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:08:41,401 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schot ik deze twee dia &apos; s zien dat de arctische ijs kapt , die de meeste drie miljoen jaar de grootste deel van de lager 48 % .
2025-05-24 15:08:41,402 - INFO - joeynmt.training - Example #1
2025-05-24 15:08:41,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:08:41,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:08:41,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'sch@@', 'ei@@', 'ding', 'van', 'het', 'ser@@', 'i@@', 'ë@@', 'n@@', 'ten@@', 's@@', 'er', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:08:41,402 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:08:41,402 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:08:41,402 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderscheiding van het seriëntenser van dit probleem omdat het niet de thickness van het ijs .
2025-05-24 15:08:41,402 - INFO - joeynmt.training - Example #2
2025-05-24 15:08:41,402 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:08:41,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:08:41,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'een', 'z@@', 'in', ',', 'het', 'ver@@', 'b@@', 'and', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:08:41,403 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:08:41,403 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:08:41,403 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is in een zin , het verband hart van het wereldwijde klimaatsysteem .
2025-05-24 15:08:41,403 - INFO - joeynmt.training - Example #3
2025-05-24 15:08:41,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:08:41,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:08:41,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:08:41,403 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:08:41,403 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:08:41,403 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in zomer .
2025-05-24 15:08:41,403 - INFO - joeynmt.training - Example #4
2025-05-24 15:08:41,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:08:41,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:08:41,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'voor@@', 'waar@@', 'ts', 'van', 'wat', 'er', 'is', 'gebeur@@', 'd', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:08:41,404 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:08:41,404 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:08:41,404 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik jullie een rapid een rapid fast-voorwaarts van wat er is gebeurd in de laatste 25 jaar .
2025-05-24 15:08:46,493 - INFO - joeynmt.training - Epoch   9, Step:    33100, Batch Loss:     1.619080, Batch Acc: 0.632671, Tokens per Sec:    14550, Lr: 0.000300
2025-05-24 15:08:51,543 - INFO - joeynmt.training - Epoch   9, Step:    33200, Batch Loss:     1.244412, Batch Acc: 0.635072, Tokens per Sec:    14443, Lr: 0.000300
2025-05-24 15:08:56,576 - INFO - joeynmt.training - Epoch   9, Step:    33300, Batch Loss:     1.291456, Batch Acc: 0.629345, Tokens per Sec:    14575, Lr: 0.000300
2025-05-24 15:09:01,621 - INFO - joeynmt.training - Epoch   9, Step:    33400, Batch Loss:     1.185845, Batch Acc: 0.633545, Tokens per Sec:    14532, Lr: 0.000300
2025-05-24 15:09:06,670 - INFO - joeynmt.training - Epoch   9, Step:    33500, Batch Loss:     1.268413, Batch Acc: 0.631658, Tokens per Sec:    14640, Lr: 0.000300
2025-05-24 15:09:06,670 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:09:06,670 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:09:15,763 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.38, acc:   0.53, generation: 9.0800[sec], evaluation: 0.0000[sec]
2025-05-24 15:09:15,842 - INFO - joeynmt.helpers - delete models/bpe_2k/33000.ckpt
2025-05-24 15:09:15,848 - INFO - joeynmt.helpers - delete C:/Users/seraf/repositories/mt-exercise-4/models/bpe_2k/33000.ckpt
2025-05-24 15:09:15,848 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint C:\Users\seraf\repositories\mt-exercise-4\models\bpe_2k\33000.ckpt but file does not exist. ([WinError 2] The system cannot find the file specified: 'C:\\Users\\seraf\\repositories\\mt-exercise-4\\models\\bpe_2k\\33000.ckpt')
2025-05-24 15:09:15,848 - INFO - joeynmt.training - Example #0
2025-05-24 15:09:15,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:09:15,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:09:15,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 'f@@', 'er', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:09:15,849 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:09:15,849 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:09:15,849 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar ik deze twee dia &apos; s zien zien dat de arctische ijfer , die voor de meeste drie miljoen jaar de grootte van de lagere 48 procent .
2025-05-24 15:09:15,849 - INFO - joeynmt.training - Example #1
2025-05-24 15:09:15,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:09:15,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:09:15,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'un@@', 't', 'de', 'ser@@', 'i@@', 'be@@', 'w@@', 'u@@', 'st@@', 'zijn', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 15:09:15,850 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:09:15,850 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:09:15,850 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteunt de seribewustzijn van dit probleem omdat het niet de diickness van het ijs niet tonen .
2025-05-24 15:09:15,850 - INFO - joeynmt.training - Example #2
2025-05-24 15:09:15,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:09:15,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:09:15,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'ver@@', 'le@@', 'den', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:09:15,851 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:09:15,851 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:09:15,851 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in zekere zin , het verleden van het wereldwijde klimaatsysteem .
2025-05-24 15:09:15,851 - INFO - joeynmt.training - Example #3
2025-05-24 15:09:15,851 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:09:15,851 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:09:15,851 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:09:15,851 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:09:15,851 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:09:15,852 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in zomer .
2025-05-24 15:09:15,852 - INFO - joeynmt.training - Example #4
2025-05-24 15:09:15,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:09:15,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:09:15,852 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'je', 'een', 'r@@', 'ap@@', 'id', 'een', 'sn@@', 'el@@', 'le', 'fa@@', 'st@@', '-@@', 'voor@@', 'waar@@', 'ts', 'van', 'wat', 'er', 'gebeur@@', 'de', 'is', 'gebeur@@', 'd', '.', '</s>']
2025-05-24 15:09:15,852 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:09:15,852 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:09:15,852 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat je een rapid een snelle fast-voorwaarts van wat er gebeurde is gebeurd .
2025-05-24 15:09:20,899 - INFO - joeynmt.training - Epoch   9, Step:    33600, Batch Loss:     1.446833, Batch Acc: 0.631699, Tokens per Sec:    14313, Lr: 0.000300
2025-05-24 15:09:25,900 - INFO - joeynmt.training - Epoch   9, Step:    33700, Batch Loss:     1.380004, Batch Acc: 0.633861, Tokens per Sec:    14937, Lr: 0.000300
2025-05-24 15:09:30,865 - INFO - joeynmt.training - Epoch   9, Step:    33800, Batch Loss:     1.250598, Batch Acc: 0.634120, Tokens per Sec:    14745, Lr: 0.000300
2025-05-24 15:09:35,928 - INFO - joeynmt.training - Epoch   9, Step:    33900, Batch Loss:     1.278165, Batch Acc: 0.630357, Tokens per Sec:    14669, Lr: 0.000300
2025-05-24 15:09:40,977 - INFO - joeynmt.training - Epoch   9, Step:    34000, Batch Loss:     1.435888, Batch Acc: 0.638344, Tokens per Sec:    14490, Lr: 0.000300
2025-05-24 15:09:40,977 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:09:40,977 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:09:50,178 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.69, ppl:   5.41, acc:   0.53, generation: 9.1880[sec], evaluation: 0.0000[sec]
2025-05-24 15:09:50,179 - INFO - joeynmt.training - Example #0
2025-05-24 15:09:50,179 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:09:50,179 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:09:50,179 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'za@@', 'g', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'de', 'de@@', 'mon@@', 'str@@', 'eren', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:09:50,179 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:09:50,179 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:09:50,179 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar zag ik deze twee dia &apos; s zien dat de arctische ijs de demonstreren , die de meeste drie miljoen jaar de grootte van de lager 48 procent .
2025-05-24 15:09:50,181 - INFO - joeynmt.training - Example #1
2025-05-24 15:09:50,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:09:50,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:09:50,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'aa@@', 'd', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 's', 'van', 'het', 'ij@@', 's', 'niet', 'zien', '.', '</s>']
2025-05-24 15:09:50,181 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:09:50,181 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:09:50,182 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriaad van dit specifieke probleem omdat het niet de dicknes van het ijs niet zien .
2025-05-24 15:09:50,182 - INFO - joeynmt.training - Example #2
2025-05-24 15:09:50,182 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:09:50,182 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:09:50,182 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:09:50,182 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:09:50,182 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:09:50,182 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in zekere zin de klimaatsysteem .
2025-05-24 15:09:50,182 - INFO - joeynmt.training - Example #3
2025-05-24 15:09:50,182 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:09:50,182 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:09:50,183 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:09:50,183 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:09:50,183 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:09:50,183 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in de zomer .
2025-05-24 15:09:50,183 - INFO - joeynmt.training - Example #4
2025-05-24 15:09:50,183 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:09:50,183 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:09:50,183 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'van', 'wat', 'er', 'gebeur@@', 'de', 'is', 'gebeur@@', 'd', 'is', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:09:50,183 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:09:50,183 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:09:50,183 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid van wat er gebeurde is gebeurd is in de laatste 25 jaar .
2025-05-24 15:09:55,257 - INFO - joeynmt.training - Epoch   9, Step:    34100, Batch Loss:     1.304132, Batch Acc: 0.636075, Tokens per Sec:    14426, Lr: 0.000300
2025-05-24 15:10:00,275 - INFO - joeynmt.training - Epoch   9, Step:    34200, Batch Loss:     1.211998, Batch Acc: 0.632888, Tokens per Sec:    14731, Lr: 0.000300
2025-05-24 15:10:05,271 - INFO - joeynmt.training - Epoch   9, Step:    34300, Batch Loss:     1.394863, Batch Acc: 0.631920, Tokens per Sec:    14977, Lr: 0.000300
2025-05-24 15:10:10,259 - INFO - joeynmt.training - Epoch   9, Step:    34400, Batch Loss:     1.353228, Batch Acc: 0.629244, Tokens per Sec:    15332, Lr: 0.000300
2025-05-24 15:10:15,320 - INFO - joeynmt.training - Epoch   9, Step:    34500, Batch Loss:     1.387253, Batch Acc: 0.637313, Tokens per Sec:    14264, Lr: 0.000300
2025-05-24 15:10:15,320 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:10:15,320 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:10:24,545 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.37, acc:   0.53, generation: 9.2102[sec], evaluation: 0.0000[sec]
2025-05-24 15:10:24,621 - INFO - joeynmt.helpers - delete models/bpe_2k/33500.ckpt
2025-05-24 15:10:24,625 - INFO - joeynmt.helpers - delete C:/Users/seraf/repositories/mt-exercise-4/models/bpe_2k/33500.ckpt
2025-05-24 15:10:24,625 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint C:\Users\seraf\repositories\mt-exercise-4\models\bpe_2k\33500.ckpt but file does not exist. ([WinError 2] The system cannot find the file specified: 'C:\\Users\\seraf\\repositories\\mt-exercise-4\\models\\bpe_2k\\33500.ckpt')
2025-05-24 15:10:24,626 - INFO - joeynmt.training - Example #0
2025-05-24 15:10:24,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:10:24,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:10:24,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', 'is', 'ge@@', 'we@@', 'est', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 15:10:24,627 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:10:24,627 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:10:24,627 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zodat de arctische ijs , die voor de meeste drie miljoen jaar de grootste van de lager 40 procent is geweest van de lager 40 procent .
2025-05-24 15:10:24,627 - INFO - joeynmt.training - Example #1
2025-05-24 15:10:24,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:10:24,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:10:24,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'eren', 'van', 'dit', 'probleem', ',', 'omdat', 'het', 'de', 'di@@', 'ep@@', 's@@', 'sa@@', 'f@@', 'r@@', 'om@@', 'm@@', 'el@@', 'ing', 'niet', 'de', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 15:10:24,628 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:10:24,628 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:10:24,628 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieren van dit probleem , omdat het de diepssafrommeling niet de van de ijs .
2025-05-24 15:10:24,628 - INFO - joeynmt.training - Example #2
2025-05-24 15:10:24,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:10:24,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:10:24,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'in', 'een', 'z@@', 'in', 'het', 'bij@@', 'er@@', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:10:24,628 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:10:24,628 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:10:24,628 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , in een zin het bijervan de wereldwijde klimaatsysteem .
2025-05-24 15:10:24,629 - INFO - joeynmt.training - Example #3
2025-05-24 15:10:24,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:10:24,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:10:24,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'iden', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:10:24,629 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:10:24,629 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:10:24,629 - INFO - joeynmt.training - 	Hypothesis: Het verbreiden in de zomer en contracten in zomer .
2025-05-24 15:10:24,629 - INFO - joeynmt.training - Example #4
2025-05-24 15:10:24,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:10:24,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:10:24,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'ier', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:10:24,630 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:10:24,630 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:10:24,630 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat zien dat je een rapier van wat er gebeurde de laatste 25 jaar .
2025-05-24 15:10:29,711 - INFO - joeynmt.training - Epoch   9, Step:    34600, Batch Loss:     1.387392, Batch Acc: 0.629683, Tokens per Sec:    14131, Lr: 0.000300
2025-05-24 15:10:34,722 - INFO - joeynmt.training - Epoch   9, Step:    34700, Batch Loss:     1.328018, Batch Acc: 0.632664, Tokens per Sec:    14378, Lr: 0.000300
2025-05-24 15:10:39,963 - INFO - joeynmt.training - Epoch   9, Step:    34800, Batch Loss:     1.282503, Batch Acc: 0.633394, Tokens per Sec:    14459, Lr: 0.000300
2025-05-24 15:10:45,000 - INFO - joeynmt.training - Epoch   9, Step:    34900, Batch Loss:     1.194890, Batch Acc: 0.632180, Tokens per Sec:    14761, Lr: 0.000300
2025-05-24 15:10:49,927 - INFO - joeynmt.training - Epoch   9, Step:    35000, Batch Loss:     1.261383, Batch Acc: 0.632027, Tokens per Sec:    14674, Lr: 0.000300
2025-05-24 15:10:49,927 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:10:49,927 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:10:59,391 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.35, acc:   0.53, generation: 9.4496[sec], evaluation: 0.0000[sec]
2025-05-24 15:10:59,392 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:10:59,469 - INFO - joeynmt.helpers - delete models/bpe_2k/32500.ckpt
2025-05-24 15:10:59,474 - INFO - joeynmt.training - Example #0
2025-05-24 15:10:59,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:10:59,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:10:59,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:10:59,475 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:10:59,475 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:10:59,475 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zodat de arctische ijs , die voor de meeste drie miljoen jaar de afgelopen drie miljoen jaar de grootte van de lager 48 staten van de lager 48 procent .
2025-05-24 15:10:59,475 - INFO - joeynmt.training - Example #1
2025-05-24 15:10:59,476 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:10:59,476 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:10:59,476 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 'ten@@', 'is', 'van', 'dit', 'probleem', 'omdat', 'het', 'd@@', 'ic@@', 'k@@', 'ni@@', 's@@', 'heid', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:10:59,476 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:10:59,476 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:10:59,476 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriëntenis van dit probleem omdat het dicknisheid van het ijs .
2025-05-24 15:10:59,476 - INFO - joeynmt.training - Example #2
2025-05-24 15:10:59,476 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:10:59,476 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:10:59,476 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'een', 'z@@', 'in', ',', 'het', 'b@@', 'ru@@', 'i@@', 'ken', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:10:59,477 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:10:59,477 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:10:59,477 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is in een zin , het bruiken van het wereldwijde klimaatsysteem .
2025-05-24 15:10:59,477 - INFO - joeynmt.training - Example #3
2025-05-24 15:10:59,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:10:59,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:10:59,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'ru@@', 'i@@', 'ken', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:10:59,478 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:10:59,478 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:10:59,478 - INFO - joeynmt.training - 	Hypothesis: Het verbruiken in de zomer en contracten in zomer .
2025-05-24 15:10:59,478 - INFO - joeynmt.training - Example #4
2025-05-24 15:10:59,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:10:59,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:10:59,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'sn@@', 'el@@', 'le', 'voor@@', 'uit', 'wat', 'er', 'is', 'gebeur@@', 'd', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:10:59,478 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:10:59,478 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:10:59,478 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een snelle vooruit wat er is gebeurd in de laatste 25 jaar .
2025-05-24 15:11:04,462 - INFO - joeynmt.training - Epoch   9, Step:    35100, Batch Loss:     1.207662, Batch Acc: 0.626983, Tokens per Sec:    14311, Lr: 0.000300
2025-05-24 15:11:09,483 - INFO - joeynmt.training - Epoch   9, Step:    35200, Batch Loss:     1.404175, Batch Acc: 0.629774, Tokens per Sec:    14859, Lr: 0.000300
2025-05-24 15:11:14,463 - INFO - joeynmt.training - Epoch   9, Step:    35300, Batch Loss:     1.415026, Batch Acc: 0.628045, Tokens per Sec:    14933, Lr: 0.000300
2025-05-24 15:11:19,375 - INFO - joeynmt.training - Epoch   9, Step:    35400, Batch Loss:     1.277953, Batch Acc: 0.634394, Tokens per Sec:    14865, Lr: 0.000300
2025-05-24 15:11:24,340 - INFO - joeynmt.training - Epoch   9, Step:    35500, Batch Loss:     1.325551, Batch Acc: 0.633565, Tokens per Sec:    15089, Lr: 0.000300
2025-05-24 15:11:24,340 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:11:24,341 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:11:33,682 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.32, acc:   0.53, generation: 9.3277[sec], evaluation: 0.0000[sec]
2025-05-24 15:11:33,682 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:11:33,763 - INFO - joeynmt.helpers - delete models/bpe_2k/31000.ckpt
2025-05-24 15:11:33,769 - INFO - joeynmt.training - Example #0
2025-05-24 15:11:33,769 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:11:33,769 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:11:33,769 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'k@@', 'a@@', 'p', '&apos;', 's', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:11:33,770 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:11:33,770 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:11:33,770 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien dat de arctische kap &apos; s , die de meeste drie miljoen jaar de grootste deel van de lager 48 staten , heeft de grootte van de lagere 48 procent .
2025-05-24 15:11:33,770 - INFO - joeynmt.training - Example #1
2025-05-24 15:11:33,770 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:11:33,770 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:11:33,770 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 'e', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 15:11:33,770 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:11:33,771 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:11:33,771 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriëne van dit probleem omdat het de dickness van het ijs niet laat zien .
2025-05-24 15:11:33,771 - INFO - joeynmt.training - Example #2
2025-05-24 15:11:33,771 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:11:33,771 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:11:33,771 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'een', 'z@@', 'in', ',', 'het', 'sla@@', 'g', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:11:33,771 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:11:33,771 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:11:33,771 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in een zin , het slag van het wereldwijde klimaatsysteem .
2025-05-24 15:11:33,772 - INFO - joeynmt.training - Example #3
2025-05-24 15:11:33,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:11:33,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:11:33,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'dt', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:11:33,772 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:11:33,772 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:11:33,772 - INFO - joeynmt.training - 	Hypothesis: Het verbreidt in de zomer en contracten in de zomer .
2025-05-24 15:11:33,772 - INFO - joeynmt.training - Example #4
2025-05-24 15:11:33,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:11:33,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:11:33,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'sn@@', 'el@@', 'le', 'sn@@', 'el@@', 'le', 'voor@@', 'uit', 'wat', 'er', 'is', 'gebeur@@', 'd', 'is', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:11:33,773 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:11:33,773 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:11:33,773 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een snelle snelle vooruit wat er is gebeurd is in de laatste 25 jaar .
2025-05-24 15:11:38,807 - INFO - joeynmt.training - Epoch   9, Step:    35600, Batch Loss:     1.364325, Batch Acc: 0.629983, Tokens per Sec:    14356, Lr: 0.000300
2025-05-24 15:11:43,768 - INFO - joeynmt.training - Epoch   9, Step:    35700, Batch Loss:     1.230266, Batch Acc: 0.628748, Tokens per Sec:    15160, Lr: 0.000300
2025-05-24 15:11:48,645 - INFO - joeynmt.training - Epoch   9, Step:    35800, Batch Loss:     1.410133, Batch Acc: 0.627915, Tokens per Sec:    14844, Lr: 0.000300
2025-05-24 15:11:53,625 - INFO - joeynmt.training - Epoch   9, Step:    35900, Batch Loss:     1.222063, Batch Acc: 0.630014, Tokens per Sec:    14367, Lr: 0.000300
2025-05-24 15:11:53,879 - INFO - joeynmt.training - Epoch   9: total training loss 5047.05
2025-05-24 15:11:53,879 - INFO - joeynmt.training - EPOCH 10
2025-05-24 15:11:58,518 - INFO - joeynmt.training - Epoch  10, Step:    36000, Batch Loss:     1.256706, Batch Acc: 0.649297, Tokens per Sec:    15078, Lr: 0.000300
2025-05-24 15:11:58,519 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:11:58,519 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:12:07,589 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.32, acc:   0.53, generation: 9.0559[sec], evaluation: 0.0000[sec]
2025-05-24 15:12:07,589 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:12:07,671 - INFO - joeynmt.helpers - delete models/bpe_2k/34500.ckpt
2025-05-24 15:12:07,675 - INFO - joeynmt.training - Example #0
2025-05-24 15:12:07,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:12:07,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:12:07,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'en@@', 'ste', 'ij@@', 's', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'is', 'ge@@', 'we@@', 'est', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 15:12:07,676 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:12:07,676 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:12:07,676 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien dat de arctienste ijs van de laatste drie miljoen jaar de grootste deel van de lager 48 is geweest van de lager 40 procent .
2025-05-24 15:12:07,676 - INFO - joeynmt.training - Example #1
2025-05-24 15:12:07,676 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:12:07,676 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:12:07,676 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 'ten@@', 'sl@@', 'ot@@', 'te', 'van', 'dit', 'probleem', 'omdat', 'het', 'het', 'het', 'niet', 'de', 'd@@', 'oel', 'van', 'het', 'ij@@', 's', 'niet', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriëntenslotte van dit probleem omdat het het het niet de doel van het ijs niet van de ijs .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - Example #2
2025-05-24 15:12:07,677 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:12:07,677 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:12:07,677 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'k@@', 'a@@', 'p', 'is', ',', 'het', 'be@@', 'per@@', 'k@@', 'ende', 'har@@', 't', 'van', 'de', 'glob@@', 'ale', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs kap is , het beperkende hart van de globale klimaatsysteem .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - Example #3
2025-05-24 15:12:07,677 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:12:07,677 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:12:07,677 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'dt', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:12:07,677 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:12:07,678 - INFO - joeynmt.training - 	Hypothesis: Het verbreidt in de winter en contracten in de zomer .
2025-05-24 15:12:07,678 - INFO - joeynmt.training - Example #4
2025-05-24 15:12:07,678 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:12:07,678 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:12:07,678 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'jullie', 'een', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'van', 'wat', 'er', 'is', 'gebeur@@', 'd', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:12:07,678 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:12:07,678 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:12:07,678 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat jullie een snelle voorwaarts van wat er is gebeurd in de afgelopen 25 jaar .
2025-05-24 15:12:12,646 - INFO - joeynmt.training - Epoch  10, Step:    36100, Batch Loss:     1.194459, Batch Acc: 0.654616, Tokens per Sec:    14442, Lr: 0.000300
2025-05-24 15:12:17,577 - INFO - joeynmt.training - Epoch  10, Step:    36200, Batch Loss:     1.181907, Batch Acc: 0.647626, Tokens per Sec:    14840, Lr: 0.000300
2025-05-24 15:12:22,571 - INFO - joeynmt.training - Epoch  10, Step:    36300, Batch Loss:     1.235212, Batch Acc: 0.652596, Tokens per Sec:    14682, Lr: 0.000300
2025-05-24 15:12:27,507 - INFO - joeynmt.training - Epoch  10, Step:    36400, Batch Loss:     1.202967, Batch Acc: 0.646580, Tokens per Sec:    14580, Lr: 0.000300
2025-05-24 15:12:32,441 - INFO - joeynmt.training - Epoch  10, Step:    36500, Batch Loss:     1.205266, Batch Acc: 0.650792, Tokens per Sec:    15440, Lr: 0.000300
2025-05-24 15:12:32,442 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:12:32,442 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:12:41,653 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.36, acc:   0.53, generation: 9.1978[sec], evaluation: 0.0000[sec]
2025-05-24 15:12:41,734 - INFO - joeynmt.helpers - delete models/bpe_2k/31500.ckpt
2025-05-24 15:12:41,739 - INFO - joeynmt.training - Example #0
2025-05-24 15:12:41,739 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:12:41,739 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:12:41,739 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'sch@@', 'ree@@', 'f', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'ge@@', 'we@@', 'est', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'ge@@', 'st@@', 'eld', 'heeft', '.', '</s>']
2025-05-24 15:12:41,739 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:12:41,739 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:12:41,740 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar schreef ik deze twee dia &apos; s zien dat de arctische ijs , die de meeste drie miljoen jaar de grootste deel geweest van de lagere 48 gesteld heeft .
2025-05-24 15:12:41,740 - INFO - joeynmt.training - Example #1
2025-05-24 15:12:41,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:12:41,740 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:12:41,740 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 'ten@@', 'sl@@', 'ot@@', 'te', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'en@@', 'st', 'niet', 'de', 'di@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:12:41,740 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:12:41,740 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:12:41,740 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriëntenslotte van dit probleem omdat het de dienst niet de diickness van het ijs .
2025-05-24 15:12:41,740 - INFO - joeynmt.training - Example #2
2025-05-24 15:12:41,740 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:12:41,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:12:41,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'be@@', 'per@@', 'kt', 'har@@', 't', 'van', 'de', 'glob@@', 'ale', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:12:41,741 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:12:41,741 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:12:41,741 - INFO - joeynmt.training - 	Hypothesis: De arctische ijscap is , in zekere zin , het beperkt hart van de globale klimaatsysteem .
2025-05-24 15:12:41,741 - INFO - joeynmt.training - Example #3
2025-05-24 15:12:41,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:12:41,741 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:12:41,741 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'ru@@', 'i@@', 'kt', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:12:41,741 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:12:41,741 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:12:41,741 - INFO - joeynmt.training - 	Hypothesis: Het verbruikt in de winter en contracten in zomer .
2025-05-24 15:12:41,741 - INFO - joeynmt.training - Example #4
2025-05-24 15:12:41,741 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:12:41,743 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:12:41,743 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'ik', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:12:41,743 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:12:41,743 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:12:41,743 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat ik een rapid zal zijn van wat er gebeurd is in de afgelopen 25 jaar .
2025-05-24 15:12:46,803 - INFO - joeynmt.training - Epoch  10, Step:    36600, Batch Loss:     1.172775, Batch Acc: 0.643976, Tokens per Sec:    14499, Lr: 0.000300
2025-05-24 15:12:51,772 - INFO - joeynmt.training - Epoch  10, Step:    36700, Batch Loss:     1.243685, Batch Acc: 0.646038, Tokens per Sec:    14715, Lr: 0.000300
2025-05-24 15:12:56,694 - INFO - joeynmt.training - Epoch  10, Step:    36800, Batch Loss:     1.148264, Batch Acc: 0.649709, Tokens per Sec:    15005, Lr: 0.000300
2025-05-24 15:13:01,657 - INFO - joeynmt.training - Epoch  10, Step:    36900, Batch Loss:     1.284707, Batch Acc: 0.641661, Tokens per Sec:    14495, Lr: 0.000300
2025-05-24 15:13:06,648 - INFO - joeynmt.training - Epoch  10, Step:    37000, Batch Loss:     1.351617, Batch Acc: 0.639995, Tokens per Sec:    14635, Lr: 0.000300
2025-05-24 15:13:06,648 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:13:06,648 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:13:15,621 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.33, acc:   0.53, generation: 8.9587[sec], evaluation: 0.0000[sec]
2025-05-24 15:13:15,696 - INFO - joeynmt.helpers - delete models/bpe_2k/32000.ckpt
2025-05-24 15:13:15,702 - INFO - joeynmt.training - Example #0
2025-05-24 15:13:15,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:13:15,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:13:15,703 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '0', '%', '.', '</s>']
2025-05-24 15:13:15,703 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:13:15,703 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:13:15,703 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde twee dia &apos; s zien zien dat de arctische ijs , die voor de meeste drie miljoen jaar de grootste deel van de afgelopen drie miljoen jaar de grootte van de lagere 40 % .
2025-05-24 15:13:15,703 - INFO - joeynmt.training - Example #1
2025-05-24 15:13:15,703 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:13:15,703 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:13:15,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'stel@@', 'ling', 'onder@@', 'stel@@', 'ling', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'en@@', 'st', 'niet', 'de', 'di@@', 'en@@', 'st', 'van', 'het', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 15:13:15,704 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:13:15,704 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:13:15,704 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstelling onderstelling van dit specifieke probleem omdat het de dienst niet de dienst van het ijs niet .
2025-05-24 15:13:15,704 - INFO - joeynmt.training - Example #2
2025-05-24 15:13:15,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:13:15,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:13:15,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'is', 'in', 'een', 'be@@', 'st@@', 'rij@@', 'den', ',', 'het', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:13:15,705 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:13:15,705 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:13:15,705 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in een bestrijden , het hart van het wereldwijde klimaatsysteem .
2025-05-24 15:13:15,705 - INFO - joeynmt.training - Example #3
2025-05-24 15:13:15,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:13:15,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:13:15,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'b@@', 'and', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:13:15,705 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:13:15,705 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:13:15,705 - INFO - joeynmt.training - 	Hypothesis: Het verband in winter en contracten in de zomer .
2025-05-24 15:13:15,705 - INFO - joeynmt.training - Example #4
2025-05-24 15:13:15,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:13:15,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:13:15,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'jullie', 'een', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'zijn', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:13:15,706 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:13:15,706 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:13:15,706 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat jullie een snelle voorwaarts zijn van wat er gebeurde de de laatste 25 jaar .
2025-05-24 15:13:20,691 - INFO - joeynmt.training - Epoch  10, Step:    37100, Batch Loss:     1.256625, Batch Acc: 0.640859, Tokens per Sec:    14556, Lr: 0.000300
2025-05-24 15:13:25,698 - INFO - joeynmt.training - Epoch  10, Step:    37200, Batch Loss:     1.368184, Batch Acc: 0.642499, Tokens per Sec:    14627, Lr: 0.000300
2025-05-24 15:13:30,618 - INFO - joeynmt.training - Epoch  10, Step:    37300, Batch Loss:     1.167195, Batch Acc: 0.641336, Tokens per Sec:    14838, Lr: 0.000300
2025-05-24 15:13:35,568 - INFO - joeynmt.training - Epoch  10, Step:    37400, Batch Loss:     1.032595, Batch Acc: 0.645072, Tokens per Sec:    14786, Lr: 0.000300
2025-05-24 15:13:40,559 - INFO - joeynmt.training - Epoch  10, Step:    37500, Batch Loss:     1.359936, Batch Acc: 0.641659, Tokens per Sec:    14544, Lr: 0.000300
2025-05-24 15:13:40,560 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:13:40,560 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:13:49,823 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.34, acc:   0.53, generation: 9.2506[sec], evaluation: 0.0000[sec]
2025-05-24 15:13:49,900 - INFO - joeynmt.helpers - delete models/bpe_2k/36500.ckpt
2025-05-24 15:13:49,905 - INFO - joeynmt.training - Example #0
2025-05-24 15:13:49,905 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:13:49,905 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:13:49,905 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'en@@', 'ste', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', '%', 'van', 'de', 'la@@', 'ger', '4@@', '0', 'procent', '.', '</s>']
2025-05-24 15:13:49,905 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:13:49,905 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:13:49,906 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zien zo dat de arctienste ijs , die voor de meeste drie miljoen jaar de grootste deel van de lager 48 % van de lager 40 procent .
2025-05-24 15:13:49,906 - INFO - joeynmt.training - Example #1
2025-05-24 15:13:49,906 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:13:49,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:13:49,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'en@@', 'st', 'niet', 'ton@@', 'en', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 15:13:49,906 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:13:49,906 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:13:49,906 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriënt van dit specifieke probleem omdat het de dienst niet tonen van het ijs niet tonen .
2025-05-24 15:13:49,906 - INFO - joeynmt.training - Example #2
2025-05-24 15:13:49,906 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:13:49,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:13:49,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'en@@', 'tal@@', 'len', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'be@@', 'st@@', 'rij@@', 'den', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:13:49,907 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:13:49,907 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:13:49,907 - INFO - joeynmt.training - 	Hypothesis: De arctientallen is in zekere zin , het bestrijden hart van het wereldwijde klimaatsysteem .
2025-05-24 15:13:49,907 - INFO - joeynmt.training - Example #3
2025-05-24 15:13:49,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:13:49,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:13:49,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:13:49,907 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:13:49,907 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:13:49,907 - INFO - joeynmt.training - 	Hypothesis: Het expands in winter en contracten in zomer .
2025-05-24 15:13:49,907 - INFO - joeynmt.training - Example #4
2025-05-24 15:13:49,908 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:13:49,908 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:13:49,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'je', 'een', 'sn@@', 'el@@', 'le', 'ton@@', 'en', 'dat', 'je', 'een', 'sn@@', 'el@@', 'le', 'ver@@', 'kra@@', 'cht', 'is', '.', '</s>']
2025-05-24 15:13:49,908 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:13:49,908 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:13:49,908 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik je een snelle tonen dat je een snelle verkracht is .
2025-05-24 15:13:54,965 - INFO - joeynmt.training - Epoch  10, Step:    37600, Batch Loss:     1.256833, Batch Acc: 0.642671, Tokens per Sec:    14323, Lr: 0.000300
2025-05-24 15:14:00,032 - INFO - joeynmt.training - Epoch  10, Step:    37700, Batch Loss:     1.400022, Batch Acc: 0.642285, Tokens per Sec:    14667, Lr: 0.000300
2025-05-24 15:14:05,012 - INFO - joeynmt.training - Epoch  10, Step:    37800, Batch Loss:     1.225023, Batch Acc: 0.646680, Tokens per Sec:    14864, Lr: 0.000300
2025-05-24 15:14:10,050 - INFO - joeynmt.training - Epoch  10, Step:    37900, Batch Loss:     1.144758, Batch Acc: 0.639768, Tokens per Sec:    14660, Lr: 0.000300
2025-05-24 15:14:15,111 - INFO - joeynmt.training - Epoch  10, Step:    38000, Batch Loss:     1.410550, Batch Acc: 0.641232, Tokens per Sec:    14761, Lr: 0.000300
2025-05-24 15:14:15,112 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:14:15,112 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:14:23,728 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.31, acc:   0.53, generation: 8.6055[sec], evaluation: 0.0000[sec]
2025-05-24 15:14:23,729 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:14:23,813 - INFO - joeynmt.helpers - delete models/bpe_2k/35000.ckpt
2025-05-24 15:14:23,818 - INFO - joeynmt.training - Example #0
2025-05-24 15:14:23,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:14:23,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:14:23,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', ',', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', 'ca@@', 'p', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'la@@', 'g@@', 'ere', 'ge@@', 'st@@', 'aten', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:14:23,819 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:14:23,819 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:14:23,819 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien , dat de arctische ijs cap , die de meeste drie miljoen jaar de grootste lagere gestaten van de lagere 48 procent .
2025-05-24 15:14:23,819 - INFO - joeynmt.training - Example #1
2025-05-24 15:14:23,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:14:23,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:14:23,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'van', 'het', 'ser@@', 'i@@', 'be@@', 'id@@', 's@@', 'zor@@', 'g', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'ke', 'niet', 'ton@@', 'en', 'van', 'het', 'ij@@', 's', 'niet', 'ton@@', 'en', '.', '</s>']
2025-05-24 15:14:23,820 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:14:23,820 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:14:23,820 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten van het seribeidszorg van dit probleem omdat het de dikke niet tonen van het ijs niet tonen .
2025-05-24 15:14:23,820 - INFO - joeynmt.training - Example #2
2025-05-24 15:14:23,820 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:14:23,820 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:14:23,820 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:14:23,821 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:14:23,821 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:14:23,821 - INFO - joeynmt.training - 	Hypothesis: De arctische ijscap is , in zekere zin de klimaatsysteem .
2025-05-24 15:14:23,821 - INFO - joeynmt.training - Example #3
2025-05-24 15:14:23,821 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:14:23,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:14:23,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'd@@', 'w@@', 'and@@', 's', 'in', 'de', 'w@@', 'ind', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:14:23,821 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:14:23,821 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:14:23,822 - INFO - joeynmt.training - 	Hypothesis: Het verdwands in de wind en contracten in de zomer .
2025-05-24 15:14:23,822 - INFO - joeynmt.training - Example #4
2025-05-24 15:14:23,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:14:23,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:14:23,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'jullie', 'een', 'sn@@', 'el@@', 'le', 'to@@', 'on@@', 't', 'je', 'van', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.', '</s>']
2025-05-24 15:14:23,822 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:14:23,822 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:14:23,822 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik jullie een snelle toont je van de afgelopen 25 jaar is gebeurd .
2025-05-24 15:14:28,833 - INFO - joeynmt.training - Epoch  10, Step:    38100, Batch Loss:     1.241483, Batch Acc: 0.639234, Tokens per Sec:    14359, Lr: 0.000300
2025-05-24 15:14:33,863 - INFO - joeynmt.training - Epoch  10, Step:    38200, Batch Loss:     1.188047, Batch Acc: 0.636931, Tokens per Sec:    14327, Lr: 0.000300
2025-05-24 15:14:38,805 - INFO - joeynmt.training - Epoch  10, Step:    38300, Batch Loss:     1.135902, Batch Acc: 0.639819, Tokens per Sec:    14615, Lr: 0.000300
2025-05-24 15:14:43,885 - INFO - joeynmt.training - Epoch  10, Step:    38400, Batch Loss:     1.216335, Batch Acc: 0.639985, Tokens per Sec:    14533, Lr: 0.000300
2025-05-24 15:14:48,872 - INFO - joeynmt.training - Epoch  10, Step:    38500, Batch Loss:     1.348837, Batch Acc: 0.636601, Tokens per Sec:    14907, Lr: 0.000300
2025-05-24 15:14:48,872 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:14:48,872 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:14:57,850 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.30, acc:   0.53, generation: 8.9627[sec], evaluation: 0.0000[sec]
2025-05-24 15:14:57,850 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:14:57,926 - INFO - joeynmt.helpers - delete models/bpe_2k/37500.ckpt
2025-05-24 15:14:57,932 - INFO - joeynmt.training - Example #0
2025-05-24 15:14:57,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:14:57,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:14:57,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'to@@', 'on@@', 'de', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'st@@', 'aten', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', 'heeft', 'ge@@', 'we@@', 'est', '.', '</s>']
2025-05-24 15:14:57,932 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:14:57,932 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:14:57,932 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar toonde ik deze twee dia &apos; s zien dat de arctische ijs , die voor de meeste drie miljoen jaar de grootste staten van de lagere 48 procent heeft geweest .
2025-05-24 15:14:57,933 - INFO - joeynmt.training - Example #1
2025-05-24 15:14:57,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:14:57,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:14:57,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'stel@@', 't', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 't', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'ke', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 15:14:57,933 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:14:57,933 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:14:57,933 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstelt de seriënt van dit specifieke probleem omdat het de dikke niet laat zien .
2025-05-24 15:14:57,933 - INFO - joeynmt.training - Example #2
2025-05-24 15:14:57,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:14:57,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:14:57,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', '-@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:14:57,934 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:14:57,934 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:14:57,934 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs-cap is , in zekere zin de klimaatsysteem .
2025-05-24 15:14:57,934 - INFO - joeynmt.training - Example #3
2025-05-24 15:14:57,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:14:57,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:14:57,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'd@@', 'w@@', 'oe@@', 'st@@', 'ers', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:14:57,935 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:14:57,935 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:14:57,935 - INFO - joeynmt.training - 	Hypothesis: Het verdwoesters in de zomer .
2025-05-24 15:14:57,935 - INFO - joeynmt.training - Example #4
2025-05-24 15:14:57,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:14:57,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:14:57,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'zal', 'een', 'sn@@', 'el@@', 'le', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'van', 'wat', 'er', 'gebeur@@', 'd', 'is', 'gebeur@@', 'd', 'in', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:14:57,935 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:14:57,935 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:14:57,936 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie zal een snelle snelle voorwaarts van wat er gebeurd is gebeurd in de laatste 25 jaar .
2025-05-24 15:15:03,038 - INFO - joeynmt.training - Epoch  10, Step:    38600, Batch Loss:     1.267400, Batch Acc: 0.635803, Tokens per Sec:    14412, Lr: 0.000300
2025-05-24 15:15:08,479 - INFO - joeynmt.training - Epoch  10, Step:    38700, Batch Loss:     1.393041, Batch Acc: 0.637968, Tokens per Sec:    13499, Lr: 0.000300
2025-05-24 15:15:13,549 - INFO - joeynmt.training - Epoch  10, Step:    38800, Batch Loss:     1.051620, Batch Acc: 0.636260, Tokens per Sec:    14838, Lr: 0.000300
2025-05-24 15:15:18,605 - INFO - joeynmt.training - Epoch  10, Step:    38900, Batch Loss:     1.281752, Batch Acc: 0.642689, Tokens per Sec:    14610, Lr: 0.000300
2025-05-24 15:15:23,680 - INFO - joeynmt.training - Epoch  10, Step:    39000, Batch Loss:     1.224848, Batch Acc: 0.636367, Tokens per Sec:    14622, Lr: 0.000300
2025-05-24 15:15:23,681 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:15:23,681 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:15:33,057 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.31, acc:   0.53, generation: 9.3621[sec], evaluation: 0.0000[sec]
2025-05-24 15:15:33,133 - INFO - joeynmt.helpers - delete models/bpe_2k/37000.ckpt
2025-05-24 15:15:33,138 - INFO - joeynmt.training - Example #0
2025-05-24 15:15:33,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:15:33,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:15:33,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'zo', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'en@@', 'tal@@', 'len', ',', 'die', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:15:33,139 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:15:33,139 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:15:33,139 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zien zo dat de arctientallen , die de meeste drie miljoen jaar de grootste deel van de lagere 48 procent .
2025-05-24 15:15:33,139 - INFO - joeynmt.training - Example #1
2025-05-24 15:15:33,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:15:33,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:15:33,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ë@@', 'n@@', 'te', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'ke', 'ij@@', 's', 'niet', 'de', 'di@@', 'k@@', 'ne@@', 'ss', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 15:15:33,140 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:15:33,140 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:15:33,140 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriënte van dit probleem omdat het de dikke ijs niet de dikness van het ijs .
2025-05-24 15:15:33,140 - INFO - joeynmt.training - Example #2
2025-05-24 15:15:33,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:15:33,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:15:33,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'en@@', 'tal@@', 'len', 'is', ',', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', ',', 'het', 'be@@', 'st@@', 'rij@@', 'den', 'van', 'de', 'werel@@', 'd@@', 'wij@@', 'de', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:15:33,141 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:15:33,141 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:15:33,141 - INFO - joeynmt.training - 	Hypothesis: De arctientallen is , in zekere zin , het bestrijden van de wereldwijde klimaatsysteem .
2025-05-24 15:15:33,141 - INFO - joeynmt.training - Example #3
2025-05-24 15:15:33,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:15:33,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:15:33,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'dt', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:15:33,141 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:15:33,142 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:15:33,142 - INFO - joeynmt.training - 	Hypothesis: Het verbreidt in winter en contracten in de zomer .
2025-05-24 15:15:33,142 - INFO - joeynmt.training - Example #4
2025-05-24 15:15:33,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:15:33,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:15:33,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'van', 'de', 'dia', 'die', 'ik', 'laat', 'je', 'een', 'sne@@', 'l', 'voor@@', 'waar@@', 'ts', 'van', 'wat', 'er', 'gebeur@@', 'de', 'de', 'de', 'laatste', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:15:33,142 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:15:33,142 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:15:33,142 - INFO - joeynmt.training - 	Hypothesis: De volgende dia van de dia die ik laat je een snel voorwaarts van wat er gebeurde de de laatste 25 jaar .
2025-05-24 15:15:38,218 - INFO - joeynmt.training - Epoch  10, Step:    39100, Batch Loss:     1.266045, Batch Acc: 0.634868, Tokens per Sec:    14721, Lr: 0.000300
2025-05-24 15:15:43,237 - INFO - joeynmt.training - Epoch  10, Step:    39200, Batch Loss:     1.306801, Batch Acc: 0.634052, Tokens per Sec:    14538, Lr: 0.000300
2025-05-24 15:15:48,336 - INFO - joeynmt.training - Epoch  10, Step:    39300, Batch Loss:     1.398275, Batch Acc: 0.642441, Tokens per Sec:    14491, Lr: 0.000300
2025-05-24 15:15:53,274 - INFO - joeynmt.training - Epoch  10, Step:    39400, Batch Loss:     1.084531, Batch Acc: 0.641901, Tokens per Sec:    14580, Lr: 0.000300
2025-05-24 15:15:58,336 - INFO - joeynmt.training - Epoch  10, Step:    39500, Batch Loss:     1.260392, Batch Acc: 0.636670, Tokens per Sec:    14825, Lr: 0.000300
2025-05-24 15:15:58,336 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:15:58,336 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:16:07,534 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.67, ppl:   5.29, acc:   0.53, generation: 9.1843[sec], evaluation: 0.0000[sec]
2025-05-24 15:16:07,534 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 15:16:07,611 - INFO - joeynmt.helpers - delete models/bpe_2k/35500.ckpt
2025-05-24 15:16:07,617 - INFO - joeynmt.training - Example #0
2025-05-24 15:16:07,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'w@@', 'ed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'percent', '.']
2025-05-24 15:16:07,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'li@@', 'et', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'ton@@', 'en', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'af@@', 'gel@@', 'open', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'V@@', 'S', ',', 'met', '4@@', '0', '%', 'ge@@', 'k@@', 'r@@', 'om@@', 'pen', 'was', '.']
2025-05-24 15:16:07,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'laat', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zien', 'dat', 'de', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', 'stu@@', 'k', ',', 'die', 'voor', 'de', 'mee@@', 'ste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'ste', 'deel', 'van', 'de', 'la@@', 'ger', '4@@', '8', '%', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'procent', '.', '</s>']
2025-05-24 15:16:07,617 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 15:16:07,618 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 15:16:07,618 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar laat ik deze twee dia &apos; s zien zien dat de arctische ijsstuk , die voor de meeste drie miljoen jaar de grootste deel van de lager 48 % van de lager 48 procent .
2025-05-24 15:16:07,618 - INFO - joeynmt.training - Example #1
2025-05-24 15:16:07,618 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'parti@@', 'cu@@', 'lar', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ic@@', 'k@@', 'ne@@', 'ss', 'of', 'the', 'ice', '.']
2025-05-24 15:16:07,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 15:16:07,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'ste@@', 'un@@', 't', 'de', 'ser@@', 'ie@@', 'ten', 'van', 'dit', 'speci@@', 'f@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'ic@@', 'k@@', 'nie@@', 'ti@@', 'gen', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 15:16:07,618 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 15:16:07,618 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 15:16:07,618 - INFO - joeynmt.training - 	Hypothesis: Maar deze ondersteunt de serieten van dit specifieke probleem omdat het niet de dicknietigen van het ijs laat zien .
2025-05-24 15:16:07,618 - INFO - joeynmt.training - Example #2
2025-05-24 15:16:07,618 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'he@@', 'art', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system', '.']
2025-05-24 15:16:07,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.']
2025-05-24 15:16:07,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'ti@@', 'sche', 'ij@@', 's@@', 'ca@@', 'p', 'is', 'in', 'ze@@', 'k@@', 'ere', 'z@@', 'in', 'het', 'ver@@', 'ze@@', 'tten', 'har@@', 't', 'van', 'het', 'werel@@', 'd@@', 'k@@', 'li@@', 'ma@@', 'at@@', 'systeem', '.', '</s>']
2025-05-24 15:16:07,619 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 15:16:07,619 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 15:16:07,619 - INFO - joeynmt.training - 	Hypothesis: De arctische ijscap is in zekere zin het verzetten hart van het wereldklimaatsysteem .
2025-05-24 15:16:07,619 - INFO - joeynmt.training - Example #3
2025-05-24 15:16:07,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'and@@', 's', 'in', 'win@@', 'ter', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'su@@', 'mm@@', 'er', '.']
2025-05-24 15:16:07,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'z@@', 'et', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 15:16:07,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'iden', 'in', 'win@@', 'ter', 'en', 'con@@', 'tr@@', 'ac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 15:16:07,619 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 15:16:07,620 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 15:16:07,620 - INFO - joeynmt.training - 	Hypothesis: Het verbreiden in winter en contracten in de zomer .
2025-05-24 15:16:07,620 - INFO - joeynmt.training - Example #4
2025-05-24 15:16:07,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', '-@@', 'for@@', 'ward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '2@@', '5', 'years', '.']
2025-05-24 15:16:07,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'el@@', 'de', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', 'is', 'gebeur@@', 'd', '.']
2025-05-24 15:16:07,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'je', 'een', 'sn@@', 'el@@', 'le', 'voor@@', 'waar@@', 'ts', 'van', 'wat', 'er', 'is', 'gebeur@@', 'd', 'in', 'de', 'af@@', 'gel@@', 'open', '2@@', '5', 'jaar', '.', '</s>']
2025-05-24 15:16:07,620 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 15:16:07,620 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 15:16:07,620 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat je een snelle voorwaarts van wat er is gebeurd in de afgelopen 25 jaar .
2025-05-24 15:16:12,707 - INFO - joeynmt.training - Epoch  10, Step:    39600, Batch Loss:     1.104913, Batch Acc: 0.638763, Tokens per Sec:    13994, Lr: 0.000300
2025-05-24 15:16:17,757 - INFO - joeynmt.training - Epoch  10, Step:    39700, Batch Loss:     1.191408, Batch Acc: 0.637870, Tokens per Sec:    14498, Lr: 0.000300
2025-05-24 15:16:22,781 - INFO - joeynmt.training - Epoch  10, Step:    39800, Batch Loss:     1.302215, Batch Acc: 0.633517, Tokens per Sec:    14986, Lr: 0.000300
2025-05-24 15:16:27,275 - INFO - joeynmt.training - Epoch  10: total training loss 4941.98
2025-05-24 15:16:27,275 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-24 15:16:27,276 - INFO - joeynmt.training - Best validation result (greedy) at step    39500:   5.29 ppl.
2025-05-24 15:16:27,293 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 15:16:27,338 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 15:16:27,415 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\bpe_2k\39500.ckpt.
2025-05-24 15:16:27,430 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=1997),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=1997),
	loss_function=None)
2025-05-24 15:16:27,431 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-24 15:16:27,431 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:16:27,431 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:16:52,323 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 24.8794[sec], evaluation: 0.0000[sec]
2025-05-24 15:16:52,324 - INFO - joeynmt.prediction - Translations saved to: C:\Users\seraf\repositories\mt-exercise-4\models\bpe_2k\00039500.hyps.dev.
2025-05-24 15:16:52,325 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-24 15:16:52,325 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 15:16:52,325 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 15:17:27,792 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 35.4480[sec], evaluation: 0.0000[sec]
2025-05-24 15:17:27,794 - INFO - joeynmt.prediction - Translations saved to: C:\Users\seraf\repositories\mt-exercise-4\models\bpe_2k\00039500.hyps.test.

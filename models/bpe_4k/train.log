2025-05-24 18:38:45,066 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -                           cfg.name : bpe_4k
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -                     cfg.data.train : data/train
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-24 18:38:45,068 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : bpe2/vocab.joint
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 4000
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : bpe2/codes.bpe
2025-05-24 18:38:45,069 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : nl
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : bpe2/vocab.joint
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 4000
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : bpe2/codes.bpe
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_4k
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-24 18:38:45,070 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-24 18:38:45,071 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-24 18:38:45,072 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-24 18:38:45,086 - INFO - joeynmt.data - Building tokenizer...
2025-05-24 18:38:45,096 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-24 18:38:45,096 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-24 18:38:45,096 - INFO - joeynmt.data - Loading train set...
2025-05-24 18:38:45,268 - INFO - joeynmt.data - Building vocabulary...
2025-05-24 18:38:45,448 - INFO - joeynmt.data - Loading dev set...
2025-05-24 18:38:45,450 - INFO - joeynmt.data - Loading test set...
2025-05-24 18:38:45,453 - INFO - joeynmt.data - Data loaded.
2025-05-24 18:38:45,453 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=nl, has_trg=True, random_subset=-1)
2025-05-24 18:38:45,453 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=1003, src_lang=en, trg_lang=nl, has_trg=True, random_subset=-1)
2025-05-24 18:38:45,454 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1777, src_lang=en, trg_lang=nl, has_trg=True, random_subset=-1)
2025-05-24 18:38:45,454 - INFO - joeynmt.data - First training example:
	[SRC] Al G@@ ore : A@@ ver@@ ting the climate crisis
	[TRG] Al G@@ ore over het af@@ w@@ enden van de kli@@ maat@@ crisis
2025-05-24 18:38:45,454 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) - (5) -- (6) – (7) — (8)  (9) ​
2025-05-24 18:38:45,454 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) - (5) -- (6) – (7) — (8)  (9) ​
2025-05-24 18:38:45,454 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 3993
2025-05-24 18:38:45,454 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 3993
2025-05-24 18:38:45,461 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 18:38:45,521 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 18:38:45,526 - INFO - joeynmt.model - Total params: 3921408
2025-05-24 18:38:45,527 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-24 18:38:45,527 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3993),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3993),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-24 18:38:45,623 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-24 18:38:45,623 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-24 18:38:45,623 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-24 18:38:45,623 - INFO - joeynmt.training - EPOCH 1
2025-05-24 18:38:52,458 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.055238, Batch Acc: 0.061050, Tokens per Sec:    10309, Lr: 0.000300
2025-05-24 18:38:58,136 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.062030, Batch Acc: 0.102346, Tokens per Sec:    12582, Lr: 0.000300
2025-05-24 18:39:03,836 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.892083, Batch Acc: 0.116659, Tokens per Sec:    12968, Lr: 0.000300
2025-05-24 18:39:09,166 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.797808, Batch Acc: 0.125980, Tokens per Sec:    13660, Lr: 0.000300
2025-05-24 18:39:14,634 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.688316, Batch Acc: 0.138689, Tokens per Sec:    12915, Lr: 0.000300
2025-05-24 18:39:14,635 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:39:14,635 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:39:27,761 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.70, ppl:  40.27, acc:   0.13, generation: 13.0950[sec], evaluation: 0.0000[sec]
2025-05-24 18:39:27,761 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:39:27,840 - INFO - joeynmt.training - Example #0
2025-05-24 18:39:27,840 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:39:27,840 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:39:27,840 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']
2025-05-24 18:39:27,840 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:39:27,841 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:39:27,841 - INFO - joeynmt.training - 	Hypothesis: Ik ik ik ik ik ik ik ik ik ik ik de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de
2025-05-24 18:39:27,841 - INFO - joeynmt.training - Example #1
2025-05-24 18:39:27,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:39:27,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:39:27,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'het', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']
2025-05-24 18:39:27,841 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:39:27,841 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:39:27,841 - INFO - joeynmt.training - 	Hypothesis: Maar het de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de
2025-05-24 18:39:27,841 - INFO - joeynmt.training - Example #2
2025-05-24 18:39:27,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:39:27,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:39:27,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']
2025-05-24 18:39:27,842 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:39:27,842 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:39:27,842 - INFO - joeynmt.training - 	Hypothesis: De de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de
2025-05-24 18:39:27,842 - INFO - joeynmt.training - Example #3
2025-05-24 18:39:27,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:39:27,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:39:27,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'een', 'ver@@', 'ver@@', 'ver@@', 'ver@@', 'ver@@', 'ver@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'k@@', 'ten', '.', '</s>']
2025-05-24 18:39:27,843 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:39:27,843 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:39:27,843 - INFO - joeynmt.training - 	Hypothesis: Het is een ververververververkkkkkkkkkkkkkkkkkkkkkkkkkkkkkten .
2025-05-24 18:39:27,843 - INFO - joeynmt.training - Example #4
2025-05-24 18:39:27,843 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:39:27,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:39:27,843 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'ik', 'het', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']
2025-05-24 18:39:27,844 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:39:27,844 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:39:27,844 - INFO - joeynmt.training - 	Hypothesis: Ik ik ik ik ik ik ik ik ik ik ik ik ik het de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de
2025-05-24 18:39:33,032 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.571788, Batch Acc: 0.150257, Tokens per Sec:    13598, Lr: 0.000300
2025-05-24 18:39:38,504 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.447705, Batch Acc: 0.157781, Tokens per Sec:    12997, Lr: 0.000300
2025-05-24 18:39:43,723 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.426778, Batch Acc: 0.168639, Tokens per Sec:    13649, Lr: 0.000300
2025-05-24 18:39:49,070 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.409912, Batch Acc: 0.177322, Tokens per Sec:    13042, Lr: 0.000300
2025-05-24 18:39:54,472 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.306724, Batch Acc: 0.181673, Tokens per Sec:    13808, Lr: 0.000300
2025-05-24 18:39:54,472 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:39:54,472 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:40:07,549 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.48, ppl:  32.57, acc:   0.17, generation: 13.0627[sec], evaluation: 0.0000[sec]
2025-05-24 18:40:07,549 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:40:07,634 - INFO - joeynmt.training - Example #0
2025-05-24 18:40:07,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:40:07,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:40:07,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'deze', 'van', 'deze', 'van', 'deze', 'van', 'deze', 'van', 'deze', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:07,634 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:40:07,634 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:40:07,634 - INFO - joeynmt.training - 	Hypothesis: In deze van deze van deze van deze van deze van de wereld van de wereld van de wereld van de wereld van de wereld van de wereld van de wereld van de wereld .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - Example #1
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'ik', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'deze', 'wereld', ',', 'maar', 'het', 'niet', 'het', 'niet', 'van', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:07,635 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - 	Hypothesis: Maar ik de wereld van de wereld van de wereld van deze wereld , maar het niet het niet van de wereld .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - Example #2
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'eerste', 'is', 'een', 'van', 'de', 'wereld', 'is', 'een', 'van', 'de', 'wereld', ',', 'de', 'wereld', 'van', 'de', 'wereld', 'van', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:07,635 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - 	Hypothesis: De eerste is een van de wereld is een van de wereld , de wereld van de wereld van de wereld .
2025-05-24 18:40:07,635 - INFO - joeynmt.training - Example #3
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:40:07,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:40:07,637 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'was', 'in', 'de', 'wereld', 'in', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:07,637 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:40:07,637 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:40:07,637 - INFO - joeynmt.training - 	Hypothesis: Het was in de wereld in de wereld .
2025-05-24 18:40:07,637 - INFO - joeynmt.training - Example #4
2025-05-24 18:40:07,637 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:40:07,637 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:40:07,637 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'een', 'van', 'mijn', 'eerste', ',', 'dat', 'je', 'een', 'van', 'een', 'soort', 'van', 'de', 'wereld', 'van', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:07,637 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:40:07,637 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:40:07,637 - INFO - joeynmt.training - 	Hypothesis: Het is een van mijn eerste , dat je een van een soort van de wereld van de wereld .
2025-05-24 18:40:12,994 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.320709, Batch Acc: 0.189903, Tokens per Sec:    13128, Lr: 0.000300
2025-05-24 18:40:18,202 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.328808, Batch Acc: 0.197381, Tokens per Sec:    13333, Lr: 0.000300
2025-05-24 18:40:23,609 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.327925, Batch Acc: 0.203580, Tokens per Sec:    13272, Lr: 0.000300
2025-05-24 18:40:28,805 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.254578, Batch Acc: 0.211897, Tokens per Sec:    13718, Lr: 0.000300
2025-05-24 18:40:34,132 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.065680, Batch Acc: 0.221546, Tokens per Sec:    13445, Lr: 0.000300
2025-05-24 18:40:34,132 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:40:34,132 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:40:46,536 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.30, ppl:  27.18, acc:   0.19, generation: 12.3923[sec], evaluation: 0.0000[sec]
2025-05-24 18:40:46,536 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:40:46,615 - INFO - joeynmt.training - Example #0
2025-05-24 18:40:46,615 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:40:46,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:40:46,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Een', 'van', 'deze', 'eerste', 'jaar', 'geleden', ',', 'die', 'deze', 'drie', 'jaar', 'die', 'de', 'de', 'eerste', ',', 'die', 'de', 'eerste', ',', 'die', 'de', 'eerste', 'jaar', ',', 'die', 'de', 'eerste', 'jaar', ',', 'die', 'de', 'jaren', 'was', 'de', 'jaren', 'van', 'de', 'jaar', 'was', 'de', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '.000', '.', '</s>']
2025-05-24 18:40:46,616 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:40:46,616 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:40:46,616 - INFO - joeynmt.training - 	Hypothesis: Een van deze eerste jaar geleden , die deze drie jaar die de de eerste , die de eerste , die de eerste jaar , die de eerste jaar , die de jaren was de jaren van de jaar was de 111111111111.000 .
2025-05-24 18:40:46,616 - INFO - joeynmt.training - Example #1
2025-05-24 18:40:46,616 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:40:46,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:40:46,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'is', 'de', 'van', 'deze', 'van', 'deze', 'k@@', 'on@@', 'on@@', 'on@@', 'de', 'van', 'deze', 'k@@', 'on@@', 'de', 'wereld', ',', 'omdat', 'het', 'niet', 'de', 'k@@', 'on@@', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Hypothesis: Maar dit is de van deze van deze konononde van deze konde wereld , omdat het niet de konde wereld .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - Example #2
2025-05-24 18:40:46,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:40:46,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:40:46,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'een', 'van', 'de', 'eerste', 'is', 'een', 'van', 'de', 'wereld', 'is', ',', ',', 'de', 'eerste', 'van', 'de', 'wereld', 'van', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Hypothesis: Het is een van de eerste is een van de wereld is , , de eerste van de wereld van de wereld .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - Example #3
2025-05-24 18:40:46,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:40:46,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:40:46,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'de', 'wereld', 'in', 'de', 'wereld', 'en', 'en', 'in', 'de', 'wereld', '.', '</s>']
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - 	Hypothesis: Het is de wereld in de wereld en en in de wereld .
2025-05-24 18:40:46,617 - INFO - joeynmt.training - Example #4
2025-05-24 18:40:46,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:40:46,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:40:46,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'een', 'van', 'mijn', 'eerste', 'jaar', 'was', 'dat', 'een', 'van', 'wat', 'is', 'van', 'wat', 'is', ',', 'dat', 'is', 'de', 'eerste', 'jaar', '.', '</s>']
2025-05-24 18:40:46,619 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:40:46,619 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:40:46,619 - INFO - joeynmt.training - 	Hypothesis: Het is een van mijn eerste jaar was dat een van wat is van wat is , dat is de eerste jaar .
2025-05-24 18:40:51,829 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.139450, Batch Acc: 0.225174, Tokens per Sec:    13300, Lr: 0.000300
2025-05-24 18:40:57,129 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.993337, Batch Acc: 0.237429, Tokens per Sec:    13571, Lr: 0.000300
2025-05-24 18:41:02,420 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.897130, Batch Acc: 0.244495, Tokens per Sec:    12949, Lr: 0.000300
2025-05-24 18:41:07,838 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.035806, Batch Acc: 0.252503, Tokens per Sec:    13197, Lr: 0.000300
2025-05-24 18:41:13,101 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.844148, Batch Acc: 0.263450, Tokens per Sec:    13369, Lr: 0.000300
2025-05-24 18:41:13,101 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:41:13,103 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:41:26,276 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.06, ppl:  21.35, acc:   0.24, generation: 13.1537[sec], evaluation: 0.0000[sec]
2025-05-24 18:41:26,276 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:41:26,352 - INFO - joeynmt.training - Example #0
2025-05-24 18:41:26,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:41:26,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:41:26,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'twee', 'jaar', 'geleden', 'ik', 'deze', 'twee', 'twee', 'miljoen', 'die', 'de', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@']
2025-05-24 18:41:26,353 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:41:26,353 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:41:26,353 - INFO - joeynmt.training - 	Hypothesis: Ik heb twee jaar geleden ik deze twee twee miljoen die de gegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegege
2025-05-24 18:41:26,353 - INFO - joeynmt.training - Example #1
2025-05-24 18:41:26,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:41:26,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:41:26,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'is', 'de', 're@@', 'chten', 'van', 'deze', 'ver@@', 'scha@@', 'k@@', 'k@@', 'ende', 'omdat', 'het', 'niet', 'niet', 'niet', 'niet', 'alleen', 'de', 'wereld', 'niet', 'niet', 'de', 'wereld', '.', '</s>']
2025-05-24 18:41:26,353 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:41:26,353 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:41:26,353 - INFO - joeynmt.training - 	Hypothesis: Maar dit is de rechten van deze verschakkende omdat het niet niet niet niet alleen de wereld niet niet de wereld .
2025-05-24 18:41:26,353 - INFO - joeynmt.training - Example #2
2025-05-24 18:41:26,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:41:26,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:41:26,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'eerste', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@']
2025-05-24 18:41:26,354 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:41:26,354 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:41:26,354 - INFO - joeynmt.training - 	Hypothesis: De eerste gegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegege
2025-05-24 18:41:26,354 - INFO - joeynmt.training - Example #3
2025-05-24 18:41:26,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:41:26,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:41:26,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'in', 'de', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@', 'ge@@']
2025-05-24 18:41:26,354 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:41:26,355 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:41:26,355 - INFO - joeynmt.training - 	Hypothesis: Het is in de gegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegegege
2025-05-24 18:41:26,355 - INFO - joeynmt.training - Example #4
2025-05-24 18:41:26,355 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:41:26,355 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:41:26,355 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'eerste', 'eerste', 'eerste', 'was', 'ik', 'je', 'een', 're@@', 'k', 'van', 'wat', 'er', 'is', 'is', 'dat', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'er', 'nog', 'nog', '.', '</s>']
2025-05-24 18:41:26,355 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:41:26,355 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:41:26,355 - INFO - joeynmt.training - 	Hypothesis: De eerste eerste eerste was ik je een rek van wat er is is dat er er er er er er er er er er nog nog .
2025-05-24 18:41:31,757 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.824579, Batch Acc: 0.275475, Tokens per Sec:    12999, Lr: 0.000300
2025-05-24 18:41:37,005 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.763106, Batch Acc: 0.287188, Tokens per Sec:    13486, Lr: 0.000300
2025-05-24 18:41:42,396 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.714884, Batch Acc: 0.307942, Tokens per Sec:    13553, Lr: 0.000300
2025-05-24 18:41:47,633 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.813879, Batch Acc: 0.314345, Tokens per Sec:    13723, Lr: 0.000300
2025-05-24 18:41:53,089 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.568161, Batch Acc: 0.316737, Tokens per Sec:    13152, Lr: 0.000300
2025-05-24 18:41:53,089 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:41:53,089 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:42:06,261 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.84, ppl:  17.12, acc:   0.28, generation: 13.1592[sec], evaluation: 0.0000[sec]
2025-05-24 18:42:06,261 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:42:06,340 - INFO - joeynmt.training - Example #0
2025-05-24 18:42:06,340 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:42:06,340 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:42:06,340 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'ben', 'deze', 'twee', 'jaar', 'die', 'ik', 'deze', 'twee', 'twee', 'jaar', 'die', 'de', 'k@@', 'ol@@', 'ol@@', 'ol@@', 'ol@@', 'ol@@', 'ol@@', 'ol@@', 'k', ',', 'die', 'voor', 'de', 'meest', 'van', 'de', 'drie', 'jaar', 'heeft', 'de', 'drie', 'jaar', 'de', 'meest', 'van', 'de', 'afgelopen', 'miljoen', 'jaar', ',', 'is', 'het', 'het', 'een', 'procent', 'van', '100', 'procent', '.', '</s>']
2025-05-24 18:42:06,340 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:42:06,341 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:42:06,341 - INFO - joeynmt.training - 	Hypothesis: Ik ben deze twee jaar die ik deze twee twee jaar die de kolololololololk , die voor de meest van de drie jaar heeft de drie jaar de meest van de afgelopen miljoen jaar , is het het een procent van 100 procent .
2025-05-24 18:42:06,341 - INFO - joeynmt.training - Example #1
2025-05-24 18:42:06,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:42:06,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:42:06,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'is', 'de', 're@@', 'ger@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'dige', 'van', 'de', 'de', 'k@@', 'waa@@', 'd', 'van', 'de', 'de', 'de', 'k@@', 'ent', '.', '</s>']
2025-05-24 18:42:06,341 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:42:06,341 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:42:06,341 - INFO - joeynmt.training - 	Hypothesis: Maar dit is de regeriiiiiiiiiiiiiiidige van de de kwaad van de de de kent .
2025-05-24 18:42:06,341 - INFO - joeynmt.training - Example #2
2025-05-24 18:42:06,341 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:42:06,341 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:42:06,341 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'k@@', 'ol@@', 'ol@@', 'ol@@', 'k', 'is', ',', 'is', 'een', 'idee', ',', 'het', 'har@@', 't', 'van', 'de', 'de', 'de', 'menselijke', 'systeem', '.', '</s>']
2025-05-24 18:42:06,342 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:42:06,342 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:42:06,342 - INFO - joeynmt.training - 	Hypothesis: De kolololk is , is een idee , het hart van de de de menselijke systeem .
2025-05-24 18:42:06,342 - INFO - joeynmt.training - Example #3
2025-05-24 18:42:06,342 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:42:06,342 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:42:06,342 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'is', 'de', 'in@@', 't@@', 't@@', 'oon', 'en', 'ver@@', 'gelij@@', 'ks', '.', '</s>']
2025-05-24 18:42:06,342 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:42:06,342 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:42:06,342 - INFO - joeynmt.training - 	Hypothesis: Het is de inttoon en vergelijks .
2025-05-24 18:42:06,342 - INFO - joeynmt.training - Example #4
2025-05-24 18:42:06,343 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:42:06,343 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:42:06,343 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'volgende', 'volgende', 'volgende', ':', 'Ik', 'wil', 'je', 'een', 'r@@', 'oon', 'van', 'wat', 'er', 'is', 'het', '25', 'jaar', '.', '</s>']
2025-05-24 18:42:06,343 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:42:06,343 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:42:06,343 - INFO - joeynmt.training - 	Hypothesis: De volgende volgende volgende volgende : Ik wil je een roon van wat er is het 25 jaar .
2025-05-24 18:42:11,758 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.610670, Batch Acc: 0.333730, Tokens per Sec:    12834, Lr: 0.000300
2025-05-24 18:42:17,066 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.709646, Batch Acc: 0.338016, Tokens per Sec:    13492, Lr: 0.000300
2025-05-24 18:42:22,465 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.480906, Batch Acc: 0.351510, Tokens per Sec:    13467, Lr: 0.000300
2025-05-24 18:42:27,857 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.427962, Batch Acc: 0.359650, Tokens per Sec:    13105, Lr: 0.000300
2025-05-24 18:42:33,367 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.333237, Batch Acc: 0.361270, Tokens per Sec:    13002, Lr: 0.000300
2025-05-24 18:42:33,367 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:42:33,367 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:42:43,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.66, ppl:  14.24, acc:   0.32, generation: 10.0165[sec], evaluation: 0.0000[sec]
2025-05-24 18:42:43,394 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:42:43,485 - INFO - joeynmt.helpers - delete models/bpe_4k/500.ckpt
2025-05-24 18:42:43,491 - INFO - joeynmt.training - Example #0
2025-05-24 18:42:43,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:42:43,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:42:43,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'deze', 'twee', 'ge@@', 'spre@@', 'k', ',', 'dus', 'ik', 'deze', 'twee', 'ge@@', 'spre@@', 'k', ',', 'die', 'de', 'k@@', 'ost', ',', 'die', 'de', 'ver@@', 'gelijk@@', 'bare', 'jaren', 'drie', 'miljoen', 'jaar', 'is', 'de', 'de', 'ge@@', 'spre@@', 'kken', 'van', 'de', 'laatste', 'jaren', ',', 'in', 'een', 'jaar', '.', '</s>']
2025-05-24 18:42:43,492 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:42:43,492 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:42:43,492 - INFO - joeynmt.training - 	Hypothesis: Ik heb deze twee gesprek , dus ik deze twee gesprek , die de kost , die de vergelijkbare jaren drie miljoen jaar is de de gesprekken van de laatste jaren , in een jaar .
2025-05-24 18:42:43,492 - INFO - joeynmt.training - Example #1
2025-05-24 18:42:43,492 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:42:43,492 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:42:43,492 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'van', 'de', 'ver@@', 'ver@@', 'gelijk@@', 'baar', 'zijn', ',', 'van', 'deze', 'probleem', 'omdat', 'het', 'het', 'niet', 'de', 'k@@', 'ost', 'niet', 'de', 'k@@', 'ost', 'van', 'de', 'k@@', 'ru@@', 'st@@', 'ige', 'probleem', '.', '</s>']
2025-05-24 18:42:43,493 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:42:43,493 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:42:43,493 - INFO - joeynmt.training - 	Hypothesis: Maar dit van de ververgelijkbaar zijn , van deze probleem omdat het het niet de kost niet de kost van de krustige probleem .
2025-05-24 18:42:43,493 - INFO - joeynmt.training - Example #2
2025-05-24 18:42:43,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:42:43,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:42:43,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'k@@', 'wal@@', 'iteit', 'is', ',', 'in', 'een', 'probleem', ',', 'in', 'een', 'probleem', ',', 'het', 'ver@@', 'gelijk@@', 'ingen', 'van', 'de', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:42:43,493 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:42:43,493 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:42:43,493 - INFO - joeynmt.training - 	Hypothesis: De kwaliteit is , in een probleem , in een probleem , het vergelijkingen van de de klimaatsysteem .
2025-05-24 18:42:43,493 - INFO - joeynmt.training - Example #3
2025-05-24 18:42:43,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:42:43,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:42:43,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'gelijk@@', 'ingen', 'in', 'de', 'ver@@', 'we@@', 'zen', '.', '</s>']
2025-05-24 18:42:43,494 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:42:43,494 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:42:43,494 - INFO - joeynmt.training - 	Hypothesis: Het vergelijkingen in de verwezen .
2025-05-24 18:42:43,494 - INFO - joeynmt.training - Example #4
2025-05-24 18:42:43,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:42:43,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:42:43,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'volgende', 'was', 'ik', 'jullie', 'jullie', 'een', 'r@@', 'ation@@', 'eel', 'ge@@', 'zicht', 'van', 'wat', 'er', 'gebeurde', '.', '</s>']
2025-05-24 18:42:43,494 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:42:43,494 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:42:43,495 - INFO - joeynmt.training - 	Hypothesis: De volgende volgende was ik jullie jullie een rationeel gezicht van wat er gebeurde .
2025-05-24 18:42:48,916 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.521060, Batch Acc: 0.377733, Tokens per Sec:    12969, Lr: 0.000300
2025-05-24 18:42:54,299 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.333956, Batch Acc: 0.385163, Tokens per Sec:    13461, Lr: 0.000300
2025-05-24 18:42:59,571 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.254986, Batch Acc: 0.392489, Tokens per Sec:    13895, Lr: 0.000300
2025-05-24 18:43:04,934 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.354325, Batch Acc: 0.400899, Tokens per Sec:    13572, Lr: 0.000300
2025-05-24 18:43:10,401 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.349186, Batch Acc: 0.402885, Tokens per Sec:    13162, Lr: 0.000300
2025-05-24 18:43:10,401 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:43:10,402 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:43:19,622 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.51, ppl:  12.33, acc:   0.35, generation: 9.2085[sec], evaluation: 0.0000[sec]
2025-05-24 18:43:19,622 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:43:19,700 - INFO - joeynmt.helpers - delete models/bpe_4k/1000.ckpt
2025-05-24 18:43:19,706 - INFO - joeynmt.training - Example #0
2025-05-24 18:43:19,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:43:19,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:43:19,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'deze', 'twee', 'stu@@', 'kken', ',', 'dus', 'die', 'de', 'groot@@', 'te', 'te', 'te', 'die', 'de', 'ar@@', 'c@@', 'ijf@@', 'ers', ',', 'die', 'de', 'meeste', 'drie', 'jaar', ',', 'die', 'voor', 'de', 'afgelopen', 'drie', 'jaar', 'was', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'jaar', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:43:19,707 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:43:19,707 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:43:19,707 - INFO - joeynmt.training - 	Hypothesis: Ik heb deze twee stukken , dus die de grootte te te die de arcijfers , die de meeste drie jaar , die voor de afgelopen drie jaar was de grootte van de grootjaar , heeft door 40 procent .
2025-05-24 18:43:19,707 - INFO - joeynmt.training - Example #1
2025-05-24 18:43:19,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:43:19,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:43:19,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'ver@@', 'ko@@', 'st@@', 'te', 'de', 'ver@@', 'ko@@', 'cht', 'van', 'deze', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'k@@', 'wal@@', 'iteit', 'van', 'de', 'ma@@', 'g', '.', '</s>']
2025-05-24 18:43:19,707 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:43:19,707 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:43:19,708 - INFO - joeynmt.training - 	Hypothesis: Maar deze verkostte de verkocht van deze specifieke probleem omdat het niet de kwaliteit van de mag .
2025-05-24 18:43:19,708 - INFO - joeynmt.training - Example #2
2025-05-24 18:43:19,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:43:19,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:43:19,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'k@@', 'tische', 'k@@', 'tische', 'k@@', 'tische', 'de@@', 'cen@@', 'trum', ',', 'de', 'ver@@', 'ko@@', 'cht', 'van', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:43:19,708 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:43:19,708 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:43:19,708 - INFO - joeynmt.training - 	Hypothesis: De ktische ktische ktische decentrum , de verkocht van de klimaatsysteem .
2025-05-24 18:43:19,708 - INFO - joeynmt.training - Example #3
2025-05-24 18:43:19,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:43:19,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:43:19,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'chten', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'cep@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:43:19,709 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:43:19,709 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:43:19,709 - INFO - joeynmt.training - 	Hypothesis: Het verwachten in de winter en concepten in de zomer .
2025-05-24 18:43:19,709 - INFO - joeynmt.training - Example #4
2025-05-24 18:43:19,709 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:43:19,709 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:43:19,709 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'volgende', 'moment', 'dat', 'ik', 'jullie', 'jullie', 'een', 'r@@', 'ap@@', 'ap@@', 't', 'van', 'wat', 'er', 'er', 'er', 'er', 'gebeurde', '.', '</s>']
2025-05-24 18:43:19,709 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:43:19,709 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:43:19,709 - INFO - joeynmt.training - 	Hypothesis: De volgende volgende moment dat ik jullie jullie een rapapt van wat er er er er gebeurde .
2025-05-24 18:43:24,773 - INFO - joeynmt.training - Epoch   1: total training loss 10947.97
2025-05-24 18:43:24,773 - INFO - joeynmt.training - EPOCH 2
2025-05-24 18:43:25,099 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     2.144699, Batch Acc: 0.415817, Tokens per Sec:    13642, Lr: 0.000300
2025-05-24 18:43:30,523 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     2.163343, Batch Acc: 0.431949, Tokens per Sec:    13185, Lr: 0.000300
2025-05-24 18:43:35,805 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     2.226902, Batch Acc: 0.430562, Tokens per Sec:    13507, Lr: 0.000300
2025-05-24 18:43:41,083 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     2.152006, Batch Acc: 0.437118, Tokens per Sec:    13778, Lr: 0.000300
2025-05-24 18:43:46,424 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     2.255932, Batch Acc: 0.450611, Tokens per Sec:    13556, Lr: 0.000300
2025-05-24 18:43:46,424 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:43:46,424 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:43:57,106 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.41, ppl:  11.17, acc:   0.38, generation: 10.6714[sec], evaluation: 0.0000[sec]
2025-05-24 18:43:57,106 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:43:57,190 - INFO - joeynmt.helpers - delete models/bpe_4k/1500.ckpt
2025-05-24 18:43:57,196 - INFO - joeynmt.training - Example #0
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'deze', 'twee', 'gra@@', 'den', 'zo', 'dat', 'deze', 'twee', 'gra@@', 'den', 'die', 'de', 'ar@@', 'c@@', 'tische', 'kun@@', 'str@@', 'aten', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', ',', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', '40', 'procent', '.', '</s>']
2025-05-24 18:43:57,197 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:43:57,197 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:43:57,197 - INFO - joeynmt.training - 	Hypothesis: Ik heb deze twee graden zo dat deze twee graden die de arctische kunstraten , die voor de laatste drie miljoen jaar , die voor de laatste drie miljoen jaar , is de grootte van de grootte van 40 procent .
2025-05-24 18:43:57,197 - INFO - joeynmt.training - Example #1
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'van', 'de', 'ser@@', 'ie@@', 'ie@@', 'dere', 'probleem', 'omdat', 'het', 'niet', 'het', 'het', 'idee', 'is', 'omdat', 'het', 'niet', 'het', 'laten', 'laten', 'zien', 'van', 'de', 'prijs', 'van', 'de', 'ma@@', 'cht', '.', '</s>']
2025-05-24 18:43:57,197 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:43:57,197 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:43:57,197 - INFO - joeynmt.training - 	Hypothesis: Maar dit van de serieiedere probleem omdat het niet het het idee is omdat het niet het laten laten zien van de prijs van de macht .
2025-05-24 18:43:57,197 - INFO - joeynmt.training - Example #2
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:43:57,197 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'oppervla@@', 'k', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'ver@@', 'gelijk@@', 'er@@', 'end', 'van', 'het', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:43:57,198 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:43:57,198 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:43:57,198 - INFO - joeynmt.training - 	Hypothesis: De arctische oppervlak is , in een zin , het vergelijkerend van het klimaatsysteem .
2025-05-24 18:43:57,198 - INFO - joeynmt.training - Example #3
2025-05-24 18:43:57,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:43:57,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:43:57,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'chten', 'in', 'win@@', 'ter', 'en', 'con@@', 'con@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'ten', '.', '</s>']
2025-05-24 18:43:57,198 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:43:57,198 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:43:57,198 - INFO - joeynmt.training - 	Hypothesis: Het verwachten in winter en conconcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurcurten .
2025-05-24 18:43:57,199 - INFO - joeynmt.training - Example #4
2025-05-24 18:43:57,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:43:57,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:43:57,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'ap@@', 'ij@@', 'ks', 'zijn', 'ge@@', 'k', 'van', 'wat', 'er', 'gebeurde', 'over', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:43:57,199 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:43:57,199 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:43:57,199 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapapijks zijn gek van wat er gebeurde over de laatste 25 jaar .
2025-05-24 18:44:02,516 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     2.183586, Batch Acc: 0.445543, Tokens per Sec:    13233, Lr: 0.000300
2025-05-24 18:44:07,844 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.008310, Batch Acc: 0.450714, Tokens per Sec:    13536, Lr: 0.000300
2025-05-24 18:44:13,072 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     2.084107, Batch Acc: 0.451303, Tokens per Sec:    13594, Lr: 0.000300
2025-05-24 18:44:18,569 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     2.143635, Batch Acc: 0.455180, Tokens per Sec:    13285, Lr: 0.000300
2025-05-24 18:44:23,766 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     2.041874, Batch Acc: 0.457049, Tokens per Sec:    13997, Lr: 0.000300
2025-05-24 18:44:23,766 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:44:23,767 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:44:34,047 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.33, ppl:  10.29, acc:   0.40, generation: 10.2689[sec], evaluation: 0.0000[sec]
2025-05-24 18:44:34,047 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:44:34,132 - INFO - joeynmt.helpers - delete models/bpe_4k/2000.ckpt
2025-05-24 18:44:34,138 - INFO - joeynmt.training - Example #0
2025-05-24 18:44:34,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:44:34,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:44:34,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'n', 'n', 'jon@@', 'g@@', 'ere', 'st@@', 'appen', 'die', 'de', 'ar@@', 'c@@', 'tische', 'jaar', ',', 'die', 'voor', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'hele', '4@@', '8', 'st@@', 'oel', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:44:34,139 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:44:34,139 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:44:34,139 - INFO - joeynmt.training - 	Hypothesis: Ik heb deze twee dia &apos; s zo &apos; n n n jongere stappen die de arctische jaar , die voor de afgelopen drie miljoen jaar is de grootte van de hele 48 stoel , heeft door 40 procent .
2025-05-24 18:44:34,139 - INFO - joeynmt.training - Example #1
2025-05-24 18:44:34,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:44:34,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:44:34,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mmen', 'de', 'ser@@', 'ie@@', 'ie@@', 'dere', 'probleem', 'omdat', 'het', 'niet', 'het', 'ver@@', 'beel@@', 'ding', 'is', ',', 'omdat', 'het', 'niet', 'de', 'ge@@', 'wi@@', 'cht', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:44:34,139 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:44:34,139 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:44:34,139 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemmen de serieiedere probleem omdat het niet het verbeelding is , omdat het niet de gewicht van de ijs .
2025-05-24 18:44:34,139 - INFO - joeynmt.training - Example #2
2025-05-24 18:44:34,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:44:34,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:44:34,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'oppervla@@', 'k', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'de', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:44:34,140 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:44:34,140 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:44:34,140 - INFO - joeynmt.training - 	Hypothesis: De arctische oppervlak is , in een zin , het hart van de wereldwijd klimaatsysteem .
2025-05-24 18:44:34,140 - INFO - joeynmt.training - Example #3
2025-05-24 18:44:34,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:44:34,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:44:34,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht@@', 'de', 'in', 'win@@', 'ter', 'en', 'con@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 'cur@@', 's@@', 'er', '.', '</s>']
2025-05-24 18:44:34,140 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:44:34,140 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:44:34,140 - INFO - joeynmt.training - 	Hypothesis: Het verwachtde in winter en concurcurcurcurcurcurser .
2025-05-24 18:44:34,140 - INFO - joeynmt.training - Example #4
2025-05-24 18:44:34,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:44:34,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:44:34,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'p', 'zal', 'een', 'r@@', 'ap@@', 'p', 'van', 'wat', 'er', 'gebeurde', 'gebeurde', '.', '</s>']
2025-05-24 18:44:34,141 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:44:34,141 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:44:34,141 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapp zal een rapp van wat er gebeurde gebeurde .
2025-05-24 18:44:39,442 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.215663, Batch Acc: 0.458742, Tokens per Sec:    13160, Lr: 0.000300
2025-05-24 18:44:44,660 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.191797, Batch Acc: 0.466504, Tokens per Sec:    13533, Lr: 0.000300
2025-05-24 18:44:50,118 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.031986, Batch Acc: 0.470447, Tokens per Sec:    13809, Lr: 0.000300
2025-05-24 18:44:55,396 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.100943, Batch Acc: 0.474616, Tokens per Sec:    13762, Lr: 0.000300
2025-05-24 18:45:00,674 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.170911, Batch Acc: 0.467968, Tokens per Sec:    13641, Lr: 0.000300
2025-05-24 18:45:00,674 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:45:00,674 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:45:09,871 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.28, ppl:   9.75, acc:   0.41, generation: 9.1837[sec], evaluation: 0.0000[sec]
2025-05-24 18:45:09,871 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:45:09,953 - INFO - joeynmt.helpers - delete models/bpe_4k/2500.ckpt
2025-05-24 18:45:09,959 - INFO - joeynmt.training - Example #0
2025-05-24 18:45:09,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:45:09,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:45:09,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ik', 'heb', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'twee', 'dia', '&apos;', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'jaar', 'jaar', 'lang', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ge@@', 'deel@@', 'te', ',', 'heeft', 'door', '40', '%', '.', '</s>']
2025-05-24 18:45:09,960 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:45:09,960 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:45:09,960 - INFO - joeynmt.training - 	Hypothesis: Ik heb deze twee dia &apos; s zo &apos; n twee dia &apos; s die de arctische jaar jaar lang , die de afgelopen drie miljoen jaar is de grootte van de afgelopen drie miljoen jaar gedeelte , heeft door 40 % .
2025-05-24 18:45:09,960 - INFO - joeynmt.training - Example #1
2025-05-24 18:45:09,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:45:09,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:45:09,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mm@@', 'ing', 'van', 'de', 'ser@@', 'ie', 'van', 'dit', 'soort', 'probleem', 'omdat', 'het', 'niet', 'de', 'k@@', 'ru@@', 'i@@', 'cht', 'niet', 'de', 'k@@', 'ent', '.', '</s>']
2025-05-24 18:45:09,961 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:45:09,961 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:45:09,961 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemming van de serie van dit soort probleem omdat het niet de kruicht niet de kent .
2025-05-24 18:45:09,961 - INFO - joeynmt.training - Example #2
2025-05-24 18:45:09,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:45:09,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:45:09,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', 'is', ',', 'in', 'een', 'gevoel', 'van', 'het', 'wereldwij@@', 'd', '.', '</s>']
2025-05-24 18:45:09,962 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:45:09,962 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:45:09,962 - INFO - joeynmt.training - 	Hypothesis: De arctische cap is , in een gevoel van het wereldwijd .
2025-05-24 18:45:09,962 - INFO - joeynmt.training - Example #3
2025-05-24 18:45:09,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:45:09,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:45:09,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht@@', 'te', 'in', 'win@@', 'ter', 'en', 'con@@', 'cur@@', 'cur@@', 's@@', 'er', 'in', 'de', 'zon', '.', '</s>']
2025-05-24 18:45:09,962 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:45:09,962 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:45:09,962 - INFO - joeynmt.training - 	Hypothesis: Het verwachtte in winter en concurcurser in de zon .
2025-05-24 18:45:09,962 - INFO - joeynmt.training - Example #4
2025-05-24 18:45:09,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:45:09,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:45:09,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'u@@', 'u@@', 'i@@', '-@@', 'voor@@', 'uit@@', 'gang', 'van', 'wat', 'er', 'gebeurde', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 18:45:09,963 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:45:09,963 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:45:09,963 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapuui-vooruitgang van wat er gebeurde in de afgelopen 25 jaar .
2025-05-24 18:45:15,416 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.140014, Batch Acc: 0.474486, Tokens per Sec:    13126, Lr: 0.000300
2025-05-24 18:45:24,783 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.278385, Batch Acc: 0.479534, Tokens per Sec:     7625, Lr: 0.000300
2025-05-24 18:45:30,206 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     1.816729, Batch Acc: 0.475277, Tokens per Sec:    13227, Lr: 0.000300
2025-05-24 18:45:35,445 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     1.905260, Batch Acc: 0.484510, Tokens per Sec:    13738, Lr: 0.000300
2025-05-24 18:45:40,665 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     1.912274, Batch Acc: 0.485396, Tokens per Sec:    13696, Lr: 0.000300
2025-05-24 18:45:40,665 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:45:40,665 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:45:50,408 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.23, ppl:   9.27, acc:   0.42, generation: 9.7325[sec], evaluation: 0.0000[sec]
2025-05-24 18:45:50,409 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:45:50,486 - INFO - joeynmt.helpers - delete models/bpe_4k/3000.ckpt
2025-05-24 18:45:50,491 - INFO - joeynmt.training - Example #0
2025-05-24 18:45:50,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:45:50,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:45:50,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meest', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'hele', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'v@@', 'le@@', 'er@@', 'end', '4@@', '8', '.', '</s>']
2025-05-24 18:45:50,492 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:45:50,492 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:45:50,492 - INFO - joeynmt.training - 	Hypothesis: Het jaar heb ik deze twee dia &apos; s zo demonstreren die de arctische cap , die voor de meest drie miljoen jaar is de grootte van de hele drie miljoen jaar is de grootte van de vleerend 48 .
2025-05-24 18:45:50,492 - INFO - joeynmt.training - Example #1
2025-05-24 18:45:50,492 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:45:50,492 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:45:50,492 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'un@@', 't', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'th@@', 'ul@@', 'dige', 'ver@@', 'gelij@@', 'king', 'niet', '.', '</s>']
2025-05-24 18:45:50,493 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:45:50,493 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:45:50,493 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteunt de serieus van dit specifieke probleem omdat het niet de thuldige vergelijking niet .
2025-05-24 18:45:50,493 - INFO - joeynmt.training - Example #2
2025-05-24 18:45:50,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:45:50,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:45:50,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', 'is', ',', 'in', 'een', 'gevoel', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:45:50,493 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:45:50,493 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:45:50,494 - INFO - joeynmt.training - 	Hypothesis: De arctische cap is , in een gevoel van het globale klimaatsysteem .
2025-05-24 18:45:50,494 - INFO - joeynmt.training - Example #3
2025-05-24 18:45:50,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:45:50,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:45:50,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht@@', 'end', 'in', 'win@@', 'ter', 'en', 'con@@', 'stan@@', 'digheden', 'in', 'de', 'win@@', 'ter', '.', '</s>']
2025-05-24 18:45:50,494 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:45:50,494 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:45:50,494 - INFO - joeynmt.training - 	Hypothesis: Het verwachtend in winter en constandigheden in de winter .
2025-05-24 18:45:50,494 - INFO - joeynmt.training - Example #4
2025-05-24 18:45:50,495 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:45:50,495 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:45:50,495 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'ge@@', 'r@@', 'ap@@', 'id', 'van', 'wat', 'er', 'gebeurde', 'over', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:45:50,495 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:45:50,495 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:45:50,495 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik een rapid zijn gerapid van wat er gebeurde over de laatste 25 jaar .
2025-05-24 18:45:55,816 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.010480, Batch Acc: 0.491789, Tokens per Sec:    13413, Lr: 0.000300
2025-05-24 18:46:01,084 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     1.939073, Batch Acc: 0.488042, Tokens per Sec:    13646, Lr: 0.000300
2025-05-24 18:46:06,466 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     1.757291, Batch Acc: 0.487244, Tokens per Sec:    13199, Lr: 0.000300
2025-05-24 18:46:11,696 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     1.835663, Batch Acc: 0.491535, Tokens per Sec:    13483, Lr: 0.000300
2025-05-24 18:46:16,977 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.032860, Batch Acc: 0.493144, Tokens per Sec:    13645, Lr: 0.000300
2025-05-24 18:46:16,977 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:46:16,977 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:46:25,988 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.19, ppl:   8.98, acc:   0.43, generation: 8.9985[sec], evaluation: 0.0000[sec]
2025-05-24 18:46:25,988 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:46:26,067 - INFO - joeynmt.helpers - delete models/bpe_4k/3500.ckpt
2025-05-24 18:46:26,072 - INFO - joeynmt.training - Example #0
2025-05-24 18:46:26,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:46:26,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:46:26,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'ij@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ge@@', 'sla@@', 'at', ',', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'se', '4@@', '8', '.', '</s>']
2025-05-24 18:46:26,072 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:46:26,072 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:46:26,072 - INFO - joeynmt.training - 	Hypothesis: Vijig jaar heb ik deze twee dia &apos; s zo &apos; n geslaat , die de arctische cap , die voor de laatste drie miljoen jaar de grootte van de grootte van de grootte van de grootse 48 .
2025-05-24 18:46:26,072 - INFO - joeynmt.training - Example #1
2025-05-24 18:46:26,072 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:46:26,072 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:46:26,072 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mm@@', 'ing', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'hele', 'hele', 'k@@', 'oe@@', 'p@@', 'ijn', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:46:26,073 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:46:26,073 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:46:26,073 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemming van dit specifieke probleem omdat het niet de hele hele koepijn van de ijs .
2025-05-24 18:46:26,073 - INFO - joeynmt.training - Example #2
2025-05-24 18:46:26,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:46:26,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:46:26,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't@@', 't@@', 'ende', 'har@@', 't@@', 'tel@@', 'ijke', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:46:26,073 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:46:26,073 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:46:26,073 - INFO - joeynmt.training - 	Hypothesis: De arctische cap is , in een zin , het harttende harttelijke klimaatsysteem .
2025-05-24 18:46:26,073 - INFO - joeynmt.training - Example #3
2025-05-24 18:46:26,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:46:26,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:46:26,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'du@@', 'b@@', 'bel@@', 'en', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'con@@', 'stan@@', 'ten', 'in', 'de', 'win@@', 'ter', 'en', '.', '</s>']
2025-05-24 18:46:26,074 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:46:26,074 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:46:26,074 - INFO - joeynmt.training - 	Hypothesis: Het verdubbelen in de winter en conconstanten in de winter en .
2025-05-24 18:46:26,074 - INFO - joeynmt.training - Example #4
2025-05-24 18:46:26,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:46:26,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:46:26,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', '&apos;', 's', 'zal', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'van', 'wat', 'er', 'gebeurde', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:46:26,074 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:46:26,074 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:46:26,074 - INFO - joeynmt.training - 	Hypothesis: De volgende dia &apos; s zal ik je een rapid zijn van wat er gebeurde in de laatste 25 jaar .
2025-05-24 18:46:31,409 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     1.959486, Batch Acc: 0.491615, Tokens per Sec:    12827, Lr: 0.000300
2025-05-24 18:46:36,752 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     1.852762, Batch Acc: 0.495772, Tokens per Sec:    13216, Lr: 0.000300
2025-05-24 18:46:42,008 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     1.866983, Batch Acc: 0.493707, Tokens per Sec:    13546, Lr: 0.000300
2025-05-24 18:46:47,312 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.794884, Batch Acc: 0.497134, Tokens per Sec:    13523, Lr: 0.000300
2025-05-24 18:46:52,732 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     1.920685, Batch Acc: 0.502843, Tokens per Sec:    13207, Lr: 0.000300
2025-05-24 18:46:52,732 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:46:52,732 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:47:01,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.17, ppl:   8.72, acc:   0.43, generation: 8.6518[sec], evaluation: 0.0000[sec]
2025-05-24 18:47:01,394 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:47:01,473 - INFO - joeynmt.helpers - delete models/bpe_4k/4000.ckpt
2025-05-24 18:47:01,480 - INFO - joeynmt.training - Example #0
2025-05-24 18:47:01,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:47:01,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:47:01,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'jaar', ',', 'die', 'de', 'meest', 'van', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'gen', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:47:01,480 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:47:01,480 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:47:01,480 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s die de arctische ijs jaar , die de meest van de afgelopen drie miljoen jaar is de grootte van de grootte van de grootte van de grootte van de lagen , heeft door 40 procent .
2025-05-24 18:47:01,480 - INFO - joeynmt.training - Example #1
2025-05-24 18:47:01,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:47:01,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:47:01,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'dere', 'probleem', 'omdat', 'het', 'niet', 'de', 'ver@@', 'm@@', 'oord', 'is', 'omdat', 'het', 'niet', 'de', 'k@@', 'wal@@', 'iteit', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:47:01,481 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:47:01,481 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:47:01,481 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de seriedere probleem omdat het niet de vermoord is omdat het niet de kwaliteit van de ijs .
2025-05-24 18:47:01,481 - INFO - joeynmt.training - Example #2
2025-05-24 18:47:01,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:47:01,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:47:01,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'arti@@', 'kel', '&apos;', 's', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:47:01,482 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:47:01,482 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:47:01,482 - INFO - joeynmt.training - 	Hypothesis: Het artikel &apos; s ijs is , in een zin , het hart van het globale klimaatsysteem .
2025-05-24 18:47:01,482 - INFO - joeynmt.training - Example #3
2025-05-24 18:47:01,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:47:01,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:47:01,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht@@', 'end', 'in', 'win@@', 'ter', 'en', 'con@@', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:47:01,482 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:47:01,482 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:47:01,482 - INFO - joeynmt.training - 	Hypothesis: Het verwachtend in winter en concontracten in de zomer .
2025-05-24 18:47:01,482 - INFO - joeynmt.training - Example #4
2025-05-24 18:47:01,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:47:01,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:47:01,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'u@@', 'il@@', '-@@', 'voor@@', 'uit', 'wat', 'er', 'gebeurde', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:47:01,483 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:47:01,483 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:47:01,483 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapuil-vooruit wat er gebeurde in de laatste 25 jaar .
2025-05-24 18:47:06,739 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.995870, Batch Acc: 0.506155, Tokens per Sec:    13511, Lr: 0.000300
2025-05-24 18:47:12,252 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     1.938770, Batch Acc: 0.498756, Tokens per Sec:    12829, Lr: 0.000300
2025-05-24 18:47:17,654 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.613401, Batch Acc: 0.506578, Tokens per Sec:    13062, Lr: 0.000300
2025-05-24 18:47:23,088 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     1.846955, Batch Acc: 0.503707, Tokens per Sec:    12682, Lr: 0.000300
2025-05-24 18:47:28,561 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.820509, Batch Acc: 0.505730, Tokens per Sec:    13203, Lr: 0.000300
2025-05-24 18:47:28,561 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:47:28,561 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:47:38,601 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.13, ppl:   8.45, acc:   0.44, generation: 10.0277[sec], evaluation: 0.0000[sec]
2025-05-24 18:47:38,601 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:47:38,683 - INFO - joeynmt.helpers - delete models/bpe_4k/4500.ckpt
2025-05-24 18:47:38,690 - INFO - joeynmt.training - Example #0
2025-05-24 18:47:38,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:47:38,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:47:38,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'an@@', 'ig', 'jaar', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tisch', 'ij@@', 's', 'jaar', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'grote', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', '.', '</s>']
2025-05-24 18:47:38,690 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:47:38,690 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:47:38,690 - INFO - joeynmt.training - 	Hypothesis: Vanig jaar jaar heb ik deze twee dia &apos; s zo demonstreren dat de arctisch ijs jaar , die voor de laatste drie miljoen jaar de grootte van de grootte van de grootte van de grootte van de grootte van de grootte van de grote , die voor de laatste drie miljoen jaar .
2025-05-24 18:47:38,691 - INFO - joeynmt.training - Example #1
2025-05-24 18:47:38,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:47:38,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:47:38,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'dere', 'probleem', 'omdat', 'het', 'niet', 'de', 'ver@@', 'gelij@@', 'king', 'niet', 'de', 'ver@@', 'gelij@@', 'king', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:47:38,691 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:47:38,691 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:47:38,691 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de seriedere probleem omdat het niet de vergelijking niet de vergelijking van de ijs .
2025-05-24 18:47:38,691 - INFO - joeynmt.training - Example #2
2025-05-24 18:47:38,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:47:38,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:47:38,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'ca@@', 'p', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs cap is , in een zin , het hart van het wereldwijd klimaat .
2025-05-24 18:47:38,692 - INFO - joeynmt.training - Example #3
2025-05-24 18:47:38,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:47:38,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:47:38,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht@@', 'te', 'in', 'win@@', 'ter', 'en', 'con@@', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Hypothesis: Het verwachtte in winter en concontracten in zomer .
2025-05-24 18:47:38,692 - INFO - joeynmt.training - Example #4
2025-05-24 18:47:38,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:47:38,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:47:38,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', '&apos;', 's', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'voor@@', 'uit@@', 'gang', 'van', 'wat', 'er', 'gebeurde', ',', '25', 'jaar', '.', '</s>']
2025-05-24 18:47:38,692 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:47:38,694 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:47:38,694 - INFO - joeynmt.training - 	Hypothesis: De volgende dia &apos; s zal ik jullie een rapid vooruitgang van wat er gebeurde , 25 jaar .
2025-05-24 18:47:44,254 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.942540, Batch Acc: 0.504523, Tokens per Sec:    12475, Lr: 0.000300
2025-05-24 18:47:48,742 - INFO - joeynmt.training - Epoch   2: total training loss 7129.64
2025-05-24 18:47:48,742 - INFO - joeynmt.training - EPOCH 3
2025-05-24 18:47:49,675 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     1.837681, Batch Acc: 0.537994, Tokens per Sec:    13071, Lr: 0.000300
2025-05-24 18:47:55,189 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     1.893490, Batch Acc: 0.524409, Tokens per Sec:    12853, Lr: 0.000300
2025-05-24 18:48:00,801 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     1.831041, Batch Acc: 0.531379, Tokens per Sec:    12624, Lr: 0.000300
2025-05-24 18:48:06,478 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     1.844481, Batch Acc: 0.529001, Tokens per Sec:    13059, Lr: 0.000300
2025-05-24 18:48:06,479 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:48:06,479 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:48:17,232 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.11, ppl:   8.25, acc:   0.45, generation: 10.7422[sec], evaluation: 0.0000[sec]
2025-05-24 18:48:17,233 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:48:17,320 - INFO - joeynmt.helpers - delete models/bpe_4k/5000.ckpt
2025-05-24 18:48:17,326 - INFO - joeynmt.training - Example #0
2025-05-24 18:48:17,326 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:48:17,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:48:17,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'ice', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'l@@', 'aa@@', 'g@@', 's@@', 's@@', 'je', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:48:17,327 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:48:17,327 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:48:17,327 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zo demonstreren dat de arcice cap , die voor de laatste drie miljoen jaar de grootte van de grootte van de lag van de grootte van de laagssje , heeft door 40 procent .
2025-05-24 18:48:17,327 - INFO - joeynmt.training - Example #1
2025-05-24 18:48:17,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:48:17,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:48:17,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'van', 'het', 'probleem', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'du@@', 'w@@', 'heid', 'niet', 'de', 'van', 'de', 'ij@@', 's', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:48:17,328 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:48:17,328 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:48:17,328 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten van het probleem van dit probleem omdat het de duwheid niet de van de ijs van de ijs .
2025-05-24 18:48:17,328 - INFO - joeynmt.training - Example #2
2025-05-24 18:48:17,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:48:17,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:48:17,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:48:17,328 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:48:17,328 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:48:17,329 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het hart van het wereldwijd klimaatsysteem .
2025-05-24 18:48:17,329 - INFO - joeynmt.training - Example #3
2025-05-24 18:48:17,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:48:17,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:48:17,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:48:17,329 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:48:17,329 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:48:17,329 - INFO - joeynmt.training - 	Hypothesis: Het verwacht in winter en contracten in zomer .
2025-05-24 18:48:17,329 - INFO - joeynmt.training - Example #4
2025-05-24 18:48:17,330 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:48:17,330 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:48:17,330 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'het', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 18:48:17,331 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:48:17,331 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:48:17,331 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik je een rapid in het afgelopen 25 jaar .
2025-05-24 18:48:22,702 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.712606, Batch Acc: 0.526961, Tokens per Sec:    13210, Lr: 0.000300
2025-05-24 18:48:27,952 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.821481, Batch Acc: 0.524205, Tokens per Sec:    13275, Lr: 0.000300
2025-05-24 18:48:33,314 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.817169, Batch Acc: 0.526983, Tokens per Sec:    13381, Lr: 0.000300
2025-05-24 18:48:38,594 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.621954, Batch Acc: 0.531546, Tokens per Sec:    13790, Lr: 0.000300
2025-05-24 18:48:43,983 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     1.878140, Batch Acc: 0.525622, Tokens per Sec:    13433, Lr: 0.000300
2025-05-24 18:48:43,984 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:48:43,984 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:48:53,302 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.10, ppl:   8.13, acc:   0.45, generation: 9.3079[sec], evaluation: 0.0000[sec]
2025-05-24 18:48:53,302 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:48:53,382 - INFO - joeynmt.helpers - delete models/bpe_4k/5500.ckpt
2025-05-24 18:48:53,388 - INFO - joeynmt.training - Example #0
2025-05-24 18:48:53,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:48:53,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:48:53,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', ',', 'die', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'hij', 'door', '40', '.', '</s>']
2025-05-24 18:48:53,389 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:48:53,389 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:48:53,389 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s , die demonstreren die de arctische ijs , die voor de laatste drie miljoen jaar de grootte van de lager 48 staten , heeft hij door 40 .
2025-05-24 18:48:53,389 - INFO - joeynmt.training - Example #1
2025-05-24 18:48:53,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:48:53,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:48:53,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'bed@@', 're@@', 'ig@@', 'heid', 'niet', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:48:53,389 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:48:53,389 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:48:53,389 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieoudheid van dit specifieke probleem omdat het niet de bedreigheid niet van de ijs .
2025-05-24 18:48:53,390 - INFO - joeynmt.training - Example #2
2025-05-24 18:48:53,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:48:53,390 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:48:53,390 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:48:53,390 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:48:53,390 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:48:53,390 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het hart van het wereldwijd klimaatsysteem .
2025-05-24 18:48:53,390 - INFO - joeynmt.training - Example #3
2025-05-24 18:48:53,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:48:53,390 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:48:53,390 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'wordt', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:48:53,391 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:48:53,391 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:48:53,391 - INFO - joeynmt.training - 	Hypothesis: Het wordt in winter en contracten in de zomer .
2025-05-24 18:48:53,391 - INFO - joeynmt.training - Example #4
2025-05-24 18:48:53,391 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:48:53,391 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:48:53,391 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zijn', 'in', 'wat', 'er', 'gebeurde', ',', 'over', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:48:53,391 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:48:53,391 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:48:53,391 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie een rapid zijn in wat er gebeurde , over de laatste 25 jaar .
2025-05-24 18:48:58,629 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.687788, Batch Acc: 0.521063, Tokens per Sec:    13226, Lr: 0.000300
2025-05-24 18:49:03,902 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     1.688956, Batch Acc: 0.525220, Tokens per Sec:    13541, Lr: 0.000300
2025-05-24 18:49:09,253 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.744969, Batch Acc: 0.528315, Tokens per Sec:    13549, Lr: 0.000300
2025-05-24 18:49:14,459 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.713573, Batch Acc: 0.530184, Tokens per Sec:    13352, Lr: 0.000300
2025-05-24 18:49:19,889 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.783114, Batch Acc: 0.531304, Tokens per Sec:    13078, Lr: 0.000300
2025-05-24 18:49:19,889 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:49:19,889 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:49:28,664 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.08, ppl:   8.04, acc:   0.45, generation: 8.7649[sec], evaluation: 0.0000[sec]
2025-05-24 18:49:28,664 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:49:28,750 - INFO - joeynmt.helpers - delete models/bpe_4k/6000.ckpt
2025-05-24 18:49:28,756 - INFO - joeynmt.training - Example #0
2025-05-24 18:49:28,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:49:28,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:49:28,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', ',', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:49:28,757 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:49:28,757 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:49:28,757 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s , die de arctische ijs , die voor de meeste afgelopen drie miljoen jaar de grootte van de grootte van de lager 48 staten , heeft door 40 procent .
2025-05-24 18:49:28,757 - INFO - joeynmt.training - Example #1
2025-05-24 18:49:28,757 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:49:28,757 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:49:28,757 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'ste@@', 'mt', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'bed@@', 'i@@', 'ding', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:49:28,757 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:49:28,758 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:49:28,758 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstemt de serieoudheid van dit probleem omdat het niet de bediding van het ijs van het ijs .
2025-05-24 18:49:28,758 - INFO - joeynmt.training - Example #2
2025-05-24 18:49:28,758 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:49:28,758 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:49:28,758 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 18:49:28,758 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:49:28,758 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:49:28,758 - INFO - joeynmt.training - 	Hypothesis: De arctische cap is , in een zin , het hart van het globale klimaat .
2025-05-24 18:49:28,758 - INFO - joeynmt.training - Example #3
2025-05-24 18:49:28,758 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:49:28,758 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:49:28,758 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'p@@', 'ijn', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:49:28,758 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:49:28,760 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:49:28,760 - INFO - joeynmt.training - 	Hypothesis: Het duipijn en contracten in de zomer .
2025-05-24 18:49:28,760 - INFO - joeynmt.training - Example #4
2025-05-24 18:49:28,760 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:49:28,760 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:49:28,760 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laten', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'van', 'wat', 'er', 'gebeurde', '.', '</s>']
2025-05-24 18:49:28,760 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:49:28,760 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:49:28,760 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laten zien dat een rapid van wat er gebeurde .
2025-05-24 18:49:34,052 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.858992, Batch Acc: 0.534293, Tokens per Sec:    13196, Lr: 0.000300
2025-05-24 18:49:39,345 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.705098, Batch Acc: 0.536175, Tokens per Sec:    13660, Lr: 0.000300
2025-05-24 18:49:44,713 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.622796, Batch Acc: 0.534948, Tokens per Sec:    13433, Lr: 0.000300
2025-05-24 18:49:49,917 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.612491, Batch Acc: 0.532996, Tokens per Sec:    13604, Lr: 0.000300
2025-05-24 18:49:55,251 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.788293, Batch Acc: 0.535309, Tokens per Sec:    13099, Lr: 0.000300
2025-05-24 18:49:55,251 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:49:55,251 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:50:04,597 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.93, acc:   0.46, generation: 9.3345[sec], evaluation: 0.0000[sec]
2025-05-24 18:50:04,597 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:50:04,682 - INFO - joeynmt.helpers - delete models/bpe_4k/6500.ckpt
2025-05-24 18:50:04,687 - INFO - joeynmt.training - Example #0
2025-05-24 18:50:04,687 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:50:04,687 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:50:04,687 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:50:04,688 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:50:04,688 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:50:04,688 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zo demonstreren dat de arctische ijs , die de afgelopen drie miljoen jaar de grootte van de lager 48 staten , heeft door 40 procent .
2025-05-24 18:50:04,688 - INFO - joeynmt.training - Example #1
2025-05-24 18:50:04,688 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:50:04,688 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:50:04,688 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'van', 'dit', 'probleem', 'omdat', 'het', 'een', 'probleem', 'van', 'dit', 'probleem', 'niet', 'laat', 'zien', 'omdat', 'het', 'de', 'd@@', 'w@@', 'ol@@', 'k', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:50:04,688 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:50:04,688 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:50:04,688 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten van dit probleem omdat het een probleem van dit probleem niet laat zien omdat het de dwolk van de ijs .
2025-05-24 18:50:04,688 - INFO - joeynmt.training - Example #2
2025-05-24 18:50:04,689 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:50:04,689 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:50:04,689 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', 'is', ',', 'in', 'een', 'gevoel', ',', 'het', 'be@@', 'ta@@', 'ald', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:50:04,689 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:50:04,689 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:50:04,689 - INFO - joeynmt.training - 	Hypothesis: De arctische cap is , in een gevoel , het betaald van het wereldwijd klimaatsysteem .
2025-05-24 18:50:04,689 - INFO - joeynmt.training - Example #3
2025-05-24 18:50:04,689 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:50:04,689 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:50:04,689 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'b@@', 'bel@@', 'en', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', '.', '</s>']
2025-05-24 18:50:04,689 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:50:04,690 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:50:04,690 - INFO - joeynmt.training - 	Hypothesis: Het dubbelen in de zomer en contracten .
2025-05-24 18:50:04,690 - INFO - joeynmt.training - Example #4
2025-05-24 18:50:04,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:50:04,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:50:04,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'voor@@', 'uit', 'wat', 'er', 'gebeurd', 'is', 'gebeurd', 'gebeurd', '.', '</s>']
2025-05-24 18:50:04,691 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:50:04,691 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:50:04,691 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik je een rapid vooruit wat er gebeurd is gebeurd gebeurd .
2025-05-24 18:50:09,923 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.626376, Batch Acc: 0.530810, Tokens per Sec:    13512, Lr: 0.000300
2025-05-24 18:50:15,342 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     2.007335, Batch Acc: 0.533028, Tokens per Sec:    13217, Lr: 0.000300
2025-05-24 18:50:20,597 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.643019, Batch Acc: 0.537658, Tokens per Sec:    13230, Lr: 0.000300
2025-05-24 18:50:25,844 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.612007, Batch Acc: 0.530266, Tokens per Sec:    13502, Lr: 0.000300
2025-05-24 18:50:31,105 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.697919, Batch Acc: 0.533420, Tokens per Sec:    13487, Lr: 0.000300
2025-05-24 18:50:31,106 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:50:31,106 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:50:39,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.05, ppl:   7.75, acc:   0.46, generation: 8.2759[sec], evaluation: 0.0000[sec]
2025-05-24 18:50:39,394 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:50:39,473 - INFO - joeynmt.helpers - delete models/bpe_4k/7000.ckpt
2025-05-24 18:50:39,478 - INFO - joeynmt.training - Example #0
2025-05-24 18:50:39,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:50:39,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:50:39,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', ',', 'die', 'voor', 'de', 'grootste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', 'jaren', 'van', 'de', 'la@@', 'g@@', 'ere', 'jaren', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:50:39,479 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:50:39,479 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:50:39,479 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat de arctische ijs , die voor de meeste laatste drie miljoen jaar , die voor de grootste drie miljoen jaar de grootte van de lagere jaren van de lagere jaren , heeft door 40 procent .
2025-05-24 18:50:39,479 - INFO - joeynmt.training - Example #1
2025-05-24 18:50:39,479 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:50:39,479 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:50:39,479 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'dan@@', 'k@@', 'heid', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 18:50:39,479 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:50:39,479 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:50:39,479 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit probleem omdat het niet de dankheid van het ijs laat zien .
2025-05-24 18:50:39,479 - INFO - joeynmt.training - Example #2
2025-05-24 18:50:39,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:50:39,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:50:39,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:50:39,480 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:50:39,480 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:50:39,480 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in een zin , het hart van het wereldwijde klimaatsysteem .
2025-05-24 18:50:39,480 - INFO - joeynmt.training - Example #3
2025-05-24 18:50:39,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:50:39,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:50:39,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'b@@', 'bel@@', 'en', 'in', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:50:39,481 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:50:39,481 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:50:39,481 - INFO - joeynmt.training - 	Hypothesis: Het dubbelen in zomer en contracten in zomer .
2025-05-24 18:50:39,481 - INFO - joeynmt.training - Example #4
2025-05-24 18:50:39,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:50:39,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:50:39,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'voor@@', 'uit@@', 'zien', 'van', 'wat', 'er', 'gebeurde', 'gebeurde', '.', '</s>']
2025-05-24 18:50:39,481 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:50:39,481 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:50:39,481 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je een rapid zal vooruitzien van wat er gebeurde gebeurde .
2025-05-24 18:50:44,781 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.641171, Batch Acc: 0.538959, Tokens per Sec:    13511, Lr: 0.000300
2025-05-24 18:50:50,099 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.797770, Batch Acc: 0.537448, Tokens per Sec:    13196, Lr: 0.000300
2025-05-24 18:50:55,377 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.553232, Batch Acc: 0.532257, Tokens per Sec:    13693, Lr: 0.000300
2025-05-24 18:51:00,773 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.619768, Batch Acc: 0.539035, Tokens per Sec:    13546, Lr: 0.000300
2025-05-24 18:51:06,148 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.732340, Batch Acc: 0.539516, Tokens per Sec:    13381, Lr: 0.000300
2025-05-24 18:51:06,149 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:51:06,149 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:51:14,337 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.03, ppl:   7.62, acc:   0.46, generation: 8.1755[sec], evaluation: 0.0000[sec]
2025-05-24 18:51:14,337 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:51:14,422 - INFO - joeynmt.helpers - delete models/bpe_4k/7500.ckpt
2025-05-24 18:51:14,427 - INFO - joeynmt.training - Example #0
2025-05-24 18:51:14,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:51:14,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:51:14,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', ',', 'die', 'voor', 'de', 'grootste', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:51:14,428 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:51:14,428 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:51:14,428 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zo &apos; n arctische ijs , die voor de meeste laatste drie miljoen jaar , die voor de grootste van de lagere 48 staten , heeft door 40 procent .
2025-05-24 18:51:14,428 - INFO - joeynmt.training - Example #1
2025-05-24 18:51:14,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:51:14,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:51:14,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'dan@@', 'k@@', 'heid', 'van', 'de', 'ij@@', 's', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:51:14,429 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:51:14,429 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:51:14,429 - INFO - joeynmt.training - 	Hypothesis: Maar dit begrip de serieoudheid van dit probleem omdat het niet de dankheid van de ijs van de ijs .
2025-05-24 18:51:14,429 - INFO - joeynmt.training - Example #2
2025-05-24 18:51:14,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:51:14,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:51:14,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'al@@', 'baar', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:51:14,429 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:51:14,430 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:51:14,430 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het betaalbaar hart van het wereldwijd klimaatsysteem .
2025-05-24 18:51:14,430 - INFO - joeynmt.training - Example #3
2025-05-24 18:51:14,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:51:14,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:51:14,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'w@@', 'w@@', 'w@@', 'w@@', 'ens', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:51:14,430 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:51:14,430 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:51:14,430 - INFO - joeynmt.training - 	Hypothesis: Het duwwwwens en contracten in zomer .
2025-05-24 18:51:14,430 - INFO - joeynmt.training - Example #4
2025-05-24 18:51:14,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:51:14,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:51:14,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeurde', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:51:14,431 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:51:14,431 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:51:14,431 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid zal zijn van wat er gebeurde in de laatste 25 jaar .
2025-05-24 18:51:19,754 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.719264, Batch Acc: 0.536768, Tokens per Sec:    13433, Lr: 0.000300
2025-05-24 18:51:25,052 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.800856, Batch Acc: 0.536413, Tokens per Sec:    13178, Lr: 0.000300
2025-05-24 18:51:30,200 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.601154, Batch Acc: 0.541936, Tokens per Sec:    13387, Lr: 0.000300
2025-05-24 18:51:35,601 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.733570, Batch Acc: 0.547309, Tokens per Sec:    13288, Lr: 0.000300
2025-05-24 18:51:40,909 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.735322, Batch Acc: 0.539580, Tokens per Sec:    13412, Lr: 0.000300
2025-05-24 18:51:40,909 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:51:40,909 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:51:50,776 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.49, acc:   0.47, generation: 9.8549[sec], evaluation: 0.0000[sec]
2025-05-24 18:51:50,776 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:51:50,855 - INFO - joeynmt.helpers - delete models/bpe_4k/8000.ckpt
2025-05-24 18:51:50,861 - INFO - joeynmt.training - Example #0
2025-05-24 18:51:50,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:51:50,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:51:50,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'ri@@', 'k', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:51:50,862 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:51:50,862 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:51:50,862 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs arctische ijs , die de meeste afgelopen drie miljoen jaar de grootte van de lagere 48 staten , heeft een krik door 40 procent .
2025-05-24 18:51:50,862 - INFO - joeynmt.training - Example #1
2025-05-24 18:51:50,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:51:50,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:51:50,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'heid', 'van', 'de', 'ij@@', 's', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:51:50,863 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:51:50,863 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:51:50,863 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serie van dit specifieke probleem omdat het niet de dheid van de ijs van de ijs .
2025-05-24 18:51:50,863 - INFO - joeynmt.training - Example #2
2025-05-24 18:51:50,863 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:51:50,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:51:50,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'ald', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 18:51:50,863 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:51:50,863 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:51:50,864 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het betaald van het globale klimaat .
2025-05-24 18:51:50,864 - INFO - joeynmt.training - Example #3
2025-05-24 18:51:50,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:51:50,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:51:50,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'be@@', 'du@@', 'ur', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:51:50,864 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:51:50,864 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:51:50,864 - INFO - joeynmt.training - 	Hypothesis: Het beduur in winter en contracten in zomer .
2025-05-24 18:51:50,864 - INFO - joeynmt.training - Example #4
2025-05-24 18:51:50,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:51:50,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:51:50,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'voor@@', 'uit', 'wat', 'er', 'gebeurde', ',', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 18:51:50,865 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:51:50,865 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:51:50,865 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid fast-vooruit wat er gebeurde , de afgelopen 25 jaar .
2025-05-24 18:51:56,239 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.662545, Batch Acc: 0.541554, Tokens per Sec:    12707, Lr: 0.000300
2025-05-24 18:52:01,535 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.448961, Batch Acc: 0.545919, Tokens per Sec:    13811, Lr: 0.000300
2025-05-24 18:52:06,141 - INFO - joeynmt.training - Epoch   3: total training loss 6250.41
2025-05-24 18:52:06,142 - INFO - joeynmt.training - EPOCH 4
2025-05-24 18:52:06,857 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     1.572300, Batch Acc: 0.567211, Tokens per Sec:    11982, Lr: 0.000300
2025-05-24 18:52:12,232 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     1.623296, Batch Acc: 0.562585, Tokens per Sec:    13262, Lr: 0.000300
2025-05-24 18:52:17,579 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     1.497799, Batch Acc: 0.571669, Tokens per Sec:    13881, Lr: 0.000300
2025-05-24 18:52:17,579 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:52:17,579 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:52:26,587 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.46, acc:   0.47, generation: 8.9960[sec], evaluation: 0.0000[sec]
2025-05-24 18:52:26,587 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:52:26,668 - INFO - joeynmt.helpers - delete models/bpe_4k/8500.ckpt
2025-05-24 18:52:26,673 - INFO - joeynmt.training - Example #0
2025-05-24 18:52:26,673 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:52:26,673 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:52:26,673 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'jaar', 'die', 'de', 'ar@@', 'c@@', 'tic@@', 'a', ',', 'wat', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'gen', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'ort', '.', '</s>']
2025-05-24 18:52:26,673 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:52:26,673 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:52:26,673 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat de arctische ijs jaar die de arctica , wat voor de laatste drie miljoen jaar de grootte van de lagen 48 staten , heeft een kort .
2025-05-24 18:52:26,673 - INFO - joeynmt.training - Example #1
2025-05-24 18:52:26,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:52:26,674 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:52:26,674 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ood@@', 'heid', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:52:26,674 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:52:26,674 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:52:26,674 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit specifieke probleem omdat het de doodheid van het ijs van het ijs .
2025-05-24 18:52:26,674 - INFO - joeynmt.training - Example #2
2025-05-24 18:52:26,674 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:52:26,674 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:52:26,674 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 'ijn', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Hypothesis: De arctische ijpijn is , in een zin , het hart van het globale klimaatsysteem .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - Example #3
2025-05-24 18:52:26,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:52:26,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:52:26,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'w@@', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Hypothesis: Het duwwinter en contracten in zomer .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - Example #4
2025-05-24 18:52:26,675 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:52:26,675 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:52:26,675 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'staat', 'is', 'gebeurd', ',', 'over', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:52:26,675 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid in staat is gebeurd , over de laatste 25 jaar .
2025-05-24 18:52:32,182 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     1.655979, Batch Acc: 0.559602, Tokens per Sec:    13097, Lr: 0.000300
2025-05-24 18:52:37,350 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     1.454449, Batch Acc: 0.562677, Tokens per Sec:    13628, Lr: 0.000300
2025-05-24 18:52:42,630 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.449553, Batch Acc: 0.562974, Tokens per Sec:    13114, Lr: 0.000300
2025-05-24 18:52:47,963 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.545262, Batch Acc: 0.562311, Tokens per Sec:    13299, Lr: 0.000300
2025-05-24 18:52:53,233 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.568066, Batch Acc: 0.562352, Tokens per Sec:    13760, Lr: 0.000300
2025-05-24 18:52:53,233 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:52:53,233 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:53:01,767 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.42, acc:   0.47, generation: 8.5186[sec], evaluation: 0.0000[sec]
2025-05-24 18:53:01,767 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:53:01,851 - INFO - joeynmt.helpers - delete models/bpe_4k/9000.ckpt
2025-05-24 18:53:01,856 - INFO - joeynmt.training - Example #0
2025-05-24 18:53:01,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:53:01,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:53:01,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'jaar', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'jaar', ',', 'die', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'ster@@', 'k', ',', 'is', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:53:01,856 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:53:01,856 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:53:01,856 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s die de arctische ijs jaar die de arctische ijs jaar , die de grootte van de lager 48 sterk , is door 40 procent .
2025-05-24 18:53:01,856 - INFO - joeynmt.training - Example #1
2025-05-24 18:53:01,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:53:01,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:53:01,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'dan@@', 'k@@', 'zij', 'het', 'niet', 'de', 'dan@@', 'k@@', 'zij', 'het', 'niet', '.', '</s>']
2025-05-24 18:53:01,858 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:53:01,858 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:53:01,858 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit specifieke probleem omdat het niet de dankzij het niet de dankzij het niet .
2025-05-24 18:53:01,858 - INFO - joeynmt.training - Example #2
2025-05-24 18:53:01,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:53:01,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:53:01,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', 'een', 'idee', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'alt', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:53:01,858 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:53:01,858 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:53:01,859 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is een idee , in een zin , het betaalt hart van het wereldwijd klimaatsysteem .
2025-05-24 18:53:01,859 - INFO - joeynmt.training - Example #3
2025-05-24 18:53:01,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:53:01,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:53:01,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 'de', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:53:01,859 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:53:01,859 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:53:01,859 - INFO - joeynmt.training - 	Hypothesis: Het duurde in winter en contracten in zomer .
2025-05-24 18:53:01,859 - INFO - joeynmt.training - Example #4
2025-05-24 18:53:01,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:53:01,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:53:01,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', 'st@@', 'aan@@', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:53:01,860 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:53:01,860 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:53:01,860 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie een rapid faststaande laatste 25 jaar .
2025-05-24 18:53:07,211 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.642590, Batch Acc: 0.561692, Tokens per Sec:    13193, Lr: 0.000300
2025-05-24 18:53:12,592 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.520675, Batch Acc: 0.561460, Tokens per Sec:    13308, Lr: 0.000300
2025-05-24 18:53:17,945 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.714339, Batch Acc: 0.555939, Tokens per Sec:    13308, Lr: 0.000300
2025-05-24 18:53:23,102 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.900842, Batch Acc: 0.559949, Tokens per Sec:    13720, Lr: 0.000300
2025-05-24 18:53:28,370 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.580097, Batch Acc: 0.555995, Tokens per Sec:    13245, Lr: 0.000300
2025-05-24 18:53:28,370 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:53:28,370 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:53:37,530 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.36, acc:   0.47, generation: 9.1512[sec], evaluation: 0.0000[sec]
2025-05-24 18:53:37,532 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:53:37,610 - INFO - joeynmt.helpers - delete models/bpe_4k/9500.ckpt
2025-05-24 18:53:37,615 - INFO - joeynmt.training - Example #0
2025-05-24 18:53:37,615 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:53:37,615 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:53:37,615 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'ij@@', 's', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'het', 'k@@', 'loo@@', 'f', 'van', '40', 'procent', '.', '</s>']
2025-05-24 18:53:37,616 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:53:37,616 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:53:37,616 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat de arctische ijs ijs ijs , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , heeft het kloof van 40 procent .
2025-05-24 18:53:37,616 - INFO - joeynmt.training - Example #1
2025-05-24 18:53:37,616 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:53:37,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:53:37,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'het', 'niet', 'de', 'dan@@', 'k@@', 'zij', 'de', 'd@@', 'oos', 'van', 'de', 'ij@@', 's', 'laten', 'zien', '.', '</s>']
2025-05-24 18:53:37,616 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:53:37,616 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:53:37,616 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten de serieoudheid van dit probleem omdat het het niet de dankzij de doos van de ijs laten zien .
2025-05-24 18:53:37,617 - INFO - joeynmt.training - Example #2
2025-05-24 18:53:37,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:53:37,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:53:37,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'al@@', 'de', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:53:37,617 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:53:37,617 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:53:37,617 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in een zin , het betaalde het hart van het wereldwijd klimaatsysteem .
2025-05-24 18:53:37,617 - INFO - joeynmt.training - Example #3
2025-05-24 18:53:37,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:53:37,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:53:37,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'w@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:53:37,618 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:53:37,618 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:53:37,618 - INFO - joeynmt.training - 	Hypothesis: Het duwt in winter en contracten in zomer .
2025-05-24 18:53:37,618 - INFO - joeynmt.training - Example #4
2025-05-24 18:53:37,618 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:53:37,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:53:37,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', 'aan@@', 'de', 'van', 'wat', 'er', 'gebeurd', 'is', 'gebeurd', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:53:37,618 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:53:37,618 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:53:37,618 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik je een rapid fastaande van wat er gebeurd is gebeurd in de laatste 25 jaar .
2025-05-24 18:53:42,967 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.681167, Batch Acc: 0.560056, Tokens per Sec:    13305, Lr: 0.000300
2025-05-24 18:53:48,223 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.543769, Batch Acc: 0.563198, Tokens per Sec:    13844, Lr: 0.000300
2025-05-24 18:53:53,578 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.494807, Batch Acc: 0.566459, Tokens per Sec:    13736, Lr: 0.000300
2025-05-24 18:53:58,743 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     1.627446, Batch Acc: 0.561557, Tokens per Sec:    13468, Lr: 0.000300
2025-05-24 18:54:04,214 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     1.748705, Batch Acc: 0.558881, Tokens per Sec:    13221, Lr: 0.000300
2025-05-24 18:54:04,214 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:54:04,214 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:54:13,580 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.25, acc:   0.48, generation: 9.3542[sec], evaluation: 0.0000[sec]
2025-05-24 18:54:13,580 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:54:13,657 - INFO - joeynmt.helpers - delete models/bpe_4k/10000.ckpt
2025-05-24 18:54:13,663 - INFO - joeynmt.training - Example #0
2025-05-24 18:54:13,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:54:13,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:54:13,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'door', '40', '%', '.', '</s>']
2025-05-24 18:54:13,664 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:54:13,664 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:54:13,664 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de arctische ijs die de arctische ijs , die de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , is door 40 % .
2025-05-24 18:54:13,664 - INFO - joeynmt.training - Example #1
2025-05-24 18:54:13,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:54:13,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:54:13,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oos', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 18:54:13,664 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:54:13,666 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:54:13,666 - INFO - joeynmt.training - 	Hypothesis: Maar deze begreep de serieoudheid van dit probleem probleem omdat het de doos van het ijs laat zien .
2025-05-24 18:54:13,666 - INFO - joeynmt.training - Example #2
2025-05-24 18:54:13,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:54:13,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:54:13,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'str@@', 'oo@@', 'm@@', 'stel@@', 't', 'van', 'het', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:54:13,666 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:54:13,666 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:54:13,666 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het stroomstelt van het klimaatsysteem .
2025-05-24 18:54:13,666 - INFO - joeynmt.training - Example #3
2025-05-24 18:54:13,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:54:13,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:54:13,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'wen', 'in', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:54:13,667 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:54:13,667 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:54:13,667 - INFO - joeynmt.training - 	Hypothesis: Het duwen in en contracten in zomer .
2025-05-24 18:54:13,667 - INFO - joeynmt.training - Example #4
2025-05-24 18:54:13,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:54:13,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:54:13,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 18:54:13,668 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:54:13,668 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:54:13,668 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid in de afgelopen 25 jaar is .
2025-05-24 18:54:18,866 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     1.536547, Batch Acc: 0.558071, Tokens per Sec:    13766, Lr: 0.000300
2025-05-24 18:54:24,014 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     1.650709, Batch Acc: 0.558048, Tokens per Sec:    13860, Lr: 0.000300
2025-05-24 18:54:29,280 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     1.614476, Batch Acc: 0.561841, Tokens per Sec:    13769, Lr: 0.000300
2025-05-24 18:54:34,550 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     1.598508, Batch Acc: 0.561530, Tokens per Sec:    13259, Lr: 0.000300
2025-05-24 18:54:39,939 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     1.647610, Batch Acc: 0.562460, Tokens per Sec:    13545, Lr: 0.000300
2025-05-24 18:54:39,940 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:54:39,940 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:54:49,755 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.18, acc:   0.48, generation: 9.8028[sec], evaluation: 0.0000[sec]
2025-05-24 18:54:49,755 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:54:49,833 - INFO - joeynmt.helpers - delete models/bpe_4k/10500.ckpt
2025-05-24 18:54:49,838 - INFO - joeynmt.training - Example #0
2025-05-24 18:54:49,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'k@@', 'eerd', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de arctische ijs die de arctische ijs , die voor de meeste drie miljoen jaar de grootte van de lager 48 staten , heeft gekeerd , heeft door 40 procent .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - Example #1
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oos', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit specifieke probleem omdat het de doos van het ijs niet laat zien .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - Example #2
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in een zin , het hart van het wereldwijde klimaatsysteem .
2025-05-24 18:54:49,839 - INFO - joeynmt.training - Example #3
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:54:49,839 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'd@@', 'rij@@', 'ft', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:54:49,841 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:54:49,841 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:54:49,841 - INFO - joeynmt.training - 	Hypothesis: Het drijft in winter en contracten in zomer .
2025-05-24 18:54:49,841 - INFO - joeynmt.training - Example #4
2025-05-24 18:54:49,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:54:49,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:54:49,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'geloof', 'van', 'wat', 'er', 'gebeurd', 'is', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:54:49,841 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:54:49,841 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:54:49,841 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid geloof van wat er gebeurd is in de laatste 25 jaar .
2025-05-24 18:54:54,998 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.667178, Batch Acc: 0.561096, Tokens per Sec:    13831, Lr: 0.000300
2025-05-24 18:55:00,394 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.627657, Batch Acc: 0.562590, Tokens per Sec:    13236, Lr: 0.000300
2025-05-24 18:55:05,587 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.599542, Batch Acc: 0.559504, Tokens per Sec:    13184, Lr: 0.000300
2025-05-24 18:55:10,907 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.475309, Batch Acc: 0.560068, Tokens per Sec:    13475, Lr: 0.000300
2025-05-24 18:55:16,140 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.570924, Batch Acc: 0.563150, Tokens per Sec:    13976, Lr: 0.000300
2025-05-24 18:55:16,141 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:55:16,141 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:55:25,749 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.12, acc:   0.48, generation: 9.5983[sec], evaluation: 0.0000[sec]
2025-05-24 18:55:25,749 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:55:25,827 - INFO - joeynmt.helpers - delete models/bpe_4k/11000.ckpt
2025-05-24 18:55:25,833 - INFO - joeynmt.training - Example #0
2025-05-24 18:55:25,834 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:55:25,834 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:55:25,834 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'sk@@', 'am@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', 'van', 'de', 'la@@', 'g', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'aar@@', 'ten', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:55:25,834 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:55:25,834 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:55:25,834 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de arctische ijskamp , die voor de meeste drie miljoen jaar de grootte van de lager van de lag van de lager 48 staten , heeft een kaarten door 40 procent .
2025-05-24 18:55:25,834 - INFO - joeynmt.training - Example #1
2025-05-24 18:55:25,834 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:55:25,834 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:55:25,834 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'i@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oos', 'niet', 'de', 'th@@', 'ul@@', 't', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:55:25,835 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:55:25,835 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:55:25,835 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serioudheid van dit probleem omdat het de doos niet de thult van het ijs .
2025-05-24 18:55:25,835 - INFO - joeynmt.training - Example #2
2025-05-24 18:55:25,835 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:55:25,835 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:55:25,835 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:55:25,835 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:55:25,835 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in een zin , het hart van het wereldwijde klimaatsysteem .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - Example #3
2025-05-24 18:55:25,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:55:25,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:55:25,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'ster@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Hypothesis: Het duistert in winter en contracten in zomer .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - Example #4
2025-05-24 18:55:25,836 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:55:25,836 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:55:25,836 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'zal', 'voor@@', 'uit@@', 'gaan', 'van', 'wat', 'er', 'gebeurt', 'over', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:55:25,836 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie een rapid zal vooruitgaan van wat er gebeurt over de laatste 25 jaar .
2025-05-24 18:55:31,151 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.741153, Batch Acc: 0.563128, Tokens per Sec:    13199, Lr: 0.000300
2025-05-24 18:55:36,365 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.585928, Batch Acc: 0.560670, Tokens per Sec:    13339, Lr: 0.000300
2025-05-24 18:55:41,603 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.579495, Batch Acc: 0.564712, Tokens per Sec:    13627, Lr: 0.000300
2025-05-24 18:55:46,973 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.668643, Batch Acc: 0.562782, Tokens per Sec:    13608, Lr: 0.000300
2025-05-24 18:55:52,162 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.709138, Batch Acc: 0.561675, Tokens per Sec:    13533, Lr: 0.000300
2025-05-24 18:55:52,162 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:55:52,162 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:56:01,876 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.07, acc:   0.48, generation: 9.7026[sec], evaluation: 0.0000[sec]
2025-05-24 18:56:01,877 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:56:01,962 - INFO - joeynmt.helpers - delete models/bpe_4k/11500.ckpt
2025-05-24 18:56:01,967 - INFO - joeynmt.training - Example #0
2025-05-24 18:56:01,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:56:01,968 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:56:01,968 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', '&apos;', 'n', 'ge@@', 'de@@', 'mon@@', 'stre@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 's@@', 's@@', 'me@@', 'de@@', 'de@@', 'de@@', '-@@', '-@@', '-@@', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'k@@', 'eerd', ',', 'heeft', 'het', 'wordt', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:56:01,968 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:56:01,968 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:56:01,968 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zo &apos; n gedemonstreeren die de arctische ijpssmededede---48 staten , heeft gekeerd , heeft het wordt door 40 procent .
2025-05-24 18:56:01,968 - INFO - joeynmt.training - Example #1
2025-05-24 18:56:01,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:56:01,968 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:56:01,968 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'd@@', 'w@@', 'heid', 'van', 'de', 'ij@@', 's', 'niet', 'te', 'laten', 'zien', '.', '</s>']
2025-05-24 18:56:01,969 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:56:01,969 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:56:01,969 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieus van dit specifieke probleem omdat het niet de dwheid van de ijs niet te laten zien .
2025-05-24 18:56:01,969 - INFO - joeynmt.training - Example #2
2025-05-24 18:56:01,969 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:56:01,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:56:01,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'al@@', 'baar', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 18:56:01,969 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:56:01,969 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:56:01,969 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het betaalbaar hart van het globale klimaat .
2025-05-24 18:56:01,970 - INFO - joeynmt.training - Example #3
2025-05-24 18:56:01,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:56:01,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:56:01,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:56:01,970 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:56:01,970 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:56:01,970 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in zomer .
2025-05-24 18:56:01,970 - INFO - joeynmt.training - Example #4
2025-05-24 18:56:01,970 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:56:01,970 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:56:01,970 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'zal', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ler', 'zijn', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 18:56:01,970 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:56:01,970 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:56:01,970 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je zal een rapid sneller zijn van wat er de afgelopen 25 jaar is .
2025-05-24 18:56:07,372 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.521201, Batch Acc: 0.563832, Tokens per Sec:    12800, Lr: 0.000300
2025-05-24 18:56:12,621 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.423877, Batch Acc: 0.561611, Tokens per Sec:    13235, Lr: 0.000300
2025-05-24 18:56:17,928 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.486322, Batch Acc: 0.564674, Tokens per Sec:    13125, Lr: 0.000300
2025-05-24 18:56:22,564 - INFO - joeynmt.training - Epoch   4: total training loss 5802.80
2025-05-24 18:56:22,564 - INFO - joeynmt.training - EPOCH 5
2025-05-24 18:56:23,176 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     1.514257, Batch Acc: 0.599975, Tokens per Sec:    13010, Lr: 0.000300
2025-05-24 18:56:28,445 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     1.655931, Batch Acc: 0.588230, Tokens per Sec:    13829, Lr: 0.000300
2025-05-24 18:56:28,446 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:56:28,446 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:56:38,443 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.08, acc:   0.48, generation: 9.9858[sec], evaluation: 0.0000[sec]
2025-05-24 18:56:38,526 - INFO - joeynmt.helpers - delete models/bpe_4k/12000.ckpt
2025-05-24 18:56:38,532 - INFO - joeynmt.training - Example #0
2025-05-24 18:56:38,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:56:38,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:56:38,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'aar@@', 'tje', 'van', '40', 'procent', '.', '</s>']
2025-05-24 18:56:38,532 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:56:38,532 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:56:38,533 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs die de arctische ijs , die voor de laatste drie miljoen jaar de grootte van de lager 48 staten , heeft een kaartje van 40 procent .
2025-05-24 18:56:38,533 - INFO - joeynmt.training - Example #1
2025-05-24 18:56:38,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:56:38,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:56:38,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'beg@@', 're@@', 'pen', 'de', 'ser@@', 'ie', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oos', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:56:38,533 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:56:38,533 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:56:38,533 - INFO - joeynmt.training - 	Hypothesis: Maar deze begrepen de serie van dit probleem omdat het de doos van het ijs van het ijs .
2025-05-24 18:56:38,533 - INFO - joeynmt.training - Example #2
2025-05-24 18:56:38,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'str@@', 'aten', 'har@@', 't', 'van', 'het', 'mon@@', 'di@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het straten hart van het mondiale klimaatsysteem .
2025-05-24 18:56:38,534 - INFO - joeynmt.training - Example #3
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'd@@', 'oo@@', 'dt', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Hypothesis: Het doodt in winter en contracten in zomer .
2025-05-24 18:56:38,534 - INFO - joeynmt.training - Example #4
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:56:38,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laten', 'zien', 'een', 'r@@', 'ap@@', 'id', 'in', 'wat', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 18:56:38,534 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:56:38,536 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:56:38,536 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laten zien een rapid in wat de afgelopen 25 jaar is .
2025-05-24 18:56:43,779 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     1.687066, Batch Acc: 0.583496, Tokens per Sec:    13239, Lr: 0.000300
2025-05-24 18:56:48,977 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     1.644172, Batch Acc: 0.585768, Tokens per Sec:    13460, Lr: 0.000300
2025-05-24 18:56:54,258 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     1.353871, Batch Acc: 0.586076, Tokens per Sec:    13279, Lr: 0.000300
2025-05-24 18:56:59,422 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     1.455567, Batch Acc: 0.584852, Tokens per Sec:    13920, Lr: 0.000300
2025-05-24 18:57:04,665 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     1.479973, Batch Acc: 0.580746, Tokens per Sec:    13303, Lr: 0.000300
2025-05-24 18:57:04,665 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:57:04,666 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:57:13,870 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.06, acc:   0.48, generation: 9.1915[sec], evaluation: 0.0000[sec]
2025-05-24 18:57:13,871 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:57:13,948 - INFO - joeynmt.helpers - delete models/bpe_4k/12500.ckpt
2025-05-24 18:57:13,953 - INFO - joeynmt.training - Example #0
2025-05-24 18:57:13,954 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:57:13,954 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:57:13,954 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'ij@@', 'sk@@', 'ra@@', 'mp', ',', 'die', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'k@@', 'rist@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:57:13,954 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:57:13,954 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:57:13,954 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat dat de arctische ijs ijskramp , die de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , heeft gekristaten , heeft door 40 procent .
2025-05-24 18:57:13,954 - INFO - joeynmt.training - Example #1
2025-05-24 18:57:13,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:57:13,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:57:13,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oos', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:57:13,955 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:57:13,955 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:57:13,955 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit specifieke probleem omdat het de doos van het ijs van het ijs .
2025-05-24 18:57:13,955 - INFO - joeynmt.training - Example #2
2025-05-24 18:57:13,955 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:57:13,955 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:57:13,955 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'str@@', 'oo@@', 'm@@', 'systeem', '.', '</s>']
2025-05-24 18:57:13,955 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:57:13,955 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:57:13,955 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in een zin , het stroomsysteem .
2025-05-24 18:57:13,955 - INFO - joeynmt.training - Example #3
2025-05-24 18:57:13,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:57:13,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:57:13,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'en@@', 't@@', 'oon', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:57:13,957 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:57:13,957 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:57:13,957 - INFO - joeynmt.training - 	Hypothesis: Het expentoon en contracten in zomer .
2025-05-24 18:57:13,957 - INFO - joeynmt.training - Example #4
2025-05-24 18:57:13,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:57:13,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:57:13,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'toon@@', 't', 'dat', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ler', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeurd', 'is', 'gebeurd', '.', '</s>']
2025-05-24 18:57:13,958 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:57:13,958 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:57:13,958 - INFO - joeynmt.training - 	Hypothesis: De volgende dia toont dat ik jullie een rapid sneller zal zijn van wat er gebeurd is gebeurd .
2025-05-24 18:57:19,145 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     1.615372, Batch Acc: 0.585275, Tokens per Sec:    13626, Lr: 0.000300
2025-05-24 18:57:24,406 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     1.680192, Batch Acc: 0.583288, Tokens per Sec:    13780, Lr: 0.000300
2025-05-24 18:57:29,638 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     1.461241, Batch Acc: 0.579594, Tokens per Sec:    13560, Lr: 0.000300
2025-05-24 18:57:34,848 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     1.609257, Batch Acc: 0.581352, Tokens per Sec:    13575, Lr: 0.000300
2025-05-24 18:57:40,121 - INFO - joeynmt.training - Epoch   5, Step:    15500, Batch Loss:     1.694603, Batch Acc: 0.576952, Tokens per Sec:    13517, Lr: 0.000300
2025-05-24 18:57:40,121 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:57:40,121 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:57:48,781 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.98, acc:   0.49, generation: 8.6494[sec], evaluation: 0.0000[sec]
2025-05-24 18:57:48,782 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:57:48,860 - INFO - joeynmt.helpers - delete models/bpe_4k/13000.ckpt
2025-05-24 18:57:48,866 - INFO - joeynmt.training - Example #0
2025-05-24 18:57:48,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:57:48,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:57:48,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 18:57:48,866 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:57:48,866 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:57:48,866 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs , die de arctische ijp , die voor de laatste drie miljoen jaar de grootte van de lagere 48 staten , heeft door 40 procent .
2025-05-24 18:57:48,866 - INFO - joeynmt.training - Example #1
2025-05-24 18:57:48,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:57:48,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:57:48,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'zo', '&apos;', 'n', 'beetje', 'de', 'bed@@', 're@@', 'ig@@', 'd', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:57:48,866 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:57:48,867 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:57:48,867 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit probleem omdat het zo &apos; n beetje de bedreigd van het ijs .
2025-05-24 18:57:48,867 - INFO - joeynmt.training - Example #2
2025-05-24 18:57:48,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:57:48,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:57:48,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'ca@@', 'p', 'is', ',', 'in', 'een', 'zin', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:57:48,867 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:57:48,867 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:57:48,867 - INFO - joeynmt.training - 	Hypothesis: Het kunstcap is , in een zin , in een zin , het hart van het wereldwijd klimaatsysteem .
2025-05-24 18:57:48,867 - INFO - joeynmt.training - Example #3
2025-05-24 18:57:48,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:57:48,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:57:48,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'wordt', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:57:48,868 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:57:48,868 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:57:48,868 - INFO - joeynmt.training - 	Hypothesis: Het wordt in winter en contracten in zomer .
2025-05-24 18:57:48,868 - INFO - joeynmt.training - Example #4
2025-05-24 18:57:48,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:57:48,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:57:48,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 're@@', 'de', 'van', 'wat', 'er', 'gebeurd', 'is', 'van', 'wat', 'er', 'gebeurd', 'is', '.', '</s>']
2025-05-24 18:57:48,868 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:57:48,868 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:57:48,868 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat een rapid rede van wat er gebeurd is van wat er gebeurd is .
2025-05-24 18:57:54,049 - INFO - joeynmt.training - Epoch   5, Step:    15600, Batch Loss:     1.534318, Batch Acc: 0.579369, Tokens per Sec:    13339, Lr: 0.000300
2025-05-24 18:57:59,495 - INFO - joeynmt.training - Epoch   5, Step:    15700, Batch Loss:     1.486323, Batch Acc: 0.577962, Tokens per Sec:    13187, Lr: 0.000300
2025-05-24 18:58:04,724 - INFO - joeynmt.training - Epoch   5, Step:    15800, Batch Loss:     1.540997, Batch Acc: 0.577330, Tokens per Sec:    13609, Lr: 0.000300
2025-05-24 18:58:10,092 - INFO - joeynmt.training - Epoch   5, Step:    15900, Batch Loss:     1.440094, Batch Acc: 0.576419, Tokens per Sec:    13374, Lr: 0.000300
2025-05-24 18:58:15,347 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     1.534126, Batch Acc: 0.577650, Tokens per Sec:    13498, Lr: 0.000300
2025-05-24 18:58:15,347 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:58:15,347 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:58:24,670 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.93, acc:   0.49, generation: 9.3080[sec], evaluation: 0.0000[sec]
2025-05-24 18:58:24,670 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:58:24,748 - INFO - joeynmt.helpers - delete models/bpe_4k/13500.ckpt
2025-05-24 18:58:24,754 - INFO - joeynmt.training - Example #0
2025-05-24 18:58:24,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:58:24,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:58:24,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'jaren', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'die', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'ort', '.', '</s>']
2025-05-24 18:58:24,754 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:58:24,754 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:58:24,754 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs , die voor de meeste jaren de grootte van de lager 48 staten , die de grootte van de lager 48 staten , heeft een kort .
2025-05-24 18:58:24,754 - INFO - joeynmt.training - Example #1
2025-05-24 18:58:24,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:58:24,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:58:24,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'du@@', 'w@@', 'heid', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:58:24,755 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:58:24,755 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:58:24,755 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieus van dit probleem omdat het de duwheid van het ijs van het ijs .
2025-05-24 18:58:24,755 - INFO - joeynmt.training - Example #2
2025-05-24 18:58:24,755 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:58:24,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:58:24,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'kun@@', 'st@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'be@@', 'do@@', 'eling', 'van', 'het', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 18:58:24,756 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:58:24,756 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:58:24,756 - INFO - joeynmt.training - 	Hypothesis: De kunstcap is , in zekere zin , het bedoeling van het klimaat .
2025-05-24 18:58:24,756 - INFO - joeynmt.training - Example #3
2025-05-24 18:58:24,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:58:24,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:58:24,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'be@@', 'st@@', 'aan@@', 'de', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:58:24,756 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:58:24,756 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:58:24,757 - INFO - joeynmt.training - 	Hypothesis: Het bestaande in winter en contracten in zomer .
2025-05-24 18:58:24,757 - INFO - joeynmt.training - Example #4
2025-05-24 18:58:24,757 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:58:24,757 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:58:24,757 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'zal', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'le', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.', '</s>']
2025-05-24 18:58:24,757 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:58:24,757 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:58:24,757 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie zal een rapid snelle van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:58:29,848 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     1.541128, Batch Acc: 0.581315, Tokens per Sec:    14090, Lr: 0.000300
2025-05-24 18:58:35,210 - INFO - joeynmt.training - Epoch   5, Step:    16200, Batch Loss:     1.356534, Batch Acc: 0.581079, Tokens per Sec:    13491, Lr: 0.000300
2025-05-24 18:58:40,379 - INFO - joeynmt.training - Epoch   5, Step:    16300, Batch Loss:     1.523638, Batch Acc: 0.576423, Tokens per Sec:    14074, Lr: 0.000300
2025-05-24 18:58:45,606 - INFO - joeynmt.training - Epoch   5, Step:    16400, Batch Loss:     1.424513, Batch Acc: 0.579812, Tokens per Sec:    14076, Lr: 0.000300
2025-05-24 18:58:50,946 - INFO - joeynmt.training - Epoch   5, Step:    16500, Batch Loss:     1.367221, Batch Acc: 0.582419, Tokens per Sec:    13709, Lr: 0.000300
2025-05-24 18:58:50,946 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:58:50,946 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:58:59,370 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.93, acc:   0.49, generation: 8.4130[sec], evaluation: 0.0000[sec]
2025-05-24 18:58:59,450 - INFO - joeynmt.helpers - delete models/bpe_4k/14500.ckpt
2025-05-24 18:58:59,455 - INFO - joeynmt.training - Example #0
2025-05-24 18:58:59,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:58:59,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:58:59,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'is', 'een', 'k@@', 'loo@@', 'f', 'van', '40', 'procent', '.', '</s>']
2025-05-24 18:58:59,456 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:58:59,456 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:58:59,456 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s die de arctische ijs , die de meeste afgelopen drie miljoen jaar de grootte van de grootte van de lagere 48 staten , is een kloof van 40 procent .
2025-05-24 18:58:59,456 - INFO - joeynmt.training - Example #1
2025-05-24 18:58:59,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:58:59,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:58:59,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ick@@', 'heid', 'niet', 'de', 'du@@', 'w@@', 'heid', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 18:58:59,456 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:58:59,456 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:58:59,456 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieus van dit probleem omdat het de dickheid niet de duwheid van het ijs .
2025-05-24 18:58:59,456 - INFO - joeynmt.training - Example #2
2025-05-24 18:58:59,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:58:59,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:58:59,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 'ij@@', 'ij@@', 'p', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:58:59,457 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:58:59,457 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:58:59,457 - INFO - joeynmt.training - 	Hypothesis: De arctische ijpijijp is , in zekere zin , het hart van het wereldwijd klimaatsysteem .
2025-05-24 18:58:59,457 - INFO - joeynmt.training - Example #3
2025-05-24 18:58:59,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:58:59,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:58:59,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'b@@', 'bel@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:58:59,457 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:58:59,457 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:58:59,457 - INFO - joeynmt.training - 	Hypothesis: Het dubbelt in winter en contracten in zomer .
2025-05-24 18:58:59,457 - INFO - joeynmt.training - Example #4
2025-05-24 18:58:59,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:58:59,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:58:59,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', 'aan@@', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 18:58:59,458 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:58:59,458 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:58:59,458 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid fastaande afgelopen 25 jaar .
2025-05-24 18:59:04,736 - INFO - joeynmt.training - Epoch   5, Step:    16600, Batch Loss:     1.590526, Batch Acc: 0.580407, Tokens per Sec:    13430, Lr: 0.000300
2025-05-24 18:59:10,227 - INFO - joeynmt.training - Epoch   5, Step:    16700, Batch Loss:     1.659897, Batch Acc: 0.579376, Tokens per Sec:    13163, Lr: 0.000300
2025-05-24 18:59:15,382 - INFO - joeynmt.training - Epoch   5, Step:    16800, Batch Loss:     1.355864, Batch Acc: 0.579339, Tokens per Sec:    13989, Lr: 0.000300
2025-05-24 18:59:20,792 - INFO - joeynmt.training - Epoch   5, Step:    16900, Batch Loss:     1.551099, Batch Acc: 0.579804, Tokens per Sec:    13327, Lr: 0.000300
2025-05-24 18:59:25,928 - INFO - joeynmt.training - Epoch   5, Step:    17000, Batch Loss:     1.480895, Batch Acc: 0.579608, Tokens per Sec:    13638, Lr: 0.000300
2025-05-24 18:59:25,928 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 18:59:25,928 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:59:35,151 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.83, acc:   0.49, generation: 9.2130[sec], evaluation: 0.0000[sec]
2025-05-24 18:59:35,152 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 18:59:35,237 - INFO - joeynmt.helpers - delete models/bpe_4k/14000.ckpt
2025-05-24 18:59:35,242 - INFO - joeynmt.training - Example #0
2025-05-24 18:59:35,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 18:59:35,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 18:59:35,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'loo@@', 'f', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'ro@@', 'l', '.', '</s>']
2025-05-24 18:59:35,243 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 18:59:35,243 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 18:59:35,243 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs , die de arctische ijs , die de laatste drie miljoen jaar de grootte van de lager 48 staten , heeft een kloof van de lager 48 staten , heeft een krol .
2025-05-24 18:59:35,243 - INFO - joeynmt.training - Example #1
2025-05-24 18:59:35,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 18:59:35,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 18:59:35,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ood@@', 'heid', 'van', 'de', 'ij@@', 's', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 18:59:35,244 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 18:59:35,244 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 18:59:35,244 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit probleem omdat het de doodheid van de ijs van de ijs .
2025-05-24 18:59:35,244 - INFO - joeynmt.training - Example #2
2025-05-24 18:59:35,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 18:59:35,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 18:59:35,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'al@@', 'de', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 18:59:35,244 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 18:59:35,244 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 18:59:35,245 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het betaalde hart van het wereldwijde klimaatsysteem .
2025-05-24 18:59:35,245 - INFO - joeynmt.training - Example #3
2025-05-24 18:59:35,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 18:59:35,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 18:59:35,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'anden', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 18:59:35,245 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 18:59:35,245 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 18:59:35,245 - INFO - joeynmt.training - 	Hypothesis: Het expanden in winter en contracten in zomer .
2025-05-24 18:59:35,245 - INFO - joeynmt.training - Example #4
2025-05-24 18:59:35,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 18:59:35,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 18:59:35,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 18:59:35,246 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 18:59:35,246 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 18:59:35,246 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid in de afgelopen 25 jaar is .
2025-05-24 18:59:40,586 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     1.573642, Batch Acc: 0.582758, Tokens per Sec:    13492, Lr: 0.000300
2025-05-24 18:59:45,918 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.382163, Batch Acc: 0.585033, Tokens per Sec:    13043, Lr: 0.000300
2025-05-24 18:59:51,204 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     1.642149, Batch Acc: 0.577257, Tokens per Sec:    13517, Lr: 0.000300
2025-05-24 18:59:56,629 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.477639, Batch Acc: 0.580583, Tokens per Sec:    13314, Lr: 0.000300
2025-05-24 19:00:01,843 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.790195, Batch Acc: 0.579041, Tokens per Sec:    13510, Lr: 0.000300
2025-05-24 19:00:01,844 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:00:01,844 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:00:11,814 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.84, acc:   0.49, generation: 9.9580[sec], evaluation: 0.0000[sec]
2025-05-24 19:00:11,894 - INFO - joeynmt.helpers - delete models/bpe_4k/15000.ckpt
2025-05-24 19:00:11,899 - INFO - joeynmt.training - Example #0
2025-05-24 19:00:11,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:00:11,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:00:11,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zien', ',', 'zodat', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'op@@', 'name', 'die', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'sla@@', 'gen', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:00:11,900 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:00:11,900 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:00:11,900 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zien , zodat de meeste drie miljoen jaar de opname die de meeste drie miljoen jaar de grootte van de lager 48 staten , heeft geslagen door 40 procent .
2025-05-24 19:00:11,900 - INFO - joeynmt.training - Example #1
2025-05-24 19:00:11,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:00:11,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:00:11,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'ood@@', 'heid', 'van', 'de', 'ij@@', 's', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 19:00:11,901 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:00:11,901 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:00:11,901 - INFO - joeynmt.training - 	Hypothesis: Maar deze begreep de serieoudheid van dit probleem omdat het de doodheid van de ijs van de ijs .
2025-05-24 19:00:11,901 - INFO - joeynmt.training - Example #2
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:00:11,902 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:00:11,902 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:00:11,902 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in zekere zin het hart van het wereldwijd klimaatsysteem .
2025-05-24 19:00:11,902 - INFO - joeynmt.training - Example #3
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'in@@', 'en', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:00:11,902 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:00:11,902 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:00:11,902 - INFO - joeynmt.training - 	Hypothesis: Het duinen in winter en contracten in zomer .
2025-05-24 19:00:11,902 - INFO - joeynmt.training - Example #4
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:00:11,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeurd', 'is', 'gebeurd', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 19:00:11,903 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:00:11,903 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:00:11,903 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat een rapid zal zijn van wat er gebeurd is gebeurd in de laatste 25 jaar .
2025-05-24 19:00:17,411 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.636862, Batch Acc: 0.580651, Tokens per Sec:    12872, Lr: 0.000300
2025-05-24 19:00:22,604 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.699110, Batch Acc: 0.582116, Tokens per Sec:    13712, Lr: 0.000300
2025-05-24 19:00:27,883 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.711187, Batch Acc: 0.577632, Tokens per Sec:    13206, Lr: 0.000300
2025-05-24 19:00:33,153 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.263047, Batch Acc: 0.588094, Tokens per Sec:    13542, Lr: 0.000300
2025-05-24 19:00:37,597 - INFO - joeynmt.training - Epoch   5: total training loss 5499.26
2025-05-24 19:00:37,597 - INFO - joeynmt.training - EPOCH 6
2025-05-24 19:00:38,575 - INFO - joeynmt.training - Epoch   6, Step:    18000, Batch Loss:     1.558750, Batch Acc: 0.607942, Tokens per Sec:    11735, Lr: 0.000300
2025-05-24 19:00:38,575 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:00:38,575 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:00:47,412 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.79, acc:   0.49, generation: 8.8258[sec], evaluation: 0.0000[sec]
2025-05-24 19:00:47,413 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:00:47,494 - INFO - joeynmt.helpers - delete models/bpe_4k/15500.ckpt
2025-05-24 19:00:47,499 - INFO - joeynmt.training - Example #0
2025-05-24 19:00:47,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:00:47,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:00:47,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', ',', 'die', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'loo@@', 'f', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'loo@@', 'f', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:00:47,500 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:00:47,500 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:00:47,500 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische cap , die de meeste drie miljoen jaar de grootte van de lager 48 staten , heeft een kloof van de lager 48 staten , heeft een kloof door 40 procent .
2025-05-24 19:00:47,500 - INFO - joeynmt.training - Example #1
2025-05-24 19:00:47,500 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:00:47,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:00:47,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'd@@', 'oos', 'van', 'het', 'ij@@', 's', 'is', '.', '</s>']
2025-05-24 19:00:47,500 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:00:47,500 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:00:47,501 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit specifieke probleem omdat het de doos van het ijs is .
2025-05-24 19:00:47,501 - INFO - joeynmt.training - Example #2
2025-05-24 19:00:47,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:00:47,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:00:47,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'v@@', 'atten', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:00:47,501 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:00:47,501 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:00:47,501 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het bevatten hart van het globale klimaatsysteem .
2025-05-24 19:00:47,501 - INFO - joeynmt.training - Example #3
2025-05-24 19:00:47,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:00:47,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:00:47,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'b@@', 'bel@@', 'en', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:00:47,502 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:00:47,502 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:00:47,502 - INFO - joeynmt.training - 	Hypothesis: Het dubbelen in de winter en contracten in zomer .
2025-05-24 19:00:47,502 - INFO - joeynmt.training - Example #4
2025-05-24 19:00:47,502 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:00:47,502 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:00:47,502 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'een', 'snel', 'snel@@', 'ler', 'zal', 'voor@@', 'uit@@', 'gaan', 'van', 'wat', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:00:47,502 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:00:47,502 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:00:47,502 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je een snel sneller zal vooruitgaan van wat de afgelopen 25 jaar is .
2025-05-24 19:00:52,823 - INFO - joeynmt.training - Epoch   6, Step:    18100, Batch Loss:     1.335818, Batch Acc: 0.610162, Tokens per Sec:    13374, Lr: 0.000300
2025-05-24 19:00:58,071 - INFO - joeynmt.training - Epoch   6, Step:    18200, Batch Loss:     1.376266, Batch Acc: 0.603991, Tokens per Sec:    13559, Lr: 0.000300
2025-05-24 19:01:03,489 - INFO - joeynmt.training - Epoch   6, Step:    18300, Batch Loss:     1.538230, Batch Acc: 0.597350, Tokens per Sec:    13250, Lr: 0.000300
2025-05-24 19:01:08,754 - INFO - joeynmt.training - Epoch   6, Step:    18400, Batch Loss:     1.549253, Batch Acc: 0.600605, Tokens per Sec:    13626, Lr: 0.000300
2025-05-24 19:01:14,057 - INFO - joeynmt.training - Epoch   6, Step:    18500, Batch Loss:     1.325880, Batch Acc: 0.604290, Tokens per Sec:    13553, Lr: 0.000300
2025-05-24 19:01:14,058 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:01:14,058 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:01:22,451 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.77, acc:   0.49, generation: 8.3826[sec], evaluation: 0.0000[sec]
2025-05-24 19:01:22,451 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:01:22,535 - INFO - joeynmt.helpers - delete models/bpe_4k/16500.ckpt
2025-05-24 19:01:22,540 - INFO - joeynmt.training - Example #0
2025-05-24 19:01:22,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:01:22,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:01:22,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'ge@@', 'de@@', 'mon@@', 'str@@', 'eren', ',', 'die', 'de', 'ar@@', 'c@@', 't@@', 'sen', ',', 'die', 'de', 'afgelopen', '3', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'een', 'k@@', 'loo@@', 'f', 'door', '40', '%', '.', '</s>']
2025-05-24 19:01:22,541 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:01:22,541 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:01:22,541 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s gedemonstreren , die de arctsen , die de afgelopen 3 miljoen jaar de grootte van de lager 48 staten , is een kloof door 40 % .
2025-05-24 19:01:22,541 - INFO - joeynmt.training - Example #1
2025-05-24 19:01:22,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:01:22,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:01:22,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ie@@', 'ou@@', 'deren', 'van', 'dit', 'specif@@', 'iek', 'probleem', 'omdat', 'het', 'de', 'bed@@', 're@@', 'ig@@', 'ing', 'niet', 'de', 'bed@@', 're@@', 'ig@@', 'd', 'is', '.', '</s>']
2025-05-24 19:01:22,541 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:01:22,541 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:01:22,542 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieieouderen van dit specifiek probleem omdat het de bedreiging niet de bedreigd is .
2025-05-24 19:01:22,542 - INFO - joeynmt.training - Example #2
2025-05-24 19:01:22,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:01:22,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:01:22,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'arti@@', 'kel@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'be@@', 'sla@@', 'at', 'har@@', 't', 'van', 'het', 'mon@@', 'di@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:01:22,542 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:01:22,542 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:01:22,542 - INFO - joeynmt.training - 	Hypothesis: De artikels is , in zekere zin , het beslaat hart van het mondiale klimaat .
2025-05-24 19:01:22,542 - INFO - joeynmt.training - Example #3
2025-05-24 19:01:22,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:01:22,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:01:22,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'w@@', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:01:22,543 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:01:22,543 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:01:22,543 - INFO - joeynmt.training - 	Hypothesis: Het duwwinter en contracten in zomer .
2025-05-24 19:01:22,543 - INFO - joeynmt.training - Example #4
2025-05-24 19:01:22,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:01:22,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:01:22,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laten', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:01:22,544 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:01:22,544 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:01:22,544 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laten zien dat je een rapid in de afgelopen 25 jaar .
2025-05-24 19:01:27,864 - INFO - joeynmt.training - Epoch   6, Step:    18600, Batch Loss:     1.435533, Batch Acc: 0.597773, Tokens per Sec:    13221, Lr: 0.000300
2025-05-24 19:01:33,093 - INFO - joeynmt.training - Epoch   6, Step:    18700, Batch Loss:     1.505826, Batch Acc: 0.598645, Tokens per Sec:    13751, Lr: 0.000300
2025-05-24 19:01:38,417 - INFO - joeynmt.training - Epoch   6, Step:    18800, Batch Loss:     1.312755, Batch Acc: 0.599726, Tokens per Sec:    13302, Lr: 0.000300
2025-05-24 19:01:43,603 - INFO - joeynmt.training - Epoch   6, Step:    18900, Batch Loss:     1.492047, Batch Acc: 0.595851, Tokens per Sec:    13877, Lr: 0.000300
2025-05-24 19:01:48,981 - INFO - joeynmt.training - Epoch   6, Step:    19000, Batch Loss:     1.556465, Batch Acc: 0.592860, Tokens per Sec:    13086, Lr: 0.000300
2025-05-24 19:01:48,982 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:01:48,982 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:01:57,776 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.78, acc:   0.50, generation: 8.7822[sec], evaluation: 0.0000[sec]
2025-05-24 19:01:57,853 - INFO - joeynmt.helpers - delete models/bpe_4k/16000.ckpt
2025-05-24 19:01:57,857 - INFO - joeynmt.training - Example #0
2025-05-24 19:01:57,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:01:57,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:01:57,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'kun@@', 'st@@', 'st@@', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:01:57,858 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:01:57,858 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:01:57,858 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs die de kunststcap , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , heeft een door 40 procent .
2025-05-24 19:01:57,858 - INFO - joeynmt.training - Example #1
2025-05-24 19:01:57,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:01:57,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:01:57,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'zo', 'niet', 'de', 'bed@@', 're@@', 'ig@@', 'ing', 'van', 'de', 'ij@@', 's', '.', '</s>']
2025-05-24 19:01:57,858 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:01:57,858 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:01:57,859 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieus van dit specifieke probleem omdat het zo niet de bedreiging van de ijs .
2025-05-24 19:01:57,859 - INFO - joeynmt.training - Example #2
2025-05-24 19:01:57,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:01:57,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:01:57,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'st@@', 'ca@@', 'p', 'is', ',', 'in', 'een', 'gevoel', ',', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:01:57,860 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:01:57,860 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:01:57,860 - INFO - joeynmt.training - 	Hypothesis: Het kunststcap is , in een gevoel , het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:01:57,860 - INFO - joeynmt.training - Example #3
2025-05-24 19:01:57,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:01:57,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:01:57,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'bre@@', 'i@@', 'dt', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:01:57,860 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:01:57,860 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:01:57,860 - INFO - joeynmt.training - 	Hypothesis: Het breidt in de winter en contracten in zomer .
2025-05-24 19:01:57,860 - INFO - joeynmt.training - Example #4
2025-05-24 19:01:57,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:01:57,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:01:57,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'laatste', '25', 'jaar', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:01:57,861 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:01:57,861 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:01:57,861 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat een rapid in de laatste 25 jaar is gebeurd .
2025-05-24 19:02:03,105 - INFO - joeynmt.training - Epoch   6, Step:    19100, Batch Loss:     1.479173, Batch Acc: 0.593275, Tokens per Sec:    13139, Lr: 0.000300
2025-05-24 19:02:08,378 - INFO - joeynmt.training - Epoch   6, Step:    19200, Batch Loss:     1.352001, Batch Acc: 0.594253, Tokens per Sec:    13433, Lr: 0.000300
2025-05-24 19:02:13,794 - INFO - joeynmt.training - Epoch   6, Step:    19300, Batch Loss:     1.377099, Batch Acc: 0.599823, Tokens per Sec:    13543, Lr: 0.000300
2025-05-24 19:02:19,044 - INFO - joeynmt.training - Epoch   6, Step:    19400, Batch Loss:     1.571336, Batch Acc: 0.594206, Tokens per Sec:    13565, Lr: 0.000300
2025-05-24 19:02:24,323 - INFO - joeynmt.training - Epoch   6, Step:    19500, Batch Loss:     1.392142, Batch Acc: 0.592933, Tokens per Sec:    13992, Lr: 0.000300
2025-05-24 19:02:24,323 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:02:24,323 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:02:33,106 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.76, acc:   0.49, generation: 8.7716[sec], evaluation: 0.0000[sec]
2025-05-24 19:02:33,106 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:02:33,185 - INFO - joeynmt.helpers - delete models/bpe_4k/17500.ckpt
2025-05-24 19:02:33,190 - INFO - joeynmt.training - Example #0
2025-05-24 19:02:33,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:02:33,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:02:33,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'dat', 'ik', 'deze', 'twee', 'dia', 'toon@@', 'de', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'een', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:02:33,191 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:02:33,191 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:02:33,191 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar dat ik deze twee dia toonde zodat de arctische cap , die voor de meeste laatste drie miljoen jaar , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is een door 40 procent .
2025-05-24 19:02:33,191 - INFO - joeynmt.training - Example #1
2025-05-24 19:02:33,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:02:33,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:02:33,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ie@@', 'ou@@', 'der@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'zo', 'de', 'd@@', 'oos', 'van', 'het', 'ij@@', 's', 'is', '.', '</s>']
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieieouderheid van dit specifieke probleem omdat het zo de doos van het ijs is .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - Example #2
2025-05-24 19:02:33,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:02:33,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:02:33,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'in', 'het', 'be@@', 'sla@@', 'gen', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , in het beslagen hart van het wereldwijde klimaat .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - Example #3
2025-05-24 19:02:33,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:02:33,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:02:33,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'p@@', 'pen', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:02:33,192 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:02:33,193 - INFO - joeynmt.training - 	Hypothesis: Het duippen in winter en contracten in de zomer .
2025-05-24 19:02:33,193 - INFO - joeynmt.training - Example #4
2025-05-24 19:02:33,193 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:02:33,193 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:02:33,193 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'je', 'een', 'snel', 'snel@@', 'ler', 'zal', 'zijn', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:02:33,194 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:02:33,194 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:02:33,194 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat je een snel sneller zal zijn van wat er de laatste 25 jaar is .
2025-05-24 19:02:38,311 - INFO - joeynmt.training - Epoch   6, Step:    19600, Batch Loss:     1.404487, Batch Acc: 0.594652, Tokens per Sec:    13529, Lr: 0.000300
2025-05-24 19:02:43,512 - INFO - joeynmt.training - Epoch   6, Step:    19700, Batch Loss:     1.403426, Batch Acc: 0.594090, Tokens per Sec:    13749, Lr: 0.000300
2025-05-24 19:02:48,809 - INFO - joeynmt.training - Epoch   6, Step:    19800, Batch Loss:     1.413761, Batch Acc: 0.591375, Tokens per Sec:    13710, Lr: 0.000300
2025-05-24 19:02:54,127 - INFO - joeynmt.training - Epoch   6, Step:    19900, Batch Loss:     1.401989, Batch Acc: 0.594525, Tokens per Sec:    13480, Lr: 0.000300
2025-05-24 19:02:59,409 - INFO - joeynmt.training - Epoch   6, Step:    20000, Batch Loss:     1.411256, Batch Acc: 0.593612, Tokens per Sec:    13626, Lr: 0.000300
2025-05-24 19:02:59,409 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:02:59,409 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:03:08,124 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.67, acc:   0.50, generation: 8.7029[sec], evaluation: 0.0000[sec]
2025-05-24 19:03:08,124 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:03:08,202 - INFO - joeynmt.helpers - delete models/bpe_4k/17000.ckpt
2025-05-24 19:03:08,208 - INFO - joeynmt.training - Example #0
2025-05-24 19:03:08,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:03:08,208 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:03:08,208 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'met', '40', 'procent', '.', '</s>']
2025-05-24 19:03:08,208 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:03:08,209 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:03:08,209 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de arctische ijs die de arctische ijs , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is met 40 procent .
2025-05-24 19:03:08,209 - INFO - joeynmt.training - Example #1
2025-05-24 19:03:08,209 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:03:08,209 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:03:08,209 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ree@@', 'p', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'd@@', 'w@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 19:03:08,209 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:03:08,209 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:03:08,209 - INFO - joeynmt.training - 	Hypothesis: Maar dit begreep de serieoudheid van dit probleem omdat het dwheid van het ijs niet laten zien .
2025-05-24 19:03:08,209 - INFO - joeynmt.training - Example #2
2025-05-24 19:03:08,210 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:03:08,210 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:03:08,210 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 'p', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'be@@', 'ste@@', 'mm@@', 'ing', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:03:08,210 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:03:08,210 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:03:08,210 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijp is , in zekere zin , het bestemming van het wereldwijd klimaat .
2025-05-24 19:03:08,210 - INFO - joeynmt.training - Example #3
2025-05-24 19:03:08,210 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:03:08,210 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:03:08,210 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'kt', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:03:08,210 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:03:08,210 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:03:08,210 - INFO - joeynmt.training - 	Hypothesis: Het duikt in winter en contracten in de zomer .
2025-05-24 19:03:08,210 - INFO - joeynmt.training - Example #4
2025-05-24 19:03:08,212 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:03:08,212 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:03:08,212 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:03:08,212 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:03:08,212 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:03:08,212 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid in de afgelopen 25 jaar .
2025-05-24 19:03:13,446 - INFO - joeynmt.training - Epoch   6, Step:    20100, Batch Loss:     1.393296, Batch Acc: 0.597676, Tokens per Sec:    13454, Lr: 0.000300
2025-05-24 19:03:18,889 - INFO - joeynmt.training - Epoch   6, Step:    20200, Batch Loss:     1.477822, Batch Acc: 0.592104, Tokens per Sec:    13207, Lr: 0.000300
2025-05-24 19:03:24,077 - INFO - joeynmt.training - Epoch   6, Step:    20300, Batch Loss:     1.570579, Batch Acc: 0.597768, Tokens per Sec:    13785, Lr: 0.000300
2025-05-24 19:03:29,345 - INFO - joeynmt.training - Epoch   6, Step:    20400, Batch Loss:     1.323436, Batch Acc: 0.593063, Tokens per Sec:    13538, Lr: 0.000300
2025-05-24 19:03:34,668 - INFO - joeynmt.training - Epoch   6, Step:    20500, Batch Loss:     1.663630, Batch Acc: 0.598007, Tokens per Sec:    13572, Lr: 0.000300
2025-05-24 19:03:34,668 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:03:34,669 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:03:43,915 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.66, acc:   0.50, generation: 9.2338[sec], evaluation: 0.0000[sec]
2025-05-24 19:03:43,916 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:03:43,993 - INFO - joeynmt.helpers - delete models/bpe_4k/18000.ckpt
2025-05-24 19:03:43,998 - INFO - joeynmt.training - Example #0
2025-05-24 19:03:43,998 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:03:43,999 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:03:43,999 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'van', 'de', 'la@@', 'ger', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'k@@', 'raa@@', 'kt', ',', 'heeft', 'een', 'k@@', 'ro@@', 'ken', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:03:43,999 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:03:43,999 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:03:43,999 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zodat de arctische ijs , die voor de meeste drie miljoen jaar van de lager van de lager 48 staten , heeft gekraakt , heeft een kroken van 40 procent .
2025-05-24 19:03:43,999 - INFO - joeynmt.training - Example #1
2025-05-24 19:03:43,999 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:03:43,999 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:03:43,999 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'd@@', 'os@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieoudheid van dit probleem omdat het de dosheid van het ijs niet laten zien .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - Example #2
2025-05-24 19:03:44,000 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:03:44,000 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:03:44,000 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'v@@', 'at', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het bevat hart van het wereldwijde klimaatsysteem .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - Example #3
2025-05-24 19:03:44,000 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:03:44,000 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:03:44,000 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'du@@', 'b@@', 'bel@@', 'en', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:03:44,000 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:03:44,002 - INFO - joeynmt.training - 	Hypothesis: Het verdubbelen in de zomer en contracten in de zomer .
2025-05-24 19:03:44,002 - INFO - joeynmt.training - Example #4
2025-05-24 19:03:44,002 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:03:44,002 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:03:44,002 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', ',', 'ik', 'laat', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ler', 'snel@@', 'ler', 'zijn', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'gebeurde', '.', '</s>']
2025-05-24 19:03:44,002 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:03:44,002 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:03:44,002 - INFO - joeynmt.training - 	Hypothesis: De volgende dia , ik laat jullie een rapid sneller sneller zijn van wat er de laatste 25 jaar gebeurde .
2025-05-24 19:03:49,195 - INFO - joeynmt.training - Epoch   6, Step:    20600, Batch Loss:     1.636195, Batch Acc: 0.595843, Tokens per Sec:    13645, Lr: 0.000300
2025-05-24 19:03:54,441 - INFO - joeynmt.training - Epoch   6, Step:    20700, Batch Loss:     1.396384, Batch Acc: 0.596725, Tokens per Sec:    13471, Lr: 0.000300
2025-05-24 19:03:59,635 - INFO - joeynmt.training - Epoch   6, Step:    20800, Batch Loss:     1.539945, Batch Acc: 0.591298, Tokens per Sec:    13824, Lr: 0.000300
2025-05-24 19:04:04,885 - INFO - joeynmt.training - Epoch   6, Step:    20900, Batch Loss:     1.388434, Batch Acc: 0.592394, Tokens per Sec:    13466, Lr: 0.000300
2025-05-24 19:04:10,055 - INFO - joeynmt.training - Epoch   6, Step:    21000, Batch Loss:     1.419947, Batch Acc: 0.595968, Tokens per Sec:    13845, Lr: 0.000300
2025-05-24 19:04:10,055 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:04:10,055 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:04:19,092 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.65, acc:   0.50, generation: 9.0274[sec], evaluation: 0.0000[sec]
2025-05-24 19:04:19,092 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:04:19,169 - INFO - joeynmt.helpers - delete models/bpe_4k/19000.ckpt
2025-05-24 19:04:19,175 - INFO - joeynmt.training - Example #0
2025-05-24 19:04:19,175 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:04:19,175 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:04:19,175 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'ge@@', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'k@@', 'loo@@', 'f', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:04:19,175 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:04:19,175 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:04:19,175 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s gedemonstreren dat de arctische ijs , die voor de laatste drie miljoen jaar de grootte van de lagere 48 staten , heeft gekloof door 40 procent .
2025-05-24 19:04:19,175 - INFO - joeynmt.training - Example #1
2025-05-24 19:04:19,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:04:19,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:04:19,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'ke', 'ge@@', 'tu@@', 'ig@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 19:04:19,176 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:04:19,176 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:04:19,176 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieus van dit probleem omdat het de dikke getuigheid van het ijs niet .
2025-05-24 19:04:19,176 - INFO - joeynmt.training - Example #2
2025-05-24 19:04:19,176 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:04:19,176 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:04:19,176 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'arti@@', 'kel', 'is', ',', 'in', 'een', 'zin', ',', 'in', 'een', 'zin', ',', 'het', 'har@@', 't', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:04:19,177 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:04:19,177 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:04:19,177 - INFO - joeynmt.training - 	Hypothesis: Het artikel is , in een zin , in een zin , het hart hart van het globale klimaatsysteem .
2025-05-24 19:04:19,177 - INFO - joeynmt.training - Example #3
2025-05-24 19:04:19,177 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:04:19,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:04:19,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'w@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:04:19,177 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:04:19,177 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:04:19,177 - INFO - joeynmt.training - 	Hypothesis: Het duwt in winter en contracten in de zomer .
2025-05-24 19:04:19,177 - INFO - joeynmt.training - Example #4
2025-05-24 19:04:19,177 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:04:19,177 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:04:19,177 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'een', 'snel@@', 'le', 'snel@@', 'ler', 'in', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:04:19,178 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:04:19,178 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:04:19,178 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat een snelle sneller in de afgelopen 25 jaar is .
2025-05-24 19:04:24,560 - INFO - joeynmt.training - Epoch   6, Step:    21100, Batch Loss:     1.591635, Batch Acc: 0.589698, Tokens per Sec:    12780, Lr: 0.000300
2025-05-24 19:04:29,749 - INFO - joeynmt.training - Epoch   6, Step:    21200, Batch Loss:     1.616595, Batch Acc: 0.585936, Tokens per Sec:    13154, Lr: 0.000300
2025-05-24 19:04:35,043 - INFO - joeynmt.training - Epoch   6, Step:    21300, Batch Loss:     1.685588, Batch Acc: 0.597745, Tokens per Sec:    13973, Lr: 0.000300
2025-05-24 19:04:40,348 - INFO - joeynmt.training - Epoch   6, Step:    21400, Batch Loss:     1.602185, Batch Acc: 0.590453, Tokens per Sec:    13034, Lr: 0.000300
2025-05-24 19:04:45,584 - INFO - joeynmt.training - Epoch   6, Step:    21500, Batch Loss:     1.604840, Batch Acc: 0.590429, Tokens per Sec:    14099, Lr: 0.000300
2025-05-24 19:04:45,584 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:04:45,584 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:04:54,831 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.58, acc:   0.50, generation: 9.2354[sec], evaluation: 0.0000[sec]
2025-05-24 19:04:54,831 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:04:54,914 - INFO - joeynmt.helpers - delete models/bpe_4k/18500.ckpt
2025-05-24 19:04:54,920 - INFO - joeynmt.training - Example #0
2025-05-24 19:04:54,920 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:04:54,920 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:04:54,920 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zo', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'grot@@', 'en@@', 'de@@', 'els', ',', 'die', 'voor', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'is', 'een', 'k@@', 'loo@@', 'f', 'van', '40', '%', '.', '</s>']
2025-05-24 19:04:54,921 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:04:54,921 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:04:54,921 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zo demonstreren dat de arctische grotendeels , die voor de meeste drie miljoen jaar de grootte van de lagere 48 staten , is een kloof van 40 % .
2025-05-24 19:04:54,921 - INFO - joeynmt.training - Example #1
2025-05-24 19:04:54,921 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:04:54,921 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:04:54,921 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'ste@@', 'e@@', 'k@@', 'jes', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'cht@@', 'the@@', 'id', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:04:54,922 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:04:54,922 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:04:54,922 - INFO - joeynmt.training - 	Hypothesis: Maar deze ondersteekjes van dit probleem omdat het de dichttheid van het ijs niet laat zien .
2025-05-24 19:04:54,922 - INFO - joeynmt.training - Example #2
2025-05-24 19:04:54,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:04:54,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:04:54,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'be@@', 'v@@', 'at', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'systeem', '.', '</s>']
2025-05-24 19:04:54,922 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:04:54,922 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:04:54,923 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in zekere zin , het bevat hart van het wereldwijd systeem .
2025-05-24 19:04:54,923 - INFO - joeynmt.training - Example #3
2025-05-24 19:04:54,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:04:54,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:04:54,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'du@@', 'w@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:04:54,923 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:04:54,923 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:04:54,923 - INFO - joeynmt.training - 	Hypothesis: Het verduwt in winter en contracten in zomer .
2025-05-24 19:04:54,923 - INFO - joeynmt.training - Example #4
2025-05-24 19:04:54,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:04:54,924 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:04:54,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'le', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:04:54,924 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:04:54,924 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:04:54,924 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat zien dat een rapid snelle van wat er de laatste 25 jaar is .
2025-05-24 19:04:59,230 - INFO - joeynmt.training - Epoch   6: total training loss 5277.91
2025-05-24 19:04:59,230 - INFO - joeynmt.training - EPOCH 7
2025-05-24 19:05:00,377 - INFO - joeynmt.training - Epoch   7, Step:    21600, Batch Loss:     1.372991, Batch Acc: 0.611745, Tokens per Sec:    13001, Lr: 0.000300
2025-05-24 19:05:05,759 - INFO - joeynmt.training - Epoch   7, Step:    21700, Batch Loss:     1.419545, Batch Acc: 0.618666, Tokens per Sec:    13621, Lr: 0.000300
2025-05-24 19:05:11,084 - INFO - joeynmt.training - Epoch   7, Step:    21800, Batch Loss:     1.277893, Batch Acc: 0.619582, Tokens per Sec:    13553, Lr: 0.000300
2025-05-24 19:05:16,348 - INFO - joeynmt.training - Epoch   7, Step:    21900, Batch Loss:     1.277188, Batch Acc: 0.614579, Tokens per Sec:    13608, Lr: 0.000300
2025-05-24 19:05:21,586 - INFO - joeynmt.training - Epoch   7, Step:    22000, Batch Loss:     1.301077, Batch Acc: 0.616515, Tokens per Sec:    13242, Lr: 0.000300
2025-05-24 19:05:21,586 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:05:21,586 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:05:30,853 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.64, acc:   0.50, generation: 9.2567[sec], evaluation: 0.0000[sec]
2025-05-24 19:05:30,932 - INFO - joeynmt.helpers - delete models/bpe_4k/19500.ckpt
2025-05-24 19:05:30,938 - INFO - joeynmt.training - Example #0
2025-05-24 19:05:30,938 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:05:30,938 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:05:30,938 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'de', 'ik', 'deze', 'twee', 'dia', 'liet', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de@@', 'mon@@', 'str@@', 'eren', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:05:30,938 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:05:30,939 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:05:30,939 - INFO - joeynmt.training - 	Hypothesis: Vorde ik deze twee dia liet zien zodat de arctische ijs die demonstreren , die voor de meeste laatste drie miljoen jaar , die voor de meeste laatste drie miljoen jaar de grootte van de lagere 48 staten , heeft door 40 procent .
2025-05-24 19:05:30,939 - INFO - joeynmt.training - Example #1
2025-05-24 19:05:30,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:05:30,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:05:30,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'e@@', 'k@@', 'su@@', 'ele', 'probleem', 'omdat', 'het', 'de', 'di@@', 'cht@@', 'st@@', 'ent', 'van', 'dit', 'probleem', 'omdat', 'het', 'zo', 'niet', 'zo', '&apos;', 'n', 'ij@@', 's', 'toon@@', 't', '.', '</s>']
2025-05-24 19:05:30,939 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:05:30,939 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:05:30,940 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteeksuele probleem omdat het de dichtstent van dit probleem omdat het zo niet zo &apos; n ijs toont .
2025-05-24 19:05:30,940 - INFO - joeynmt.training - Example #2
2025-05-24 19:05:30,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:05:30,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:05:30,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'sk@@', 'aar@@', 'd@@', 'ca@@', 'p', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:05:30,940 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:05:30,940 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:05:30,940 - INFO - joeynmt.training - 	Hypothesis: De arctische ijskaardcap is in zekere zin het hart van het globale klimaat .
2025-05-24 19:05:30,940 - INFO - joeynmt.training - Example #3
2025-05-24 19:05:30,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:05:30,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:05:30,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verwa@@', 'cht', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:05:30,941 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:05:30,941 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:05:30,941 - INFO - joeynmt.training - 	Hypothesis: Het verwacht in de winter en contracten in zomer .
2025-05-24 19:05:30,941 - INFO - joeynmt.training - Example #4
2025-05-24 19:05:30,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:05:30,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:05:30,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', 'se', 'van', 'wat', 'er', 'gebeurde', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 19:05:30,941 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:05:30,941 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:05:30,941 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat een rapid fastse van wat er gebeurde de laatste 25 jaar .
2025-05-24 19:05:36,307 - INFO - joeynmt.training - Epoch   7, Step:    22100, Batch Loss:     1.455633, Batch Acc: 0.614364, Tokens per Sec:    13338, Lr: 0.000300
2025-05-24 19:05:41,528 - INFO - joeynmt.training - Epoch   7, Step:    22200, Batch Loss:     1.527915, Batch Acc: 0.611818, Tokens per Sec:    13477, Lr: 0.000300
2025-05-24 19:05:46,955 - INFO - joeynmt.training - Epoch   7, Step:    22300, Batch Loss:     1.397207, Batch Acc: 0.610512, Tokens per Sec:    12764, Lr: 0.000300
2025-05-24 19:05:52,172 - INFO - joeynmt.training - Epoch   7, Step:    22400, Batch Loss:     1.504246, Batch Acc: 0.607314, Tokens per Sec:    13331, Lr: 0.000300
2025-05-24 19:05:57,603 - INFO - joeynmt.training - Epoch   7, Step:    22500, Batch Loss:     1.413778, Batch Acc: 0.607340, Tokens per Sec:    13105, Lr: 0.000300
2025-05-24 19:05:57,604 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:05:57,604 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:06:05,782 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.66, acc:   0.50, generation: 8.0803[sec], evaluation: 0.0000[sec]
2025-05-24 19:06:05,865 - INFO - joeynmt.helpers - delete models/bpe_4k/20000.ckpt
2025-05-24 19:06:05,871 - INFO - joeynmt.training - Example #0
2025-05-24 19:06:05,871 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:06:05,871 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:06:05,871 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'ij@@', 'sk@@', 'i@@', 'p@@', 'iek', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:06:05,871 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:06:05,871 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:06:05,871 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs ijskipiek , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is door 40 procent .
2025-05-24 19:06:05,872 - INFO - joeynmt.training - Example #1
2025-05-24 19:06:05,872 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:06:05,872 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:06:05,872 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'ert', 'het', 'ser@@', 'ie@@', 'ou@@', 'der', 'van', 'dit', 'probleem', 'probleem', 'omdat', 'het', 'de', 'di@@', 'ep', 'van', 'de', 'ij@@', 's', 'niet', 'zo', '&apos;', 'n', 'ij@@', 's', 'is', '.', '</s>']
2025-05-24 19:06:05,872 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:06:05,872 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:06:05,872 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteert het serieouder van dit probleem probleem omdat het de diep van de ijs niet zo &apos; n ijs is .
2025-05-24 19:06:05,872 - INFO - joeynmt.training - Example #2
2025-05-24 19:06:05,872 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:06:05,873 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:06:05,873 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'al@@', 'baar', 'har@@', 't', 'van', 'het', 'mon@@', 'di@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:06:05,873 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:06:05,873 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:06:05,873 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in een zin , het betaalbaar hart van het mondiale klimaat .
2025-05-24 19:06:05,873 - INFO - joeynmt.training - Example #3
2025-05-24 19:06:05,873 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:06:05,873 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:06:05,873 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:06:05,874 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:06:05,874 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:06:05,874 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in zomer .
2025-05-24 19:06:05,874 - INFO - joeynmt.training - Example #4
2025-05-24 19:06:05,874 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:06:05,874 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:06:05,874 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'tonen', 'dat', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'wat', 'er', 'gebeurde', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:06:05,874 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:06:05,874 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:06:05,875 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie tonen dat een rapid snelste vooruit wat er gebeurde de afgelopen 25 jaar .
2025-05-24 19:06:11,166 - INFO - joeynmt.training - Epoch   7, Step:    22600, Batch Loss:     1.549925, Batch Acc: 0.609068, Tokens per Sec:    13367, Lr: 0.000300
2025-05-24 19:06:16,409 - INFO - joeynmt.training - Epoch   7, Step:    22700, Batch Loss:     1.490017, Batch Acc: 0.607906, Tokens per Sec:    13637, Lr: 0.000300
2025-05-24 19:06:21,694 - INFO - joeynmt.training - Epoch   7, Step:    22800, Batch Loss:     1.390484, Batch Acc: 0.609826, Tokens per Sec:    13601, Lr: 0.000300
2025-05-24 19:06:26,885 - INFO - joeynmt.training - Epoch   7, Step:    22900, Batch Loss:     1.397691, Batch Acc: 0.606796, Tokens per Sec:    13862, Lr: 0.000300
2025-05-24 19:06:32,207 - INFO - joeynmt.training - Epoch   7, Step:    23000, Batch Loss:     1.360127, Batch Acc: 0.608053, Tokens per Sec:    13312, Lr: 0.000300
2025-05-24 19:06:32,207 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:06:32,207 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:06:41,745 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.61, acc:   0.50, generation: 9.5264[sec], evaluation: 0.0000[sec]
2025-05-24 19:06:41,823 - INFO - joeynmt.helpers - delete models/bpe_4k/22500.ckpt
2025-05-24 19:06:41,827 - INFO - joeynmt.helpers - delete C:/Users/seraf/repositories/mt-exercise-4/models/bpe_4k/22500.ckpt
2025-05-24 19:06:41,828 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint C:\Users\seraf\repositories\mt-exercise-4\models\bpe_4k\22500.ckpt but file does not exist. ([WinError 2] The system cannot find the file specified: 'C:\\Users\\seraf\\repositories\\mt-exercise-4\\models\\bpe_4k\\22500.ckpt')
2025-05-24 19:06:41,828 - INFO - joeynmt.training - Example #0
2025-05-24 19:06:41,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:06:41,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:06:41,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'ij@@', 's', 'ij@@', 'sk@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'raa@@', 'd', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:06:41,829 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:06:41,829 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:06:41,829 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs ijs ijskskap , die de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , heeft een kraad van 40 procent .
2025-05-24 19:06:41,829 - INFO - joeynmt.training - Example #1
2025-05-24 19:06:41,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:06:41,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:06:41,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'k@@', 'ke', 'van', 'de', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 19:06:41,829 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:06:41,829 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:06:41,829 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieieoudheid van dit probleem omdat het niet de dikke van de ijs niet laten zien .
2025-05-24 19:06:41,829 - INFO - joeynmt.training - Example #2
2025-05-24 19:06:41,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:06:41,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:06:41,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'str@@', 'oo@@', 'm@@', 'systeem', '.', '</s>']
2025-05-24 19:06:41,830 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:06:41,830 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:06:41,830 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in zekere zin , het stroomsysteem .
2025-05-24 19:06:41,830 - INFO - joeynmt.training - Example #3
2025-05-24 19:06:41,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:06:41,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:06:41,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'w@@', 'aam', 'in', 'het', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:06:41,830 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:06:41,830 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:06:41,830 - INFO - joeynmt.training - 	Hypothesis: Het duwaam in het winter en contracten in de zomer .
2025-05-24 19:06:41,831 - INFO - joeynmt.training - Example #4
2025-05-24 19:06:41,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:06:41,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:06:41,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 19:06:41,831 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:06:41,831 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:06:41,831 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid in de laatste 25 jaar .
2025-05-24 19:06:47,054 - INFO - joeynmt.training - Epoch   7, Step:    23100, Batch Loss:     1.485657, Batch Acc: 0.609132, Tokens per Sec:    13407, Lr: 0.000300
2025-05-24 19:06:52,381 - INFO - joeynmt.training - Epoch   7, Step:    23200, Batch Loss:     1.491869, Batch Acc: 0.605774, Tokens per Sec:    13763, Lr: 0.000300
2025-05-24 19:06:57,758 - INFO - joeynmt.training - Epoch   7, Step:    23300, Batch Loss:     1.418280, Batch Acc: 0.611700, Tokens per Sec:    13398, Lr: 0.000300
2025-05-24 19:07:03,089 - INFO - joeynmt.training - Epoch   7, Step:    23400, Batch Loss:     1.427527, Batch Acc: 0.606506, Tokens per Sec:    13582, Lr: 0.000300
2025-05-24 19:07:08,301 - INFO - joeynmt.training - Epoch   7, Step:    23500, Batch Loss:     1.225754, Batch Acc: 0.610774, Tokens per Sec:    13594, Lr: 0.000300
2025-05-24 19:07:08,301 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:07:08,301 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:07:16,837 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.62, acc:   0.50, generation: 8.5256[sec], evaluation: 0.0000[sec]
2025-05-24 19:07:16,916 - INFO - joeynmt.helpers - delete models/bpe_4k/20500.ckpt
2025-05-24 19:07:16,922 - INFO - joeynmt.training - Example #0
2025-05-24 19:07:16,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:07:16,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:07:16,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ta@@', 'ire', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'het', 'uit@@', 'er@@', 'ste', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'loo@@', 'f', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:07:16,923 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:07:16,923 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:07:16,923 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de arctische taire cap , die voor de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , heeft het uiterste 48 staten , heeft een kloof van 40 procent .
2025-05-24 19:07:16,923 - INFO - joeynmt.training - Example #1
2025-05-24 19:07:16,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:07:16,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:07:16,923 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mt', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'zo', 'niet', 'de', 'di@@', 'k@@', 'ke', 'van', 'het', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 19:07:16,923 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:07:16,923 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:07:16,923 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemt de serieoudheid van dit probleem omdat het zo niet de dikke van het ijs niet laten zien .
2025-05-24 19:07:16,923 - INFO - joeynmt.training - Example #2
2025-05-24 19:07:16,923 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:07:16,923 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:07:16,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'ca@@', 'p', 'is', 'in', 'een', 'manier', ',', 'het', 'bij@@', 'zonder', 'ij@@', 's@@', 'be@@', 'v@@', 'at', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:07:16,924 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:07:16,924 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:07:16,924 - INFO - joeynmt.training - 	Hypothesis: Het kunstcap is in een manier , het bijzonder ijsbevat hart van het globale klimaat .
2025-05-24 19:07:16,924 - INFO - joeynmt.training - Example #3
2025-05-24 19:07:16,924 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:07:16,924 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:07:16,924 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'b@@', 'bel@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:07:16,924 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:07:16,924 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:07:16,924 - INFO - joeynmt.training - 	Hypothesis: Het dubbelt in de winter en contracten in de zomer .
2025-05-24 19:07:16,925 - INFO - joeynmt.training - Example #4
2025-05-24 19:07:16,925 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:07:16,925 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:07:16,925 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'laat', 'ik', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:07:16,925 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:07:16,925 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:07:16,925 - INFO - joeynmt.training - 	Hypothesis: De volgende dia laat ik jullie een rapid snelste vooruit wat er de laatste 25 jaar is .
2025-05-24 19:07:22,113 - INFO - joeynmt.training - Epoch   7, Step:    23600, Batch Loss:     1.419197, Batch Acc: 0.600809, Tokens per Sec:    13541, Lr: 0.000300
2025-05-24 19:07:27,441 - INFO - joeynmt.training - Epoch   7, Step:    23700, Batch Loss:     1.472332, Batch Acc: 0.607413, Tokens per Sec:    13417, Lr: 0.000300
2025-05-24 19:07:32,735 - INFO - joeynmt.training - Epoch   7, Step:    23800, Batch Loss:     1.314037, Batch Acc: 0.599159, Tokens per Sec:    13561, Lr: 0.000300
2025-05-24 19:07:37,986 - INFO - joeynmt.training - Epoch   7, Step:    23900, Batch Loss:     1.433914, Batch Acc: 0.605292, Tokens per Sec:    13086, Lr: 0.000300
2025-05-24 19:07:43,335 - INFO - joeynmt.training - Epoch   7, Step:    24000, Batch Loss:     1.415569, Batch Acc: 0.602996, Tokens per Sec:    13516, Lr: 0.000300
2025-05-24 19:07:43,336 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:07:43,336 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:07:52,269 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.55, acc:   0.50, generation: 8.9223[sec], evaluation: 0.0000[sec]
2025-05-24 19:07:52,270 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:07:52,352 - INFO - joeynmt.helpers - delete models/bpe_4k/21000.ckpt
2025-05-24 19:07:52,359 - INFO - joeynmt.training - Example #0
2025-05-24 19:07:52,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:07:52,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:07:52,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'be@@', 'ta@@', 'ald', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'is', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:07:52,360 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:07:52,360 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:07:52,360 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs betaald , die voor de meeste afgelopen drie miljoen jaar de grootte van de lagere 48 staten , is door 40 procent .
2025-05-24 19:07:52,360 - INFO - joeynmt.training - Example #1
2025-05-24 19:07:52,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:07:52,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:07:52,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'eert', 'het', 'ser@@', 'ie@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'tr@@', 'ouw@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 19:07:52,360 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:07:52,360 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:07:52,360 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteert het serieieoudheid van dit probleem omdat het de trouwheid van het ijs niet laten zien .
2025-05-24 19:07:52,361 - INFO - joeynmt.training - Example #2
2025-05-24 19:07:52,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:07:52,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:07:52,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'een', 'zin', ',', 'het', 'str@@', 'atie', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:07:52,361 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:07:52,361 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:07:52,361 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in een zin , het stratie van het wereldwijde klimaatsysteem .
2025-05-24 19:07:52,361 - INFO - joeynmt.training - Example #3
2025-05-24 19:07:52,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:07:52,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:07:52,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'ik@@', 'er', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:07:52,362 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:07:52,362 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:07:52,362 - INFO - joeynmt.training - 	Hypothesis: Het duiker in winter en contracten in zomer .
2025-05-24 19:07:52,362 - INFO - joeynmt.training - Example #4
2025-05-24 19:07:52,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:07:52,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:07:52,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'voor', 'wat', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:07:52,363 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:07:52,363 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:07:52,363 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid zal zijn voor wat de afgelopen 25 jaar is gebeurd .
2025-05-24 19:07:57,668 - INFO - joeynmt.training - Epoch   7, Step:    24100, Batch Loss:     1.558798, Batch Acc: 0.607271, Tokens per Sec:    13523, Lr: 0.000300
2025-05-24 19:08:03,017 - INFO - joeynmt.training - Epoch   7, Step:    24200, Batch Loss:     1.433839, Batch Acc: 0.606599, Tokens per Sec:    13175, Lr: 0.000300
2025-05-24 19:08:08,319 - INFO - joeynmt.training - Epoch   7, Step:    24300, Batch Loss:     1.658772, Batch Acc: 0.605054, Tokens per Sec:    13641, Lr: 0.000300
2025-05-24 19:08:13,641 - INFO - joeynmt.training - Epoch   7, Step:    24400, Batch Loss:     1.393605, Batch Acc: 0.604200, Tokens per Sec:    13511, Lr: 0.000300
2025-05-24 19:08:18,763 - INFO - joeynmt.training - Epoch   7, Step:    24500, Batch Loss:     1.432975, Batch Acc: 0.603055, Tokens per Sec:    14047, Lr: 0.000300
2025-05-24 19:08:18,764 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:08:18,764 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:08:27,241 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.52, acc:   0.50, generation: 8.4662[sec], evaluation: 0.0000[sec]
2025-05-24 19:08:27,242 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:08:27,322 - INFO - joeynmt.helpers - delete models/bpe_4k/22000.ckpt
2025-05-24 19:08:27,328 - INFO - joeynmt.training - Example #0
2025-05-24 19:08:27,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:08:27,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:08:27,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'is', 'een', 'k@@', 'ro@@', 'k', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:08:27,329 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:08:27,329 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:08:27,329 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs die de arctische cap , die voor de meeste drie miljoen jaar de grootte van de lagere 48 staten , is een krok van 40 procent .
2025-05-24 19:08:27,329 - INFO - joeynmt.training - Example #1
2025-05-24 19:08:27,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:08:27,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:08:27,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'di@@', 'k@@', 'ke', 'probleem', 'de', 'di@@', 'k@@', 'ke', 'van', 'het', 'ij@@', 's', 'niet', 'laten', '.', '</s>']
2025-05-24 19:08:27,329 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:08:27,329 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:08:27,329 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieus van dit probleem omdat het dikke probleem de dikke van het ijs niet laten .
2025-05-24 19:08:27,329 - INFO - joeynmt.training - Example #2
2025-05-24 19:08:27,330 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:08:27,330 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:08:27,330 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:08:27,330 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:08:27,330 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:08:27,330 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in zekere zin het hart van het wereldwijde klimaat .
2025-05-24 19:08:27,330 - INFO - joeynmt.training - Example #3
2025-05-24 19:08:27,330 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:08:27,330 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:08:27,330 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:08:27,331 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:08:27,331 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:08:27,331 - INFO - joeynmt.training - 	Hypothesis: Het duurt in winter en contracten in zomer .
2025-05-24 19:08:27,331 - INFO - joeynmt.training - Example #4
2025-05-24 19:08:27,331 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:08:27,331 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:08:27,331 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'zal', 'ik', 'jullie', 'laten', 'zien', 'dat', 'jullie', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ler', 'is', 'van', 'wat', 'er', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:08:27,331 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:08:27,331 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:08:27,331 - INFO - joeynmt.training - 	Hypothesis: De volgende dia zal ik jullie laten zien dat jullie een rapid sneller is van wat er is gebeurd .
2025-05-24 19:08:32,633 - INFO - joeynmt.training - Epoch   7, Step:    24600, Batch Loss:     1.482804, Batch Acc: 0.606067, Tokens per Sec:    13342, Lr: 0.000300
2025-05-24 19:08:37,962 - INFO - joeynmt.training - Epoch   7, Step:    24700, Batch Loss:     1.581301, Batch Acc: 0.599801, Tokens per Sec:    13230, Lr: 0.000300
2025-05-24 19:08:43,171 - INFO - joeynmt.training - Epoch   7, Step:    24800, Batch Loss:     1.328331, Batch Acc: 0.603920, Tokens per Sec:    13419, Lr: 0.000300
2025-05-24 19:08:48,421 - INFO - joeynmt.training - Epoch   7, Step:    24900, Batch Loss:     1.594752, Batch Acc: 0.606188, Tokens per Sec:    13151, Lr: 0.000300
2025-05-24 19:08:53,762 - INFO - joeynmt.training - Epoch   7, Step:    25000, Batch Loss:     1.247126, Batch Acc: 0.605674, Tokens per Sec:    13490, Lr: 0.000300
2025-05-24 19:08:53,762 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:08:53,762 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:09:02,534 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.49, acc:   0.51, generation: 8.7618[sec], evaluation: 0.0000[sec]
2025-05-24 19:09:02,534 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:09:02,613 - INFO - joeynmt.helpers - delete models/bpe_4k/23500.ckpt
2025-05-24 19:09:02,619 - INFO - joeynmt.training - Example #0
2025-05-24 19:09:02,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:09:02,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:09:02,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', ',', 'die', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:09:02,620 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:09:02,620 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:09:02,620 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijs die de arctische ijs , die de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , heeft een door 40 procent .
2025-05-24 19:09:02,620 - INFO - joeynmt.training - Example #1
2025-05-24 19:09:02,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:09:02,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:09:02,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'd@@', 'roe@@', 'g', 'van', 'het', 'ij@@', 's', 'niet', 'laten', '.', '</s>']
2025-05-24 19:09:02,620 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:09:02,620 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:09:02,620 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieus van dit specifieke probleem omdat het droeg van het ijs niet laten .
2025-05-24 19:09:02,620 - INFO - joeynmt.training - Example #2
2025-05-24 19:09:02,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:09:02,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:09:02,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tic', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:09:02,621 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:09:02,621 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:09:02,621 - INFO - joeynmt.training - 	Hypothesis: Het arctic ijs is , in zekere zin het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:09:02,621 - INFO - joeynmt.training - Example #3
2025-05-24 19:09:02,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:09:02,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:09:02,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'wen', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:09:02,621 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:09:02,621 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:09:02,622 - INFO - joeynmt.training - 	Hypothesis: Het duwen in winter en contracten in de zomer .
2025-05-24 19:09:02,622 - INFO - joeynmt.training - Example #4
2025-05-24 19:09:02,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:09:02,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:09:02,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:09:02,622 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:09:02,622 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:09:02,622 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid in de laatste 25 jaar is .
2025-05-24 19:09:07,889 - INFO - joeynmt.training - Epoch   7, Step:    25100, Batch Loss:     1.465614, Batch Acc: 0.601545, Tokens per Sec:    13391, Lr: 0.000300
2025-05-24 19:09:12,341 - INFO - joeynmt.training - Epoch   7: total training loss 5117.91
2025-05-24 19:09:12,341 - INFO - joeynmt.training - EPOCH 8
2025-05-24 19:09:13,300 - INFO - joeynmt.training - Epoch   8, Step:    25200, Batch Loss:     1.495987, Batch Acc: 0.620639, Tokens per Sec:    14225, Lr: 0.000300
2025-05-24 19:09:18,505 - INFO - joeynmt.training - Epoch   8, Step:    25300, Batch Loss:     1.325400, Batch Acc: 0.628463, Tokens per Sec:    13971, Lr: 0.000300
2025-05-24 19:09:23,898 - INFO - joeynmt.training - Epoch   8, Step:    25400, Batch Loss:     1.333911, Batch Acc: 0.626251, Tokens per Sec:    13337, Lr: 0.000300
2025-05-24 19:09:29,106 - INFO - joeynmt.training - Epoch   8, Step:    25500, Batch Loss:     1.282009, Batch Acc: 0.624998, Tokens per Sec:    13811, Lr: 0.000300
2025-05-24 19:09:29,106 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:09:29,106 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:09:37,386 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.61, acc:   0.51, generation: 8.2696[sec], evaluation: 0.0000[sec]
2025-05-24 19:09:37,387 - INFO - joeynmt.training - Example #0
2025-05-24 19:09:37,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:09:37,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:09:37,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'het', 'ar@@', 'c@@', 'tisch', 'ij@@', 's', ',', 'wat', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'oe@@', 'p@@', 'el', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:09:37,388 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:09:37,389 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:09:37,389 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat het arctisch ijs , wat voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , de grootte van de lager 48 staten , heeft een koepel van 40 procent .
2025-05-24 19:09:37,389 - INFO - joeynmt.training - Example #1
2025-05-24 19:09:37,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:09:37,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:09:37,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'zo', 'niet', 'de', 'di@@', 'ep', 'van', 'de', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 19:09:37,389 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:09:37,389 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:09:37,390 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieieus van dit specifieke probleem omdat het zo niet de diep van de ijs van het ijs .
2025-05-24 19:09:37,390 - INFO - joeynmt.training - Example #2
2025-05-24 19:09:37,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:09:37,390 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:09:37,390 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'str@@', 'and', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:09:37,390 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:09:37,390 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:09:37,390 - INFO - joeynmt.training - 	Hypothesis: Het kunstcap is , in zekere zin het strand van het wereldwijd klimaatsysteem .
2025-05-24 19:09:37,390 - INFO - joeynmt.training - Example #3
2025-05-24 19:09:37,390 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:09:37,391 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:09:37,391 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'dt', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:09:37,391 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:09:37,391 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:09:37,391 - INFO - joeynmt.training - 	Hypothesis: Het verbreidt in de zomer en contracten in de zomer .
2025-05-24 19:09:37,391 - INFO - joeynmt.training - Example #4
2025-05-24 19:09:37,391 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:09:37,391 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:09:37,391 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laten', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ler', 'zou', 'zijn', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:09:37,392 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:09:37,392 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:09:37,392 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laten zien dat een rapid sneller zou zijn van wat er de laatste 25 jaar is .
2025-05-24 19:09:42,645 - INFO - joeynmt.training - Epoch   8, Step:    25600, Batch Loss:     1.278677, Batch Acc: 0.625784, Tokens per Sec:    13463, Lr: 0.000300
2025-05-24 19:09:48,026 - INFO - joeynmt.training - Epoch   8, Step:    25700, Batch Loss:     1.239333, Batch Acc: 0.624574, Tokens per Sec:    13430, Lr: 0.000300
2025-05-24 19:09:53,344 - INFO - joeynmt.training - Epoch   8, Step:    25800, Batch Loss:     1.196224, Batch Acc: 0.623905, Tokens per Sec:    13079, Lr: 0.000300
2025-05-24 19:09:58,773 - INFO - joeynmt.training - Epoch   8, Step:    25900, Batch Loss:     1.389521, Batch Acc: 0.625799, Tokens per Sec:    13112, Lr: 0.000300
2025-05-24 19:10:03,971 - INFO - joeynmt.training - Epoch   8, Step:    26000, Batch Loss:     1.321837, Batch Acc: 0.623547, Tokens per Sec:    13473, Lr: 0.000300
2025-05-24 19:10:03,971 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:10:03,971 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:10:12,996 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.57, acc:   0.50, generation: 9.0132[sec], evaluation: 0.0000[sec]
2025-05-24 19:10:13,076 - INFO - joeynmt.helpers - delete models/bpe_4k/23000.ckpt
2025-05-24 19:10:13,081 - INFO - joeynmt.training - Example #0
2025-05-24 19:10:13,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:10:13,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:10:13,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'de', 'k@@', 'aar@@', 'ten', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:10:13,082 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:10:13,082 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:10:13,082 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat demonstreren dat de arctische ijs die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is de kaarten van 40 procent .
2025-05-24 19:10:13,082 - INFO - joeynmt.training - Example #1
2025-05-24 19:10:13,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:10:13,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:10:13,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'beg@@', 'ri@@', 'p', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'probleem', 'omdat', 'het', 'zo', 'de', 'tr@@', 'ots', 'is', 'van', 'de', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 19:10:13,083 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:10:13,083 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:10:13,083 - INFO - joeynmt.training - 	Hypothesis: Maar dit begrip de serieus van dit probleem probleem omdat het zo de trots is van de ijs niet .
2025-05-24 19:10:13,083 - INFO - joeynmt.training - Example #2
2025-05-24 19:10:13,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:10:13,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:10:13,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'be@@', 'sla@@', 'g@@', 'ster', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:10:13,083 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:10:13,083 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:10:13,083 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is , in zekere zin het beslagster van het wereldwijd klimaatsysteem .
2025-05-24 19:10:13,083 - INFO - joeynmt.training - Example #3
2025-05-24 19:10:13,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:10:13,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:10:13,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'anden', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:10:13,084 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:10:13,084 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:10:13,084 - INFO - joeynmt.training - 	Hypothesis: Het expanden in winter en contracten in de zomer .
2025-05-24 19:10:13,084 - INFO - joeynmt.training - Example #4
2025-05-24 19:10:13,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:10:13,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:10:13,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'apen', '&apos;', 'snel@@', 'st-@@', 'voor@@', 'uit@@', 'gang', 'zijn', 'van', 'wat', 'er', 'gebeurd', 'is', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:10:13,084 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:10:13,084 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:10:13,084 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat je een rapen &apos; snelst-vooruitgang zijn van wat er gebeurd is in de afgelopen 25 jaar .
2025-05-24 19:10:18,464 - INFO - joeynmt.training - Epoch   8, Step:    26100, Batch Loss:     1.362945, Batch Acc: 0.619163, Tokens per Sec:    13336, Lr: 0.000300
2025-05-24 19:10:23,744 - INFO - joeynmt.training - Epoch   8, Step:    26200, Batch Loss:     1.375171, Batch Acc: 0.619314, Tokens per Sec:    13699, Lr: 0.000300
2025-05-24 19:10:29,106 - INFO - joeynmt.training - Epoch   8, Step:    26300, Batch Loss:     1.280263, Batch Acc: 0.619299, Tokens per Sec:    13389, Lr: 0.000300
2025-05-24 19:10:34,418 - INFO - joeynmt.training - Epoch   8, Step:    26400, Batch Loss:     1.550812, Batch Acc: 0.615943, Tokens per Sec:    13337, Lr: 0.000300
2025-05-24 19:10:39,607 - INFO - joeynmt.training - Epoch   8, Step:    26500, Batch Loss:     1.318125, Batch Acc: 0.616417, Tokens per Sec:    13539, Lr: 0.000300
2025-05-24 19:10:39,607 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:10:39,607 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:10:47,534 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.52, acc:   0.50, generation: 7.9144[sec], evaluation: 0.0000[sec]
2025-05-24 19:10:47,616 - INFO - joeynmt.helpers - delete models/bpe_4k/21500.ckpt
2025-05-24 19:10:47,621 - INFO - joeynmt.training - Example #0
2025-05-24 19:10:47,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:10:47,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:10:47,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'ge@@', 'de@@', 'mon@@', 'stre@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'sk@@', 'sk@@', 'appen', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'met', '40', 'procent', '.', '</s>']
2025-05-24 19:10:47,622 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:10:47,622 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:10:47,622 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s gedemonstreeren dat de arctische ijskskappen , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , met 40 procent .
2025-05-24 19:10:47,622 - INFO - joeynmt.training - Example #1
2025-05-24 19:10:47,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:10:47,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:10:47,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'van', 'het', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'ep', 'van', 'de', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:10:47,623 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:10:47,623 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:10:47,623 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten van het serieoudheid van dit specifieke probleem omdat het de diep van de ijs niet laat zien .
2025-05-24 19:10:47,623 - INFO - joeynmt.training - Example #2
2025-05-24 19:10:47,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:10:47,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:10:47,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 'sk@@', 'sk@@', 'appen', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:10:47,623 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:10:47,623 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:10:47,623 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijskskappen is , in zekere zin het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:10:47,623 - INFO - joeynmt.training - Example #3
2025-05-24 19:10:47,624 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:10:47,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:10:47,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ver@@', 'bre@@', 'i@@', 'dt', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:10:47,624 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:10:47,624 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:10:47,624 - INFO - joeynmt.training - 	Hypothesis: Het verbreidt in de zomer en contracten in de zomer .
2025-05-24 19:10:47,624 - INFO - joeynmt.training - Example #4
2025-05-24 19:10:47,624 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:10:47,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:10:47,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ler', 'is', 'van', 'wat', 'er', 'gebeurd', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:10:47,624 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:10:47,624 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:10:47,624 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat een rapid sneller is van wat er gebeurd is gebeurd .
2025-05-24 19:10:52,993 - INFO - joeynmt.training - Epoch   8, Step:    26600, Batch Loss:     1.279434, Batch Acc: 0.616836, Tokens per Sec:    13221, Lr: 0.000300
2025-05-24 19:10:58,202 - INFO - joeynmt.training - Epoch   8, Step:    26700, Batch Loss:     1.584431, Batch Acc: 0.619465, Tokens per Sec:    13758, Lr: 0.000300
2025-05-24 19:11:03,541 - INFO - joeynmt.training - Epoch   8, Step:    26800, Batch Loss:     1.417263, Batch Acc: 0.615048, Tokens per Sec:    13627, Lr: 0.000300
2025-05-24 19:11:08,831 - INFO - joeynmt.training - Epoch   8, Step:    26900, Batch Loss:     1.232918, Batch Acc: 0.620063, Tokens per Sec:    13439, Lr: 0.000300
2025-05-24 19:11:14,078 - INFO - joeynmt.training - Epoch   8, Step:    27000, Batch Loss:     1.552194, Batch Acc: 0.617752, Tokens per Sec:    13791, Lr: 0.000300
2025-05-24 19:11:14,078 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:11:14,078 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:11:22,206 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.43, acc:   0.51, generation: 8.1179[sec], evaluation: 0.0000[sec]
2025-05-24 19:11:22,206 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:11:22,287 - INFO - joeynmt.helpers - delete models/bpe_4k/26000.ckpt
2025-05-24 19:11:22,293 - INFO - joeynmt.training - Example #0
2025-05-24 19:11:22,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:11:22,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:11:22,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'laten', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'ka@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'sla@@', 'cht', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:11:22,294 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:11:22,294 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:11:22,294 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s laten zien zodat de arctische ijferentiekap , die voor de meeste van de laatste drie miljoen jaar de grootte van de lager 48 staten , is geslacht door 40 procent .
2025-05-24 19:11:22,294 - INFO - joeynmt.training - Example #1
2025-05-24 19:11:22,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:11:22,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:11:22,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'the@@', 'id', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:11:22,295 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:11:22,295 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:11:22,295 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serieoudheid van dit probleem omdat het de diktheid van het ijs niet laat zien .
2025-05-24 19:11:22,295 - INFO - joeynmt.training - Example #2
2025-05-24 19:11:22,295 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:11:22,295 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:11:22,295 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'ca@@', 'p', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't@@', 'slag', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:11:22,295 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:11:22,295 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:11:22,295 - INFO - joeynmt.training - 	Hypothesis: Het kunstcap is in zekere zin het hartslag van het globale klimaat .
2025-05-24 19:11:22,295 - INFO - joeynmt.training - Example #3
2025-05-24 19:11:22,295 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:11:22,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:11:22,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'en', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:11:22,296 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:11:22,296 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:11:22,296 - INFO - joeynmt.training - 	Hypothesis: Het duien in winter en contracten in de zomer .
2025-05-24 19:11:22,296 - INFO - joeynmt.training - Example #4
2025-05-24 19:11:22,296 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:11:22,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:11:22,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'je', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:11:22,296 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:11:22,296 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:11:22,296 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik je laat zien dat je een rapid in de laatste 25 jaar is .
2025-05-24 19:11:27,670 - INFO - joeynmt.training - Epoch   8, Step:    27100, Batch Loss:     1.416254, Batch Acc: 0.622144, Tokens per Sec:    12955, Lr: 0.000300
2025-05-24 19:11:32,939 - INFO - joeynmt.training - Epoch   8, Step:    27200, Batch Loss:     1.307931, Batch Acc: 0.612696, Tokens per Sec:    13403, Lr: 0.000300
2025-05-24 19:11:38,196 - INFO - joeynmt.training - Epoch   8, Step:    27300, Batch Loss:     1.580355, Batch Acc: 0.614075, Tokens per Sec:    13397, Lr: 0.000300
2025-05-24 19:11:43,561 - INFO - joeynmt.training - Epoch   8, Step:    27400, Batch Loss:     1.212424, Batch Acc: 0.616375, Tokens per Sec:    13558, Lr: 0.000300
2025-05-24 19:11:48,846 - INFO - joeynmt.training - Epoch   8, Step:    27500, Batch Loss:     1.385985, Batch Acc: 0.611722, Tokens per Sec:    13360, Lr: 0.000300
2025-05-24 19:11:48,847 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:11:48,847 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:11:57,386 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.50, acc:   0.51, generation: 8.5292[sec], evaluation: 0.0000[sec]
2025-05-24 19:11:57,470 - INFO - joeynmt.helpers - delete models/bpe_4k/24000.ckpt
2025-05-24 19:11:57,475 - INFO - joeynmt.training - Example #0
2025-05-24 19:11:57,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:11:57,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:11:57,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'me', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'is', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', 'van', 'de', 'la@@', 'ger', 'van', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:11:57,475 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:11:57,475 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:11:57,475 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zodat de arctische ijferentieme cap , die voor de meeste afgelopen drie miljoen jaar is de grootte van de lager van de lager van 48 staten , heeft een van 40 procent .
2025-05-24 19:11:57,475 - INFO - joeynmt.training - Example #1
2025-05-24 19:11:57,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:11:57,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:11:57,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'ert', 'het', 'ser@@', 'ie@@', '-@@', 'ser@@', 'i@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'di@@', 'k@@', 'the@@', 'id', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 19:11:57,477 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:11:57,477 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:11:57,477 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteert het serie-serioudheid van dit probleem omdat het diktheid van het ijs van het ijs .
2025-05-24 19:11:57,477 - INFO - joeynmt.training - Example #2
2025-05-24 19:11:57,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:11:57,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:11:57,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tisch', 'ij@@', 'fer@@', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'be@@', 'sla@@', 'gen', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:11:57,477 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:11:57,477 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:11:57,477 - INFO - joeynmt.training - 	Hypothesis: Het arctisch ijferis , in zekere zin het beslagen hart van het wereldwijd klimaat .
2025-05-24 19:11:57,477 - INFO - joeynmt.training - Example #3
2025-05-24 19:11:57,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:11:57,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:11:57,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'kt', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er@@', 'om@@', 'er@@', 'w@@', 'int', '.', '</s>']
2025-05-24 19:11:57,478 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:11:57,478 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:11:57,478 - INFO - joeynmt.training - 	Hypothesis: Het duikt in winter en contracten in de zomeromerwint .
2025-05-24 19:11:57,478 - INFO - joeynmt.training - Example #4
2025-05-24 19:11:57,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:11:57,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:11:57,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor', 'wat', 'er', 'is', 'gebeurd', 'is', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:11:57,479 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:11:57,479 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:11:57,479 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid snelste voor wat er is gebeurd is in de afgelopen 25 jaar .
2025-05-24 19:12:02,916 - INFO - joeynmt.training - Epoch   8, Step:    27600, Batch Loss:     1.356721, Batch Acc: 0.615757, Tokens per Sec:    12615, Lr: 0.000300
2025-05-24 19:12:08,147 - INFO - joeynmt.training - Epoch   8, Step:    27700, Batch Loss:     1.270880, Batch Acc: 0.613996, Tokens per Sec:    13844, Lr: 0.000300
2025-05-24 19:12:13,476 - INFO - joeynmt.training - Epoch   8, Step:    27800, Batch Loss:     1.537868, Batch Acc: 0.612709, Tokens per Sec:    13487, Lr: 0.000300
2025-05-24 19:12:18,717 - INFO - joeynmt.training - Epoch   8, Step:    27900, Batch Loss:     1.538531, Batch Acc: 0.614965, Tokens per Sec:    13220, Lr: 0.000300
2025-05-24 19:12:24,006 - INFO - joeynmt.training - Epoch   8, Step:    28000, Batch Loss:     1.355162, Batch Acc: 0.610986, Tokens per Sec:    13755, Lr: 0.000300
2025-05-24 19:12:24,006 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:12:24,006 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:12:32,836 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.48, acc:   0.51, generation: 8.8205[sec], evaluation: 0.0000[sec]
2025-05-24 19:12:32,922 - INFO - joeynmt.helpers - delete models/bpe_4k/26500.ckpt
2025-05-24 19:12:32,927 - INFO - joeynmt.training - Example #0
2025-05-24 19:12:32,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'af@@', 'geb@@', 'racht', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'sla@@', 'cht', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:12:32,928 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:12:32,928 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:12:32,928 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de afgelopen drie miljoen jaar de afgebracht , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is geslacht van 40 procent .
2025-05-24 19:12:32,928 - INFO - joeynmt.training - Example #1
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'e@@', 'k@@', 'su@@', 'ele', 'het', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'bed@@', 'ro@@', 'g', 'van', 'het', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 19:12:32,928 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:12:32,928 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:12:32,928 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteeksuele het van dit probleem omdat het de bedrog van het ijs niet .
2025-05-24 19:12:32,928 - INFO - joeynmt.training - Example #2
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:12:32,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'ca@@', 'p', 'is', 'in', 'een', 'zin', ',', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ste@@', 'mm@@', 'ing', 'van', 'het', 'mon@@', 'di@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:12:32,930 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:12:32,930 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:12:32,930 - INFO - joeynmt.training - 	Hypothesis: Het kunstcap is in een zin , in een zin , het bestemming van het mondiale klimaatsysteem .
2025-05-24 19:12:32,930 - INFO - joeynmt.training - Example #3
2025-05-24 19:12:32,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:12:32,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:12:32,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 't', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:12:32,930 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:12:32,930 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:12:32,930 - INFO - joeynmt.training - 	Hypothesis: Het duit in winter en contracten in de zomer .
2025-05-24 19:12:32,931 - INFO - joeynmt.training - Example #4
2025-05-24 19:12:32,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:12:32,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:12:32,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'ik', 'laat', 'zien', 'dat', 'het', 'een', 'r@@', 'ap@@', 'id', 'in', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:12:32,931 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:12:32,931 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:12:32,931 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat ik laat zien dat het een rapid in de afgelopen 25 jaar is .
2025-05-24 19:12:38,279 - INFO - joeynmt.training - Epoch   8, Step:    28100, Batch Loss:     1.537570, Batch Acc: 0.611907, Tokens per Sec:    13530, Lr: 0.000300
2025-05-24 19:12:43,424 - INFO - joeynmt.training - Epoch   8, Step:    28200, Batch Loss:     1.307939, Batch Acc: 0.612634, Tokens per Sec:    13910, Lr: 0.000300
2025-05-24 19:12:48,795 - INFO - joeynmt.training - Epoch   8, Step:    28300, Batch Loss:     1.215866, Batch Acc: 0.616801, Tokens per Sec:    13409, Lr: 0.000300
2025-05-24 19:12:53,884 - INFO - joeynmt.training - Epoch   8, Step:    28400, Batch Loss:     1.324173, Batch Acc: 0.613229, Tokens per Sec:    13886, Lr: 0.000300
2025-05-24 19:12:59,166 - INFO - joeynmt.training - Epoch   8, Step:    28500, Batch Loss:     1.376579, Batch Acc: 0.607897, Tokens per Sec:    13444, Lr: 0.000300
2025-05-24 19:12:59,166 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:12:59,166 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:13:08,359 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.43, acc:   0.51, generation: 9.1826[sec], evaluation: 0.0000[sec]
2025-05-24 19:13:08,360 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:13:08,445 - INFO - joeynmt.helpers - delete models/bpe_4k/24500.ckpt
2025-05-24 19:13:08,451 - INFO - joeynmt.training - Example #0
2025-05-24 19:13:08,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:13:08,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:13:08,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'af@@', 'stand', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'miljoen', 'jaar', 'was', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'het', 'ver@@', 'la@@', 'gen', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:13:08,452 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:13:08,452 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:13:08,452 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de laatste drie miljoen jaar de afstand van de lagere 48 miljoen jaar was de grootte van de lager 48 staten , heeft het verlagen van 40 procent .
2025-05-24 19:13:08,453 - INFO - joeynmt.training - Example #1
2025-05-24 19:13:08,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:13:08,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:13:08,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'deze', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'heid', 'van', 'het', 'ij@@', 's', 'niet', 'laten', 'zien', '.', '</s>']
2025-05-24 19:13:08,453 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:13:08,453 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:13:08,453 - INFO - joeynmt.training - 	Hypothesis: Maar deze onderstaten het serieieus van dit probleem omdat het de dikheid van het ijs niet laten zien .
2025-05-24 19:13:08,453 - INFO - joeynmt.training - Example #2
2025-05-24 19:13:08,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:13:08,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:13:08,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', 'in', 'een', 'zin', ',', 'het', 'be@@', 'ta@@', 'al@@', 'systeem', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:13:08,454 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:13:08,454 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:13:08,454 - INFO - joeynmt.training - 	Hypothesis: De arctische ijs is in een zin , het betaalsysteem van het wereldwijde klimaatsysteem .
2025-05-24 19:13:08,454 - INFO - joeynmt.training - Example #3
2025-05-24 19:13:08,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:13:08,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:13:08,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'p@@', 'pen', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:13:08,455 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:13:08,455 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:13:08,455 - INFO - joeynmt.training - 	Hypothesis: Het duippen in winter en contracten in de zomer .
2025-05-24 19:13:08,455 - INFO - joeynmt.training - Example #4
2025-05-24 19:13:08,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:13:08,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:13:08,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'snel@@', 'le', 'snel@@', 'ste', 'voor@@', 'uit', 'zal', 'zijn', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:13:08,455 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:13:08,456 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:13:08,456 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een snelle snelste vooruit zal zijn van wat er de laatste 25 jaar is .
2025-05-24 19:13:13,666 - INFO - joeynmt.training - Epoch   8, Step:    28600, Batch Loss:     1.448884, Batch Acc: 0.609949, Tokens per Sec:    13070, Lr: 0.000300
2025-05-24 19:13:18,875 - INFO - joeynmt.training - Epoch   8, Step:    28700, Batch Loss:     1.386300, Batch Acc: 0.617791, Tokens per Sec:    13477, Lr: 0.000300
2025-05-24 19:13:23,198 - INFO - joeynmt.training - Epoch   8: total training loss 4972.30
2025-05-24 19:13:23,198 - INFO - joeynmt.training - EPOCH 9
2025-05-24 19:13:24,071 - INFO - joeynmt.training - Epoch   9, Step:    28800, Batch Loss:     1.229102, Batch Acc: 0.634090, Tokens per Sec:    14339, Lr: 0.000300
2025-05-24 19:13:29,209 - INFO - joeynmt.training - Epoch   9, Step:    28900, Batch Loss:     1.028872, Batch Acc: 0.639161, Tokens per Sec:    14091, Lr: 0.000300
2025-05-24 19:13:34,501 - INFO - joeynmt.training - Epoch   9, Step:    29000, Batch Loss:     1.398611, Batch Acc: 0.635651, Tokens per Sec:    13094, Lr: 0.000300
2025-05-24 19:13:34,501 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:13:34,501 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:13:42,928 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.51, acc:   0.51, generation: 8.4125[sec], evaluation: 0.0000[sec]
2025-05-24 19:13:42,929 - INFO - joeynmt.training - Example #0
2025-05-24 19:13:42,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:13:42,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:13:42,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'voor@@', 'bij@@', 'e', 'ij@@', 'd@@', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ver@@', 'de@@', 'eld', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:13:42,930 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:13:42,930 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:13:42,930 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat de voorbije ijdcap , die voor de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , is verdeeld door 40 procent .
2025-05-24 19:13:42,930 - INFO - joeynmt.training - Example #1
2025-05-24 19:13:42,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:13:42,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'de', 'ser@@', 'i@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'kte', 'van', 'de', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten de serioudheid van dit probleem omdat het de dikte van de ijs niet laat zien .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - Example #2
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'ka@@', 'p', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Hypothesis: De arctische ijferentiekap is in zekere zin het hart van het wereldwijd klimaat .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - Example #3
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in zomer .
2025-05-24 19:13:42,931 - INFO - joeynmt.training - Example #4
2025-05-24 19:13:42,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:13:42,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:13:42,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'snel@@', 'le', 'van', 'wat', 'de', 'afgelopen', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:13:42,933 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:13:42,933 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:13:42,933 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een snelle van wat de afgelopen 25 jaar is .
2025-05-24 19:13:48,236 - INFO - joeynmt.training - Epoch   9, Step:    29100, Batch Loss:     1.366017, Batch Acc: 0.633695, Tokens per Sec:    13580, Lr: 0.000300
2025-05-24 19:13:53,564 - INFO - joeynmt.training - Epoch   9, Step:    29200, Batch Loss:     1.269461, Batch Acc: 0.635824, Tokens per Sec:    13157, Lr: 0.000300
2025-05-24 19:13:58,838 - INFO - joeynmt.training - Epoch   9, Step:    29300, Batch Loss:     1.538745, Batch Acc: 0.630386, Tokens per Sec:    13378, Lr: 0.000300
2025-05-24 19:14:04,088 - INFO - joeynmt.training - Epoch   9, Step:    29400, Batch Loss:     1.257632, Batch Acc: 0.633633, Tokens per Sec:    13526, Lr: 0.000300
2025-05-24 19:14:09,493 - INFO - joeynmt.training - Epoch   9, Step:    29500, Batch Loss:     1.382805, Batch Acc: 0.631011, Tokens per Sec:    13159, Lr: 0.000300
2025-05-24 19:14:09,493 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:14:09,493 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:14:18,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.50, acc:   0.51, generation: 8.8861[sec], evaluation: 0.0000[sec]
2025-05-24 19:14:18,395 - INFO - joeynmt.training - Example #0
2025-05-24 19:14:18,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:14:18,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:14:18,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'het', 'kun@@', 'st@@', 'ij@@', 'sk@@', 'i@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:14:18,396 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:14:18,396 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:14:18,396 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat het kunstijskip , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , heeft door 40 procent .
2025-05-24 19:14:18,396 - INFO - joeynmt.training - Example #1
2025-05-24 19:14:18,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:14:18,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:14:18,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'ert', 'de', 'ser@@', 'i@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'de', 'di@@', 'ep', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.', '</s>']
2025-05-24 19:14:18,397 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:14:18,397 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:14:18,397 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteert de serioudheid van dit specifieke probleem omdat het de diep van het ijs laat zien .
2025-05-24 19:14:18,397 - INFO - joeynmt.training - Example #2
2025-05-24 19:14:18,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:14:18,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:14:18,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'is', 'in', 'een', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:14:18,397 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:14:18,397 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:14:18,397 - INFO - joeynmt.training - 	Hypothesis: De arctische ijferis in een zin het hart van het wereldwijklimaatsysteem .
2025-05-24 19:14:18,398 - INFO - joeynmt.training - Example #3
2025-05-24 19:14:18,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:14:18,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:14:18,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:14:18,398 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:14:18,398 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:14:18,398 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de zomer en contracten in de zomer .
2025-05-24 19:14:18,398 - INFO - joeynmt.training - Example #4
2025-05-24 19:14:18,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:14:18,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:14:18,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'snel@@', 'le', 'snel@@', 'le', 'van', 'wat', 'er', 'is', 'gebeurd', 'gebeurd', ',', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 19:14:18,399 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:14:18,399 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:14:18,399 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een snelle snelle van wat er is gebeurd gebeurd , de laatste 25 jaar .
2025-05-24 19:14:23,623 - INFO - joeynmt.training - Epoch   9, Step:    29600, Batch Loss:     1.350801, Batch Acc: 0.634547, Tokens per Sec:    13615, Lr: 0.000300
2025-05-24 19:14:28,954 - INFO - joeynmt.training - Epoch   9, Step:    29700, Batch Loss:     1.377924, Batch Acc: 0.628716, Tokens per Sec:    13119, Lr: 0.000300
2025-05-24 19:14:34,198 - INFO - joeynmt.training - Epoch   9, Step:    29800, Batch Loss:     1.306512, Batch Acc: 0.629452, Tokens per Sec:    13615, Lr: 0.000300
2025-05-24 19:14:39,433 - INFO - joeynmt.training - Epoch   9, Step:    29900, Batch Loss:     1.356652, Batch Acc: 0.625701, Tokens per Sec:    13360, Lr: 0.000300
2025-05-24 19:14:44,721 - INFO - joeynmt.training - Epoch   9, Step:    30000, Batch Loss:     1.291443, Batch Acc: 0.622384, Tokens per Sec:    13484, Lr: 0.000300
2025-05-24 19:14:44,721 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:14:44,722 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:14:54,220 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.48, acc:   0.51, generation: 9.4882[sec], evaluation: 0.0000[sec]
2025-05-24 19:14:54,298 - INFO - joeynmt.helpers - delete models/bpe_4k/27500.ckpt
2025-05-24 19:14:54,304 - INFO - joeynmt.training - Example #0
2025-05-24 19:14:54,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:14:54,304 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:14:54,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zodat', 'de', 'kun@@', 'st@@', 'ic@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'sla@@', 'gen', 'van', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'sla@@', 'gen', 'door', '40', '%', '.', '</s>']
2025-05-24 19:14:54,305 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:14:54,305 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:14:54,305 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zodat de kunsticp , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is geslagen van 48 staten , is geslagen door 40 % .
2025-05-24 19:14:54,305 - INFO - joeynmt.training - Example #1
2025-05-24 19:14:54,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:14:54,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:14:54,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'deel', 'van', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'probleem', 'omdat', 'het', 'zo', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'niet', 'laat', '.', '</s>']
2025-05-24 19:14:54,305 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:14:54,305 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:14:54,305 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderdeel van het serieus van dit probleem probleem omdat het zo niet de dikte van het ijs niet laat .
2025-05-24 19:14:54,305 - INFO - joeynmt.training - Example #2
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tisch', 'ij@@', 's', 'ka@@', 'p', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:14:54,306 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:14:54,306 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:14:54,306 - INFO - joeynmt.training - 	Hypothesis: De arctisch ijs kap is in zekere zin het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:14:54,306 - INFO - joeynmt.training - Example #3
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'gend', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:14:54,306 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:14:54,306 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:14:54,306 - INFO - joeynmt.training - 	Hypothesis: Het duigend in de zomer en contracten in de zomer .
2025-05-24 19:14:54,306 - INFO - joeynmt.training - Example #4
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:14:54,306 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:14:54,307 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:14:54,307 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:14:54,307 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid zal zijn van wat de laatste 25 jaar is .
2025-05-24 19:14:59,631 - INFO - joeynmt.training - Epoch   9, Step:    30100, Batch Loss:     1.293041, Batch Acc: 0.624947, Tokens per Sec:    12998, Lr: 0.000300
2025-05-24 19:15:05,010 - INFO - joeynmt.training - Epoch   9, Step:    30200, Batch Loss:     1.471388, Batch Acc: 0.624837, Tokens per Sec:    13387, Lr: 0.000300
2025-05-24 19:15:10,172 - INFO - joeynmt.training - Epoch   9, Step:    30300, Batch Loss:     1.381499, Batch Acc: 0.625801, Tokens per Sec:    14056, Lr: 0.000300
2025-05-24 19:15:15,588 - INFO - joeynmt.training - Epoch   9, Step:    30400, Batch Loss:     1.441276, Batch Acc: 0.628033, Tokens per Sec:    13561, Lr: 0.000300
2025-05-24 19:15:20,838 - INFO - joeynmt.training - Epoch   9, Step:    30500, Batch Loss:     1.426278, Batch Acc: 0.623225, Tokens per Sec:    13865, Lr: 0.000300
2025-05-24 19:15:20,839 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:15:20,839 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:15:29,444 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.48, acc:   0.51, generation: 8.5950[sec], evaluation: 0.0000[sec]
2025-05-24 19:15:29,522 - INFO - joeynmt.helpers - delete models/bpe_4k/25000.ckpt
2025-05-24 19:15:29,528 - INFO - joeynmt.training - Example #0
2025-05-24 19:15:29,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:15:29,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:15:29,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'aan@@', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'st@@', 'aten', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'sp@@', 'ron@@', 'gen', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:15:29,528 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:15:29,528 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:15:29,528 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat de aandemonstreren dat de arctische ijferentiestaten , die voor de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , is gesprongen door 40 procent .
2025-05-24 19:15:29,528 - INFO - joeynmt.training - Example #1
2025-05-24 19:15:29,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:15:29,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:15:29,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'ie@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'k@@', 'ke', 'van', 'het', 'ij@@', 's', 'niet', 'laten', '.', '</s>']
2025-05-24 19:15:29,529 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:15:29,529 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:15:29,529 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieieoudheid van dit probleem omdat het niet de dikke van het ijs niet laten .
2025-05-24 19:15:29,529 - INFO - joeynmt.training - Example #2
2025-05-24 19:15:29,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:15:29,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:15:29,529 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'sk@@', 'racht', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:15:29,529 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:15:29,530 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:15:29,530 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijferentieskracht is , in zekere zin het hart van het globale klimaatsysteem .
2025-05-24 19:15:29,530 - INFO - joeynmt.training - Example #3
2025-05-24 19:15:29,530 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:15:29,530 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:15:29,530 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'bel@@', 'en', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:15:29,530 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:15:29,530 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:15:29,530 - INFO - joeynmt.training - 	Hypothesis: Het duibelen in winter en contracten in de zomer .
2025-05-24 19:15:29,530 - INFO - joeynmt.training - Example #4
2025-05-24 19:15:29,530 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:15:29,530 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:15:29,530 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'snel@@', 'ste', 'van', 'wat', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:15:29,531 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:15:29,531 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:15:29,531 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een snelste van wat de afgelopen 25 jaar is gebeurd in de afgelopen 25 jaar .
2025-05-24 19:15:34,832 - INFO - joeynmt.training - Epoch   9, Step:    30600, Batch Loss:     1.391474, Batch Acc: 0.622959, Tokens per Sec:    13515, Lr: 0.000300
2025-05-24 19:15:40,219 - INFO - joeynmt.training - Epoch   9, Step:    30700, Batch Loss:     1.338269, Batch Acc: 0.629240, Tokens per Sec:    13162, Lr: 0.000300
2025-05-24 19:15:45,472 - INFO - joeynmt.training - Epoch   9, Step:    30800, Batch Loss:     1.389606, Batch Acc: 0.626998, Tokens per Sec:    13507, Lr: 0.000300
2025-05-24 19:15:50,856 - INFO - joeynmt.training - Epoch   9, Step:    30900, Batch Loss:     1.391837, Batch Acc: 0.625697, Tokens per Sec:    13185, Lr: 0.000300
2025-05-24 19:15:56,092 - INFO - joeynmt.training - Epoch   9, Step:    31000, Batch Loss:     1.142716, Batch Acc: 0.618732, Tokens per Sec:    13977, Lr: 0.000300
2025-05-24 19:15:56,092 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:15:56,092 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:16:05,631 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.40, acc:   0.51, generation: 9.5276[sec], evaluation: 0.0000[sec]
2025-05-24 19:16:05,632 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:16:05,713 - INFO - joeynmt.helpers - delete models/bpe_4k/30000.ckpt
2025-05-24 19:16:05,718 - INFO - joeynmt.training - Example #0
2025-05-24 19:16:05,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:16:05,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:16:05,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 's@@', 'kam@@', 'ers', ',', 'die', 'voor', 'de', 'meeste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'een', 'la@@', 'ger', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:16:05,719 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:16:05,719 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:16:05,719 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijferskamers , die voor de meeste drie miljoen jaar de grootte van de lager 48 staten , is een lager van 40 procent .
2025-05-24 19:16:05,719 - INFO - joeynmt.training - Example #1
2025-05-24 19:16:05,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:16:05,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:16:05,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'onder@@', 'ste@@', 'mt', 'de', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'het', 'zo', 'niet', 'zo', 'di@@', 'ep', 'van', 'het', 'ij@@', 's', 'van', 'het', 'ij@@', 's', '.', '</s>']
2025-05-24 19:16:05,720 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:16:05,720 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:16:05,720 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderonderstemt de serieus van dit probleem omdat het het zo niet zo diep van het ijs van het ijs .
2025-05-24 19:16:05,720 - INFO - joeynmt.training - Example #2
2025-05-24 19:16:05,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:16:05,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:16:05,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', '-@@', 'ij@@', 'sk@@', 'appen', 'is', ',', 'het', 'har@@', 't', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:16:05,720 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:16:05,720 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:16:05,721 - INFO - joeynmt.training - 	Hypothesis: De arctische ijferentie-ijskappen is , het hart hart van het wereldwijde klimaatsysteem .
2025-05-24 19:16:05,721 - INFO - joeynmt.training - Example #3
2025-05-24 19:16:05,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:16:05,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:16:05,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:16:05,721 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:16:05,721 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:16:05,721 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in de zomer .
2025-05-24 19:16:05,721 - INFO - joeynmt.training - Example #4
2025-05-24 19:16:05,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:16:05,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:16:05,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'zijn', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:16:05,722 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:16:05,722 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:16:05,722 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid snelste vooruit zijn van wat er de laatste 25 jaar is .
2025-05-24 19:16:11,066 - INFO - joeynmt.training - Epoch   9, Step:    31100, Batch Loss:     1.330002, Batch Acc: 0.626020, Tokens per Sec:    12968, Lr: 0.000300
2025-05-24 19:16:16,412 - INFO - joeynmt.training - Epoch   9, Step:    31200, Batch Loss:     1.403559, Batch Acc: 0.626027, Tokens per Sec:    13414, Lr: 0.000300
2025-05-24 19:16:21,677 - INFO - joeynmt.training - Epoch   9, Step:    31300, Batch Loss:     1.341366, Batch Acc: 0.626577, Tokens per Sec:    13646, Lr: 0.000300
2025-05-24 19:16:26,882 - INFO - joeynmt.training - Epoch   9, Step:    31400, Batch Loss:     1.351359, Batch Acc: 0.624922, Tokens per Sec:    13511, Lr: 0.000300
2025-05-24 19:16:32,128 - INFO - joeynmt.training - Epoch   9, Step:    31500, Batch Loss:     1.574950, Batch Acc: 0.624487, Tokens per Sec:    13505, Lr: 0.000300
2025-05-24 19:16:32,128 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:16:32,128 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:16:41,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.46, acc:   0.51, generation: 9.2554[sec], evaluation: 0.0000[sec]
2025-05-24 19:16:41,474 - INFO - joeynmt.helpers - delete models/bpe_4k/28000.ckpt
2025-05-24 19:16:41,479 - INFO - joeynmt.training - Example #0
2025-05-24 19:16:41,479 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:16:41,479 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:16:41,479 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'zodat', 'de', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 's@@', 'ka@@', 'p', ',', 'die', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'de', 'k@@', 'loo@@', 'f', 'van', '40', '%', '.', '</s>']
2025-05-24 19:16:41,479 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:16:41,479 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:16:41,479 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zien zodat de demonstreren die de arctische ijferskap , die de laatste drie miljoen jaar de grootte van de lager 48 staten , is de kloof van 40 % .
2025-05-24 19:16:41,480 - INFO - joeynmt.training - Example #1
2025-05-24 19:16:41,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:16:41,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:16:41,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'het', 'de', 'di@@', 'cht@@', 'k@@', 'ke', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:16:41,480 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:16:41,480 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:16:41,480 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten van dit specifieke probleem omdat het het de dichtkke van het ijs niet laat zien .
2025-05-24 19:16:41,480 - INFO - joeynmt.training - Example #2
2025-05-24 19:16:41,480 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:16:41,480 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:16:41,480 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 's@@', 'ka@@', 'p', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:16:41,481 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:16:41,481 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:16:41,481 - INFO - joeynmt.training - 	Hypothesis: De arctische ijferskap is in zekere zin het hart van het wereldwijde klimaat .
2025-05-24 19:16:41,481 - INFO - joeynmt.training - Example #3
2025-05-24 19:16:41,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:16:41,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:16:41,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:16:41,481 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:16:41,481 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:16:41,481 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in de zomer .
2025-05-24 19:16:41,482 - INFO - joeynmt.training - Example #4
2025-05-24 19:16:41,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:16:41,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:16:41,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'het', 'snel', 'zal', 'een', 'snel', 'voor@@', 'uit@@', 'gang', 'zijn', 'van', 'wat', 'de', 'laatste', '25', 'jaar', 'gebeurde', '.', '</s>']
2025-05-24 19:16:41,482 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:16:41,482 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:16:41,482 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat het snel zal een snel vooruitgang zijn van wat de laatste 25 jaar gebeurde .
2025-05-24 19:16:46,894 - INFO - joeynmt.training - Epoch   9, Step:    31600, Batch Loss:     1.359944, Batch Acc: 0.619217, Tokens per Sec:    12898, Lr: 0.000300
2025-05-24 19:16:52,114 - INFO - joeynmt.training - Epoch   9, Step:    31700, Batch Loss:     1.279444, Batch Acc: 0.621863, Tokens per Sec:    14044, Lr: 0.000300
2025-05-24 19:16:57,666 - INFO - joeynmt.training - Epoch   9, Step:    31800, Batch Loss:     1.579941, Batch Acc: 0.618955, Tokens per Sec:    12927, Lr: 0.000300
2025-05-24 19:17:02,901 - INFO - joeynmt.training - Epoch   9, Step:    31900, Batch Loss:     1.580662, Batch Acc: 0.619814, Tokens per Sec:    13841, Lr: 0.000300
2025-05-24 19:17:08,233 - INFO - joeynmt.training - Epoch   9, Step:    32000, Batch Loss:     1.302872, Batch Acc: 0.626188, Tokens per Sec:    13492, Lr: 0.000300
2025-05-24 19:17:08,233 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:17:08,233 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:17:17,212 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.51, generation: 8.9682[sec], evaluation: 0.0000[sec]
2025-05-24 19:17:17,212 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:17:17,297 - INFO - joeynmt.helpers - delete models/bpe_4k/30500.ckpt
2025-05-24 19:17:17,302 - INFO - joeynmt.training - Example #0
2025-05-24 19:17:17,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:17:17,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:17:17,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p', 'ij@@', 'sk@@', 'appen', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'p@@', 'eld', 'met', '40', 'procent', '.', '</s>']
2025-05-24 19:17:17,303 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:17:17,303 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:17:17,303 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijp ijskappen , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , is gepeld met 40 procent .
2025-05-24 19:17:17,303 - INFO - joeynmt.training - Example #1
2025-05-24 19:17:17,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:17:17,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:17:17,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mt', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'de', 'di@@', 'k@@', 'the@@', 'id', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:17:17,304 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:17:17,304 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:17:17,304 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemt het serieus van dit probleem omdat het de diktheid van het ijs niet laat zien .
2025-05-24 19:17:17,304 - INFO - joeynmt.training - Example #2
2025-05-24 19:17:17,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:17:17,304 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:17:17,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:17:17,304 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:17:17,304 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:17:17,304 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijs is , in zekere zin het hart van het wereldwijde klimaat .
2025-05-24 19:17:17,304 - INFO - joeynmt.training - Example #3
2025-05-24 19:17:17,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:17:17,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:17:17,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'pen', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:17:17,305 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:17:17,305 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:17:17,305 - INFO - joeynmt.training - 	Hypothesis: Het duipen in winter en contracten in de zomer .
2025-05-24 19:17:17,305 - INFO - joeynmt.training - Example #4
2025-05-24 19:17:17,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:17:17,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:17:17,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'wat', 'er', 'gebeurd', 'is', 'gebeurd', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:17:17,305 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:17:17,305 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:17:17,306 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid snelste vooruit wat er gebeurd is gebeurd in de afgelopen 25 jaar .
2025-05-24 19:17:22,627 - INFO - joeynmt.training - Epoch   9, Step:    32100, Batch Loss:     1.524484, Batch Acc: 0.620823, Tokens per Sec:    13225, Lr: 0.000300
2025-05-24 19:17:27,809 - INFO - joeynmt.training - Epoch   9, Step:    32200, Batch Loss:     1.254678, Batch Acc: 0.620242, Tokens per Sec:    13933, Lr: 0.000300
2025-05-24 19:17:33,156 - INFO - joeynmt.training - Epoch   9, Step:    32300, Batch Loss:     1.343994, Batch Acc: 0.617379, Tokens per Sec:    13276, Lr: 0.000300
2025-05-24 19:17:37,618 - INFO - joeynmt.training - Epoch   9: total training loss 4851.13
2025-05-24 19:17:37,618 - INFO - joeynmt.training - EPOCH 10
2025-05-24 19:17:38,419 - INFO - joeynmt.training - Epoch  10, Step:    32400, Batch Loss:     1.271511, Batch Acc: 0.633174, Tokens per Sec:    13358, Lr: 0.000300
2025-05-24 19:17:43,833 - INFO - joeynmt.training - Epoch  10, Step:    32500, Batch Loss:     1.265499, Batch Acc: 0.643810, Tokens per Sec:    13358, Lr: 0.000300
2025-05-24 19:17:43,833 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:17:43,833 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:17:52,398 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.39, acc:   0.51, generation: 8.5532[sec], evaluation: 0.0000[sec]
2025-05-24 19:17:52,475 - INFO - joeynmt.helpers - delete models/bpe_4k/31500.ckpt
2025-05-24 19:17:52,480 - INFO - joeynmt.training - Example #0
2025-05-24 19:17:52,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:17:52,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:17:52,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de@@', 'mon@@', 'str@@', 'eren', 'dat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'ka@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'k@@', 'ro@@', 'l', 'met', '40', 'procent', '.', '</s>']
2025-05-24 19:17:52,481 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:17:52,481 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:17:52,481 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat demonstreren dat de arctische ijferentiekap , die voor de meeste afgelopen drie miljoen jaar de grootte van de lagere 48 staten , heeft een krol met 40 procent .
2025-05-24 19:17:52,481 - INFO - joeynmt.training - Example #1
2025-05-24 19:17:52,481 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:17:52,481 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:17:52,481 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'e@@', 'kt', 'het', 'ser@@', 'ie@@', '-@@', 'probleem', 'omdat', 'het', 'het', 'zo', 'niet', 'de', 'di@@', 'k@@', 'the@@', 'id', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:17:52,482 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:17:52,482 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:17:52,482 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteekt het serie-probleem omdat het het zo niet de diktheid van het ijs niet laat zien .
2025-05-24 19:17:52,482 - INFO - joeynmt.training - Example #2
2025-05-24 19:17:52,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:17:52,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:17:52,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'ka@@', 'p', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:17:52,482 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:17:52,482 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijferentiekap is , in zekere zin het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - Example #3
2025-05-24 19:17:52,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:17:52,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:17:52,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'du@@', 'i@@', 'p@@', 'pen', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Hypothesis: Het duippen in winter en contracten in de zomer .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - Example #4
2025-05-24 19:17:52,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:17:52,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:17:52,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'zal', 'zijn', 'van', 'wat', 'er', 'gebeurd', 'is', 'in', 'de', 'afgelopen', '25', 'jaar', '.', '</s>']
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:17:52,483 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid snelste vooruit zal zijn van wat er gebeurd is in de afgelopen 25 jaar .
2025-05-24 19:17:57,745 - INFO - joeynmt.training - Epoch  10, Step:    32600, Batch Loss:     1.267946, Batch Acc: 0.646789, Tokens per Sec:    13169, Lr: 0.000300
2025-05-24 19:18:02,999 - INFO - joeynmt.training - Epoch  10, Step:    32700, Batch Loss:     1.285199, Batch Acc: 0.644506, Tokens per Sec:    13434, Lr: 0.000300
2025-05-24 19:18:08,490 - INFO - joeynmt.training - Epoch  10, Step:    32800, Batch Loss:     1.357342, Batch Acc: 0.644946, Tokens per Sec:    13161, Lr: 0.000300
2025-05-24 19:18:13,652 - INFO - joeynmt.training - Epoch  10, Step:    32900, Batch Loss:     1.367213, Batch Acc: 0.644663, Tokens per Sec:    14111, Lr: 0.000300
2025-05-24 19:18:18,983 - INFO - joeynmt.training - Epoch  10, Step:    33000, Batch Loss:     1.313421, Batch Acc: 0.637807, Tokens per Sec:    13665, Lr: 0.000300
2025-05-24 19:18:18,983 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:18:18,983 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:18:27,449 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.42, acc:   0.51, generation: 8.4561[sec], evaluation: 0.0000[sec]
2025-05-24 19:18:27,532 - INFO - joeynmt.helpers - delete models/bpe_4k/27000.ckpt
2025-05-24 19:18:27,538 - INFO - joeynmt.training - Example #0
2025-05-24 19:18:27,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:18:27,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:18:27,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'sk@@', 'ni@@', 'str@@', 'eren', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'ge@@', 'sla@@', 'gen', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:18:27,539 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:18:27,539 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:18:27,539 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijfersknistreren , die voor de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , heeft geslagen door 40 procent .
2025-05-24 19:18:27,539 - INFO - joeynmt.training - Example #1
2025-05-24 19:18:27,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:18:27,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:18:27,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'boven', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'probleem', 'omdat', 'het', 'zo', 'niet', 'de', 'di@@', 'ep', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:18:27,539 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Hypothesis: Maar dit ondersteboven het serieus van dit probleem omdat het zo niet de diep van het ijs niet laat zien .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - Example #2
2025-05-24 19:18:27,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:18:27,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:18:27,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'kun@@', 'st@@', 'ij@@', 'sk@@', 'iek', 'is', ',', 'in', 'ze@@', 'kere', 'zin', ',', 'het', 'be@@', 'vor@@', 'm@@', 'geving', 'van', 'het', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Hypothesis: Het kunstijskiek is , in zekere zin , het bevormgeving van het globaal klimaatsysteem .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - Example #3
2025-05-24 19:18:27,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:18:27,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:18:27,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'verspre@@', 'i@@', 'dt', 'in', 'de', 'z@@', 'om@@', 'er', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - 	Hypothesis: Het verspreidt in de zomer en contracten in de zomer .
2025-05-24 19:18:27,540 - INFO - joeynmt.training - Example #4
2025-05-24 19:18:27,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:18:27,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:18:27,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'wat', 'er', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:18:27,541 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:18:27,541 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:18:27,541 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid snelste vooruit wat er is gebeurd .
2025-05-24 19:18:32,794 - INFO - joeynmt.training - Epoch  10, Step:    33100, Batch Loss:     1.350657, Batch Acc: 0.640273, Tokens per Sec:    13426, Lr: 0.000300
2025-05-24 19:18:38,091 - INFO - joeynmt.training - Epoch  10, Step:    33200, Batch Loss:     1.352596, Batch Acc: 0.640142, Tokens per Sec:    13440, Lr: 0.000300
2025-05-24 19:18:43,291 - INFO - joeynmt.training - Epoch  10, Step:    33300, Batch Loss:     1.140544, Batch Acc: 0.637978, Tokens per Sec:    13458, Lr: 0.000300
2025-05-24 19:18:48,604 - INFO - joeynmt.training - Epoch  10, Step:    33400, Batch Loss:     1.313275, Batch Acc: 0.633333, Tokens per Sec:    13184, Lr: 0.000300
2025-05-24 19:18:53,897 - INFO - joeynmt.training - Epoch  10, Step:    33500, Batch Loss:     1.387159, Batch Acc: 0.630457, Tokens per Sec:    13492, Lr: 0.000300
2025-05-24 19:18:53,897 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:18:53,897 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:19:01,791 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.45, acc:   0.51, generation: 7.8800[sec], evaluation: 0.0000[sec]
2025-05-24 19:19:01,792 - INFO - joeynmt.training - Example #0
2025-05-24 19:19:01,792 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:19:01,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:19:01,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', 'zien', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 's@@', 'me@@', 'me@@', 'de@@', 'st@@', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'met', '40', '%', 'ge@@', 'sp@@', 'ron@@', 'gen', '.', '</s>']
2025-05-24 19:19:01,793 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:19:01,793 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:19:01,793 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia zien zodat de arctische ijfersmemedestcap , die voor de laatste drie miljoen jaar de grootte van de lager 48 staten , heeft met 40 % gesprongen .
2025-05-24 19:19:01,793 - INFO - joeynmt.training - Example #1
2025-05-24 19:19:01,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:19:01,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:19:01,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mt', 'de', 'ser@@', 'i@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'zo', 'de', 'di@@', 'ep', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:19:01,793 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:19:01,794 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:19:01,794 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemt de serioudheid van dit probleem omdat het zo de diep van het ijs niet laat zien .
2025-05-24 19:19:01,794 - INFO - joeynmt.training - Example #2
2025-05-24 19:19:01,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:19:01,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:19:01,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 's@@', 'ca@@', 'p', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:19:01,794 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:19:01,794 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:19:01,794 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijferscap is , in zekere zin het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:19:01,794 - INFO - joeynmt.training - Example #3
2025-05-24 19:19:01,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:19:01,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:19:01,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:19:01,795 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:19:01,795 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:19:01,795 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in zomer .
2025-05-24 19:19:01,795 - INFO - joeynmt.training - Example #4
2025-05-24 19:19:01,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:19:01,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:19:01,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'wat', 'er', 'is', 'gebeurd', 'in', 'de', 'laatste', '25', 'jaar', '.', '</s>']
2025-05-24 19:19:01,795 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:19:01,795 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:19:01,795 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid snelste vooruit wat er is gebeurd in de laatste 25 jaar .
2025-05-24 19:19:07,010 - INFO - joeynmt.training - Epoch  10, Step:    33600, Batch Loss:     1.127874, Batch Acc: 0.635274, Tokens per Sec:    13671, Lr: 0.000300
2025-05-24 19:19:12,311 - INFO - joeynmt.training - Epoch  10, Step:    33700, Batch Loss:     1.329629, Batch Acc: 0.634527, Tokens per Sec:    13437, Lr: 0.000300
2025-05-24 19:19:17,681 - INFO - joeynmt.training - Epoch  10, Step:    33800, Batch Loss:     1.310901, Batch Acc: 0.635782, Tokens per Sec:    13366, Lr: 0.000300
2025-05-24 19:19:22,936 - INFO - joeynmt.training - Epoch  10, Step:    33900, Batch Loss:     1.267782, Batch Acc: 0.634154, Tokens per Sec:    13397, Lr: 0.000300
2025-05-24 19:19:28,363 - INFO - joeynmt.training - Epoch  10, Step:    34000, Batch Loss:     1.222183, Batch Acc: 0.635669, Tokens per Sec:    13425, Lr: 0.000300
2025-05-24 19:19:28,364 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:19:28,364 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:19:37,756 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.51, generation: 9.3808[sec], evaluation: 0.0000[sec]
2025-05-24 19:19:37,841 - INFO - joeynmt.helpers - delete models/bpe_4k/28500.ckpt
2025-05-24 19:19:37,848 - INFO - joeynmt.training - Example #0
2025-05-24 19:19:37,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:19:37,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:19:37,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 'ijn', ',', 'die', 'voor', 'de', 'meeste', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'een', 'van', '40', 'procent', '.', '</s>']
2025-05-24 19:19:37,848 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:19:37,848 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:19:37,848 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de demonstreren die de arctische ijpijn , die voor de meeste laatste drie miljoen jaar de grootte van de lager 48 staten , heeft een van 40 procent .
2025-05-24 19:19:37,850 - INFO - joeynmt.training - Example #1
2025-05-24 19:19:37,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:19:37,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:19:37,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'ste@@', 'mt', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'di@@', 'ep', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:19:37,850 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:19:37,850 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:19:37,850 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstemt het serieus van dit specifieke probleem omdat het diep niet laat zien .
2025-05-24 19:19:37,850 - INFO - joeynmt.training - Example #2
2025-05-24 19:19:37,851 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:19:37,851 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:19:37,851 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tisch', 'ij@@', 's', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:19:37,851 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:19:37,851 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:19:37,851 - INFO - joeynmt.training - 	Hypothesis: Het arctisch ijs is , in zekere zin het hart van het wereldwijde klimaat .
2025-05-24 19:19:37,852 - INFO - joeynmt.training - Example #3
2025-05-24 19:19:37,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:19:37,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:19:37,852 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'be@@', 'z@@', 'om@@', 'er', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:19:37,852 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:19:37,852 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:19:37,852 - INFO - joeynmt.training - 	Hypothesis: Het bezomer in winter en contracten in de zomer .
2025-05-24 19:19:37,852 - INFO - joeynmt.training - Example #4
2025-05-24 19:19:37,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:19:37,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:19:37,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'de', 'laatste', '25', 'jaar', 'is', '.', '</s>']
2025-05-24 19:19:37,853 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:19:37,853 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:19:37,853 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat laat zien dat je een rapid zal zijn van wat de laatste 25 jaar is .
2025-05-24 19:19:43,053 - INFO - joeynmt.training - Epoch  10, Step:    34100, Batch Loss:     1.323990, Batch Acc: 0.635722, Tokens per Sec:    13621, Lr: 0.000300
2025-05-24 19:19:48,363 - INFO - joeynmt.training - Epoch  10, Step:    34200, Batch Loss:     1.330799, Batch Acc: 0.629853, Tokens per Sec:    13312, Lr: 0.000300
2025-05-24 19:19:53,635 - INFO - joeynmt.training - Epoch  10, Step:    34300, Batch Loss:     1.236474, Batch Acc: 0.634315, Tokens per Sec:    13845, Lr: 0.000300
2025-05-24 19:19:58,946 - INFO - joeynmt.training - Epoch  10, Step:    34400, Batch Loss:     1.422866, Batch Acc: 0.629149, Tokens per Sec:    13349, Lr: 0.000300
2025-05-24 19:20:04,296 - INFO - joeynmt.training - Epoch  10, Step:    34500, Batch Loss:     1.314395, Batch Acc: 0.629801, Tokens per Sec:    13003, Lr: 0.000300
2025-05-24 19:20:04,297 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:20:04,297 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:20:12,424 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.39, acc:   0.51, generation: 8.1164[sec], evaluation: 0.0000[sec]
2025-05-24 19:20:12,500 - INFO - joeynmt.helpers - delete models/bpe_4k/33000.ckpt
2025-05-24 19:20:12,507 - INFO - joeynmt.training - Example #0
2025-05-24 19:20:12,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:20:12,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:20:12,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'laten', 'zien', 'zodat', 'de@@', 'mon@@', 'str@@', 'eren', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'van', 'de', 'laatste', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g', '4@@', '8', 'st@@', 'aten', ',', 'is', 'door', '40', '%', '.', '</s>']
2025-05-24 19:20:12,507 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:20:12,507 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:20:12,508 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s laten zien zodat demonstreren die de arctische ijp , die voor de meeste van de laatste drie miljoen jaar de grootte van de lag 48 staten , is door 40 % .
2025-05-24 19:20:12,508 - INFO - joeynmt.training - Example #1
2025-05-24 19:20:12,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:20:12,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:20:12,508 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'zo', 'niet', 'de', 'di@@', 'k', 'van', 'het', 'ij@@', 's', 'niet', '.', '</s>']
2025-05-24 19:20:12,508 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:20:12,508 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:20:12,508 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieus van dit specifieke probleem omdat het zo niet de dik van het ijs niet .
2025-05-24 19:20:12,508 - INFO - joeynmt.training - Example #2
2025-05-24 19:20:12,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:20:12,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:20:12,508 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tisch', 'ij@@', 's', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'de', 'kli@@', 'maat@@', 'systeem', '.', '</s>']
2025-05-24 19:20:12,509 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:20:12,509 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:20:12,509 - INFO - joeynmt.training - 	Hypothesis: Het arctisch ijs is in zekere zin het hart van het wereldwijde klimaatsysteem .
2025-05-24 19:20:12,509 - INFO - joeynmt.training - Example #3
2025-05-24 19:20:12,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:20:12,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:20:12,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'anden', 'in', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:20:12,509 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:20:12,509 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:20:12,509 - INFO - joeynmt.training - 	Hypothesis: Het expanden in winter en contracten in de zomer .
2025-05-24 19:20:12,509 - INFO - joeynmt.training - Example #4
2025-05-24 19:20:12,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:20:12,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:20:12,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'jullie', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'fa@@', 'st@@', 'al', '25', 'jaar', '.', '</s>']
2025-05-24 19:20:12,510 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:20:12,510 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:20:12,510 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik jullie laat zien dat je een rapid fastal 25 jaar .
2025-05-24 19:20:17,736 - INFO - joeynmt.training - Epoch  10, Step:    34600, Batch Loss:     1.308728, Batch Acc: 0.632782, Tokens per Sec:    13774, Lr: 0.000300
2025-05-24 19:20:23,156 - INFO - joeynmt.training - Epoch  10, Step:    34700, Batch Loss:     1.345556, Batch Acc: 0.629452, Tokens per Sec:    13130, Lr: 0.000300
2025-05-24 19:20:28,336 - INFO - joeynmt.training - Epoch  10, Step:    34800, Batch Loss:     1.322438, Batch Acc: 0.624153, Tokens per Sec:    13878, Lr: 0.000300
2025-05-24 19:20:33,656 - INFO - joeynmt.training - Epoch  10, Step:    34900, Batch Loss:     1.334689, Batch Acc: 0.631383, Tokens per Sec:    13407, Lr: 0.000300
2025-05-24 19:20:38,948 - INFO - joeynmt.training - Epoch  10, Step:    35000, Batch Loss:     1.245211, Batch Acc: 0.626846, Tokens per Sec:    13614, Lr: 0.000300
2025-05-24 19:20:38,948 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:20:38,949 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:20:47,182 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.51, generation: 8.2195[sec], evaluation: 0.0000[sec]
2025-05-24 19:20:47,182 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-24 19:20:47,262 - INFO - joeynmt.helpers - delete models/bpe_4k/31000.ckpt
2025-05-24 19:20:47,267 - INFO - joeynmt.training - Example #0
2025-05-24 19:20:47,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:20:47,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:20:47,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'heb', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'sk@@', 'ca@@', 'p', ',', 'die', 'voor', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'ger', '4@@', '8', 'st@@', 'aten', ',', 'heeft', 'met', '40', 'procent', '.', '</s>']
2025-05-24 19:20:47,269 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:20:47,269 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:20:47,269 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar heb ik deze twee dia &apos; s zodat de arctische ijskcap , die voor de meeste afgelopen drie miljoen jaar de grootte van de lager 48 staten , heeft met 40 procent .
2025-05-24 19:20:47,269 - INFO - joeynmt.training - Example #1
2025-05-24 19:20:47,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:20:47,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:20:47,269 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'ie@@', 'us', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'zo', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'niet', 'laat', 'zien', '.', '</s>']
2025-05-24 19:20:47,270 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:20:47,270 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:20:47,270 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serieus van dit specifieke probleem omdat het zo de dikte van het ijs niet laat zien .
2025-05-24 19:20:47,270 - INFO - joeynmt.training - Example #2
2025-05-24 19:20:47,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:20:47,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:20:47,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'ar@@', 'c@@', 'tische', 'ij@@', 'fer@@', 'en@@', 'tie@@', 'sk@@', 'la@@', 'p', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'glob@@', 'ale', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:20:47,271 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:20:47,271 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:20:47,271 - INFO - joeynmt.training - 	Hypothesis: De arctische ijferentiesklap is in zekere zin het hart van het globale klimaat .
2025-05-24 19:20:47,271 - INFO - joeynmt.training - Example #3
2025-05-24 19:20:47,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:20:47,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:20:47,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'duur@@', 't', 'in', 'de', 'win@@', 'ter', 'en', 'con@@', 'trac@@', 'ten', 'in', 'de', 'z@@', 'om@@', 'er@@', 'ij', '.', '</s>']
2025-05-24 19:20:47,271 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:20:47,271 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:20:47,272 - INFO - joeynmt.training - 	Hypothesis: Het duurt in de winter en contracten in de zomerij .
2025-05-24 19:20:47,272 - INFO - joeynmt.training - Example #4
2025-05-24 19:20:47,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:20:47,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:20:47,272 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'zal', 'een', 'r@@', 'ap@@', 'id', 'snel@@', 'ste', 'voor@@', 'uit', 'zijn', 'van', 'wat', 'er', 'de', 'laatste', '25', 'jaar', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:20:47,272 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:20:47,272 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:20:47,272 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien zal een rapid snelste vooruit zijn van wat er de laatste 25 jaar is gebeurd .
2025-05-24 19:20:52,546 - INFO - joeynmt.training - Epoch  10, Step:    35100, Batch Loss:     1.370442, Batch Acc: 0.630135, Tokens per Sec:    13473, Lr: 0.000300
2025-05-24 19:20:57,758 - INFO - joeynmt.training - Epoch  10, Step:    35200, Batch Loss:     1.472100, Batch Acc: 0.626425, Tokens per Sec:    13712, Lr: 0.000300
2025-05-24 19:21:02,937 - INFO - joeynmt.training - Epoch  10, Step:    35300, Batch Loss:     1.300516, Batch Acc: 0.624661, Tokens per Sec:    13937, Lr: 0.000300
2025-05-24 19:21:08,388 - INFO - joeynmt.training - Epoch  10, Step:    35400, Batch Loss:     1.255133, Batch Acc: 0.628692, Tokens per Sec:    13085, Lr: 0.000300
2025-05-24 19:21:13,613 - INFO - joeynmt.training - Epoch  10, Step:    35500, Batch Loss:     1.495882, Batch Acc: 0.632226, Tokens per Sec:    13641, Lr: 0.000300
2025-05-24 19:21:13,613 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:21:13,614 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:21:22,596 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.51, generation: 8.9730[sec], evaluation: 0.0000[sec]
2025-05-24 19:21:22,674 - INFO - joeynmt.helpers - delete models/bpe_4k/34500.ckpt
2025-05-24 19:21:22,680 - INFO - joeynmt.training - Example #0
2025-05-24 19:21:22,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['La@@', 'st', 'year', 'I', 'sho@@', 'wed', 'these', 'two', 'sli@@', 'des', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lo@@', 'wer', '4@@', '8', 'st@@', 'ates', ',', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent', '.']
2025-05-24 19:21:22,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zien', 'om', 'aan', 'te', 'tonen', 'dat', 'de', 'p@@', 'oo@@', 'lij@@', 'sk@@', 'a@@', 'p', ',', 'die', 'de', 'afgelopen', 'drie', 'miljoen', 'jaar', 'ongeveer', 'de', 'groot@@', 'te', 'had', 'van', 'het', 'va@@', 'stel@@', 'and', 'van', 'de', 'VS', ',', 'met', '40', '%', 'ge@@', 'k@@', 'rom@@', 'pen', 'was', '.']
2025-05-24 19:21:22,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'or@@', 'ig', 'jaar', 'liet', 'ik', 'deze', 'twee', 'dia', '&apos;', 's', 'zodat', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 'pen', 'die', 'de', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 'aar@@', 't@@', 'ste', 'ka@@', 'p', ',', 'die', 'de', 'meeste', 'afgelopen', 'drie', 'miljoen', 'jaar', 'de', 'groot@@', 'te', 'van', 'de', 'la@@', 'g@@', 'ere', '4@@', '8', 'st@@', 'aten', ',', 'is', 'ge@@', 'sla@@', 'gen', 'door', '40', 'procent', '.', '</s>']
2025-05-24 19:21:22,680 - INFO - joeynmt.training - 	Source:     Last year I showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .
2025-05-24 19:21:22,680 - INFO - joeynmt.training - 	Reference:  Vorig jaar liet ik deze twee dia &apos; s zien om aan te tonen dat de poolijskap , die de afgelopen drie miljoen jaar ongeveer de grootte had van het vasteland van de VS , met 40 % gekrompen was .
2025-05-24 19:21:22,680 - INFO - joeynmt.training - 	Hypothesis: Vorig jaar liet ik deze twee dia &apos; s zodat de arctische ijppen die de arctische ijpaartste kap , die de meeste afgelopen drie miljoen jaar de grootte van de lagere 48 staten , is geslagen door 40 procent .
2025-05-24 19:21:22,680 - INFO - joeynmt.training - Example #1
2025-05-24 19:21:22,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'under@@', 'st@@', 'ates', 'the', 'ser@@', 'i@@', 'ou@@', 'sne@@', 'ss', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', '&apos;t', 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ice', '.']
2025-05-24 19:21:22,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Maar', 'dit', 'onder@@', 'sch@@', 'at', 'eigenlijk', 'de', 'er@@', 'n@@', 'st', 'van', 'dit', 'specif@@', 'ieke', 'probleem', 'omdat', 'het', 'niet', 'de', 'di@@', 'kte', 'van', 'het', 'ij@@', 's', 'laat', 'zien', '.']
2025-05-24 19:21:22,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Maar', 'dit', 'onder@@', 'st@@', 'aten', 'het', 'ser@@', 'i@@', 'ou@@', 'd@@', 'heid', 'van', 'dit', 'probleem', 'omdat', 'het', 'di@@', 'cht@@', 'ste', 'van', 'het', 'ij@@', 's', 'niet', 'laat', '.', '</s>']
2025-05-24 19:21:22,681 - INFO - joeynmt.training - 	Source:     But this understates the seriousness of this particular problem because it doesn &apos;t show the thickness of the ice .
2025-05-24 19:21:22,681 - INFO - joeynmt.training - 	Reference:  Maar dit onderschat eigenlijk de ernst van dit specifieke probleem omdat het niet de dikte van het ijs laat zien .
2025-05-24 19:21:22,681 - INFO - joeynmt.training - 	Hypothesis: Maar dit onderstaten het serioudheid van dit probleem omdat het dichtste van het ijs niet laat .
2025-05-24 19:21:22,681 - INFO - joeynmt.training - Example #2
2025-05-24 19:21:22,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', 'c@@', 'tic', 'ice', 'ca@@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-24 19:21:22,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'ij@@', 'sk@@', 'a@@', 'p', 'op', 'de', 'N@@', 'oor@@', 'd@@', 'p@@', 'ool', 'is', 'in', 'ze@@', 'kere', 'zin', 'het', 'kl@@', 'op@@', 'p@@', 'end', 'har@@', 't', 'van', 'ons', 'glob@@', 'aal', 'kli@@', 'maat@@', 'systeem', '.']
2025-05-24 19:21:22,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'ar@@', 'c@@', 'tische', 'ij@@', 'p@@', 'ijn', 'is', ',', 'in', 'ze@@', 'kere', 'zin', 'het', 'har@@', 't', 'van', 'het', 'wereldwij@@', 'd', 'kli@@', 'ma@@', 'at', '.', '</s>']
2025-05-24 19:21:22,682 - INFO - joeynmt.training - 	Source:     The arctic ice cap is , in a sense , the beating heart of the global climate system .
2025-05-24 19:21:22,682 - INFO - joeynmt.training - 	Reference:  De ijskap op de Noordpool is in zekere zin het kloppend hart van ons globaal klimaatsysteem .
2025-05-24 19:21:22,682 - INFO - joeynmt.training - 	Hypothesis: Het arctische ijpijn is , in zekere zin het hart van het wereldwijd klimaat .
2025-05-24 19:21:22,682 - INFO - joeynmt.training - Example #3
2025-05-24 19:21:22,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'exp@@', 'ands', 'in', 'win@@', 'ter', 'and', 'con@@', 'trac@@', 'ts', 'in', 'su@@', 'mmer', '.']
2025-05-24 19:21:22,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Het', 'zet', 'uit', 'in', 'de', 'win@@', 'ter', 'en', 'k@@', 'ri@@', 'mp@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.']
2025-05-24 19:21:22,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Het', 'exp@@', 'en@@', 'i@@', 'en@@', 't', 'in', 'de', 'z@@', 'om@@', 'er', '.', '</s>']
2025-05-24 19:21:22,682 - INFO - joeynmt.training - 	Source:     It expands in winter and contracts in summer .
2025-05-24 19:21:22,682 - INFO - joeynmt.training - 	Reference:  Het zet uit in de winter en krimpt in de zomer .
2025-05-24 19:21:22,682 - INFO - joeynmt.training - 	Hypothesis: Het expenient in de zomer .
2025-05-24 19:21:22,682 - INFO - joeynmt.training - Example #4
2025-05-24 19:21:22,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'r@@', 'ap@@', 'id', 'fa@@', 'st-@@', 'forward', 'of', 'what', '&apos;s', 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-24 19:21:22,683 - DEBUG - joeynmt.training - 	Tokenized reference:  ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'is', 'een', 'vers@@', 'n@@', 'elde', 'ver@@', 'sie', 'van', 'wat', 'er', 'de', 'afgelopen', '25', 'jaar', 'is', 'gebeurd', '.']
2025-05-24 19:21:22,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['De', 'volgende', 'dia', 'die', 'ik', 'laat', 'zien', 'dat', 'je', 'een', 'r@@', 'ap@@', 'id', 'zal', 'zijn', 'van', 'wat', 'de', 'laatste', '25', 'jaar', 'is', 'gebeurd', '.', '</s>']
2025-05-24 19:21:22,683 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapid fast-forward of what &apos;s happened over the last 25 years .
2025-05-24 19:21:22,683 - INFO - joeynmt.training - 	Reference:  De volgende dia die ik laat zien is een versnelde versie van wat er de afgelopen 25 jaar is gebeurd .
2025-05-24 19:21:22,683 - INFO - joeynmt.training - 	Hypothesis: De volgende dia die ik laat zien dat je een rapid zal zijn van wat de laatste 25 jaar is gebeurd .
2025-05-24 19:21:28,004 - INFO - joeynmt.training - Epoch  10, Step:    35600, Batch Loss:     1.357174, Batch Acc: 0.630929, Tokens per Sec:    13249, Lr: 0.000300
2025-05-24 19:21:33,321 - INFO - joeynmt.training - Epoch  10, Step:    35700, Batch Loss:     1.415850, Batch Acc: 0.624109, Tokens per Sec:    13063, Lr: 0.000300
2025-05-24 19:21:38,541 - INFO - joeynmt.training - Epoch  10, Step:    35800, Batch Loss:     1.296407, Batch Acc: 0.629690, Tokens per Sec:    13632, Lr: 0.000300
2025-05-24 19:21:43,839 - INFO - joeynmt.training - Epoch  10, Step:    35900, Batch Loss:     1.391516, Batch Acc: 0.624620, Tokens per Sec:    13678, Lr: 0.000300
2025-05-24 19:21:48,209 - INFO - joeynmt.training - Epoch  10: total training loss 4741.50
2025-05-24 19:21:48,209 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-24 19:21:48,209 - INFO - joeynmt.training - Best validation result (greedy) at step    35000:   6.35 ppl.
2025-05-24 19:21:48,224 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 19:21:48,278 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 19:21:48,347 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\bpe_4k\35000.ckpt.
2025-05-24 19:21:48,362 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3993),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3993),
	loss_function=None)
2025-05-24 19:21:48,363 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-24 19:21:48,363 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:21:48,363 - INFO - joeynmt.prediction - Predicting 1003 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:22:12,003 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 23.6290[sec], evaluation: 0.0000[sec]
2025-05-24 19:22:12,004 - INFO - joeynmt.prediction - Translations saved to: C:\Users\seraf\repositories\mt-exercise-4\models\bpe_4k\00035000.hyps.dev.
2025-05-24 19:22:12,004 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-24 19:22:12,004 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 19:22:12,004 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 19:22:45,246 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 33.2246[sec], evaluation: 0.0000[sec]
2025-05-24 19:22:45,248 - INFO - joeynmt.prediction - Translations saved to: C:\Users\seraf\repositories\mt-exercise-4\models\bpe_4k\00035000.hyps.test.

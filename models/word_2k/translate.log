2025-05-24 14:04:54,485 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:04:54,521 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:04:54,575 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:04:54,752 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:04:54,769 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:04:54,770 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:04:54,774 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:06:31,119 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:06:31,156 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:06:31,203 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:06:31,352 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:06:31,370 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:06:31,370 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:06:31,373 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:06:40,666 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:06:40,702 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:06:40,748 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:06:40,891 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:06:40,908 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:06:40,908 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:06:40,913 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:08:30,357 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:08:30,393 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:08:30,443 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:08:30,604 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:08:30,622 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:08:30,622 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:08:30,625 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:08:30,625 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:08:55,058 - INFO - joeynmt.prediction - Generation took 24.4224[sec]. (No references given)
2025-05-24 14:13:00,786 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:13:00,822 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:13:00,866 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:13:01,017 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:13:01,034 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:13:01,035 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:13:01,039 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:13:01,039 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:13:25,462 - INFO - joeynmt.prediction - Generation took 24.4118[sec]. (No references given)
2025-05-24 14:13:43,909 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:13:43,946 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:13:43,991 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:13:44,121 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:13:44,140 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:13:44,140 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:13:44,144 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:13:44,144 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:14:07,491 - INFO - joeynmt.prediction - Generation took 23.3378[sec]. (No references given)
2025-05-24 14:15:25,223 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:15:25,259 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:15:25,304 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:15:25,448 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:15:25,468 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:15:25,469 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:15:25,471 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:15:25,472 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:15:49,771 - INFO - joeynmt.prediction - Generation took 24.2908[sec]. (No references given)
2025-05-24 14:17:42,005 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-24 14:17:42,042 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 14:17:42,091 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 14:17:42,267 - INFO - joeynmt.helpers - Load model from C:\Users\seraf\repositories\mt-exercise-4\models\word_2k\24000.ckpt.
2025-05-24 14:17:42,285 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:17:42,285 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 14:17:42,289 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 14:17:42,289 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 14:18:01,376 - INFO - joeynmt.prediction - Generation took 19.0756[sec]. (No references given)
2025-05-24 17:26:27,600 - INFO - root - Hello! This is Joey-NMT (version 2.3.0).
2025-05-24 17:26:27,600 - WARNING - joeynmt.config - CUDA is not available. Use cpu device.
2025-05-24 17:26:27,601 - WARNING - joeynmt.config - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 17:26:27,601 - INFO - joeynmt.data - Building tokenizer...
2025-05-24 17:26:27,601 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 17:26:27,601 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 17:26:27,601 - INFO - joeynmt.data - Building vocabulary...
2025-05-24 17:26:27,647 - INFO - joeynmt.data - Data loaded.
2025-05-24 17:26:27,647 - INFO - joeynmt.data - Train dataset: None
2025-05-24 17:26:27,647 - INFO - joeynmt.data - Valid dataset: None
2025-05-24 17:26:27,647 - INFO - joeynmt.data -  Test dataset: StreamDataset(split=test, len=0, src_lang="en", trg_lang="nl", has_trg=False, random_subset=-1, has_src_prompt=False, has_trg_prompt=False)
2025-05-24 17:26:27,647 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) a
2025-05-24 17:26:27,647 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) de (7) een (8) het (9) van
2025-05-24 17:26:27,648 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2025-05-24 17:26:27,648 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2025-05-24 17:26:27,648 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 17:26:27,733 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 17:26:27,735 - INFO - joeynmt.model - Total params: 3925248
2025-05-24 17:26:27,735 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2025-05-24 17:26:27,736 - INFO - joeynmt.prediction - Loading model from /Users/merterol/Desktop/MT ex4/mt-exercise-4/models/word_2k/best.ckpt
2025-05-24 17:26:27,818 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-24 17:26:27,829 - INFO - joeynmt.prediction - Ready to decode. (device: cpu, n_gpu: 0, use_ddp: False, fp16: False)
2025-05-24 17:26:27,863 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 17:27:32,372 - INFO - joeynmt.prediction - Generation took 64.5079[sec].
2025-05-24 17:58:49,306 - INFO - root - Hello! This is Joey-NMT (version 2.3.0).
2025-05-24 17:58:49,307 - WARNING - joeynmt.config - CUDA is not available. Use cpu device.
2025-05-24 17:58:49,307 - WARNING - joeynmt.config - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-24 17:58:49,307 - INFO - joeynmt.data - Building tokenizer...
2025-05-24 17:58:49,308 - INFO - joeynmt.tokenizers - en tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 17:58:49,308 - INFO - joeynmt.tokenizers - nl tokenizer: BasicTokenizer(level=word, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none)
2025-05-24 17:58:49,308 - INFO - joeynmt.data - Building vocabulary...
2025-05-24 17:58:49,353 - INFO - joeynmt.data - Data loaded.
2025-05-24 17:58:49,353 - INFO - joeynmt.data - Train dataset: None
2025-05-24 17:58:49,353 - INFO - joeynmt.data - Valid dataset: None
2025-05-24 17:58:49,353 - INFO - joeynmt.data -  Test dataset: StreamDataset(split=test, len=0, src_lang="en", trg_lang="nl", has_trg=False, random_subset=-1, has_src_prompt=False, has_trg_prompt=False)
2025-05-24 17:58:49,353 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) a
2025-05-24 17:58:49,353 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) de (7) een (8) het (9) van
2025-05-24 17:58:49,353 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 2004
2025-05-24 17:58:49,353 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 2004
2025-05-24 17:58:49,353 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-24 17:58:49,406 - INFO - joeynmt.model - Enc-dec model built.
2025-05-24 17:58:49,407 - INFO - joeynmt.model - Total params: 3925248
2025-05-24 17:58:49,408 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2025-05-24 17:58:49,408 - INFO - joeynmt.prediction - Loading model from /Users/merterol/Desktop/MT ex4/mt-exercise-4/models/word_2k/best.ckpt
2025-05-24 17:58:49,481 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=2004),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-24 17:58:49,489 - INFO - joeynmt.prediction - Ready to decode. (device: cpu, n_gpu: 0, use_ddp: False, fp16: False)
2025-05-24 17:58:49,525 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-24 18:00:05,923 - INFO - joeynmt.prediction - Generation took 76.3976[sec].
